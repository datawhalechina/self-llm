"""
MLX-LM ç®€å•æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹
ä¸€æ¬¡æ€§ç”Ÿæˆï¼Œé€‚åˆå¿«é€Ÿæµ‹è¯•
"""

from functools import wraps
import time
import mlx.core as mx
from mlx_lm import load, generate
from mlx_lm.sample_utils import make_sampler

def time_logger(task_name=None):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            name = task_name or func.__name__
            print(f"ğŸš€ å¼€å§‹ï¼š{name}")
            s = time.time()
            out = func(*args, **kwargs)
            print(f"âœ… {name} å®Œæˆï¼Œè€—æ—¶ {time.time()-s:.2f} ç§’")
            return out
        return wrapper
    return decorator

class Config:
    def __init__(self):
        self.local_dir = "../models" # æœ¬åœ°æ¨¡å‹å­˜æ”¾è·¯å¾„
        self.model_name = "Qwen3-8B-4bit" # æ¨¡å‹

cfg = Config()   
def get_cfg():
    return cfg
    
class App:
    def __init__(self, cfg: Config):
        self.cfg = cfg
        self.init_model()

    @time_logger(task_name=f"åŠ è½½æ¨¡å‹ {get_cfg().model_name}")
    def init_model(self):
        self.model, self.tokenizer = load(f"{self.cfg.local_dir}/{self.cfg.model_name}")
        mx.eval()  # ç¡®ä¿æ¨¡å‹åŠ è½½å®Œæˆ

    def generate(self, prompt: str):
        # åˆ›å»ºé‡‡æ ·å™¨
        sampler = make_sampler(temp=0.7, top_p=0.8)
        response = generate(
            self.model, 
            self.tokenizer, 
            prompt=prompt,
            max_tokens=256,
            sampler=sampler,
            verbose=False
        )
        mx.eval()  # ç¡®ä¿æ‰€æœ‰è®¡ç®—å®Œæˆ
        return response

    def run(self, question: str = None):
        # æ„å»ºæç¤º
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": question or "è¯·ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"}
        ]
        prompt = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        # ç”Ÿæˆå›ç­”
        s_time = time.time()
        response = self.generate(prompt)
        gen_time = time.time() - s_time

        # ç»Ÿè®¡ prompt tokens
        prompt_tokens = len(self.tokenizer.encode(prompt))
        # ç»Ÿè®¡ç”Ÿæˆ tokens
        response_tokens = len(self.tokenizer.encode(response))

        print("\nğŸ¤– æ¨¡å‹å›å¤\n" + "=" * 50)
        print(response)
        print("=" * 50)

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print("\nğŸ“Š æ€§èƒ½ç»Ÿè®¡")
        print("-" * 30)
        print(f"  Prompt tokens:   {prompt_tokens}")
        print(f"  ç”Ÿæˆ tokens:     {response_tokens}")
        print(f"  ç”Ÿæˆæ€»è€—æ—¶:      {gen_time:.2f}s")
        print(f"  ç”Ÿæˆé€Ÿåº¦:        {response_tokens / gen_time:.1f} tokens/s")
        print(f"  å³°å€¼å†…å­˜:        {mx.get_peak_memory() / 1024**3:.2f} GB")
        print("-" * 30)

if __name__ == "__main__":
    app = App(get_cfg())
    app.run("è¯·ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ")