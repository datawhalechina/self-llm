
This tutorial aims to help users deploy and use large models based on Apple's native framework [MLX-LM](https://github.com/ml-explore/mlx-lm), in order to fully utilize the performance of Apple M series chips for local inference.

## ðŸ”§ Environment Setup

```bash
# Create Conda virtual environment
conda create -n mlx-lm python=3.11
conda activate mlx-lm
# Install dependencies
pip install -r requirements.txt
```

## ðŸ“– Tutorial List

<table align="center">
  <tr>
    <td valign="top" width="25%">
      â€¢ <a href="./docs/MLX-LM_Intro.md">MLX Framework Introduction</a><br>
    </td>
  </tr>

## ðŸš€ Supported Models

<table align="center">
  <tr>
    <td valign="top" width="25%">
      â€¢ <a href="./support_model.md#qwen3">Qwen3</a><br>
    </td>
  </tr>
</table>