# Qwen3-8B GRPOå¾®è°ƒåŠé€šè¿‡swanlabå¯è§†åŒ–

> åŠå°æ—¶å­¦ä¼šä½¿ç”¨ GRPO é€šè¿‡ Unsloth è®­ç»ƒè‡ªå·±çš„ç±» DeepSeek-R1 æ¨ç†æ¨¡å‹ã€‚

# å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰

> ç›¸ä¿¡å¤§å®¶å’Œæˆ‘ä¸€æ ·ï¼Œåˆšåˆšæ¥è§¦RLä¼šè§‰å¾—å¾ˆéš¾ä¸”æ²¡æœ‰å¿…è¦ï¼Œä¸ºä»€ä¹ˆåªåšSFTä¸å¯ä»¥å‘¢ï¼Ÿ
> ä¸å¦¨æƒ³è±¡ä¸€ä¸‹ï¼Œæˆ‘ä»¬ä¸æ˜¯ç›´æ¥å‘Šè¯‰æœºå™¨â€œé‡åˆ°Aæƒ…å†µå°±åšBâ€ï¼Œè€Œæ˜¯è®©æœºå™¨è‡ªå·±å»â€œæ¢ç´¢â€ï¼Œåœ¨ä¸æ–­â€œå°è¯•â€å’Œâ€œå¸å–æ•™è®­â€ä¸­å­¦ä¼šå¦‚ä½•åšå‡ºæœ€å¥½çš„å†³ç­–ï¼Œå°±åƒæˆ‘ä»¬äººç±»å­¦ä¹ æ–°æŠ€èƒ½ä¸€æ ·ï¼Œå¾€å¾€å¯ä»¥æ”¶è·æ„æƒ³ä¸åˆ°çš„æ•ˆæœï¼Œè®©æ¨¡å‹èƒ½å¤Ÿå®Œæˆæ›´åŠ å¤æ‚çš„ç¤¾ä¼šåœºæ™¯ï¼Œè´´è¿‘äºäººç±»ç†æƒ³ä¸­çš„â€œAIâ€ï¼Œè¿™å°±æ˜¯å¼ºåŒ–å­¦ä¹ çš„é­…åŠ›æ‰€åœ¨ã€‚

## ä¸ºä»€ä¹ˆè¦åšRLï¼Ÿ

ç®€å•æ¥è¯´ï¼Œå¼ºåŒ–å­¦ä¹ è‡´åŠ›äºè§£å†³â€œ**åºè´¯å†³ç­–ï¼ˆSequential Decision Makingï¼‰**â€é—®é¢˜ï¼Œä¹Ÿå°±æ˜¯å¦‚ä½•åœ¨ä¸€ç³»åˆ—è¿ç»­çš„æ­¥éª¤ä¸­åšå‡ºæœ€ä¼˜å†³ç­–ï¼Œä»¥è¾¾æˆä¸€ä¸ªé•¿è¿œçš„ç›®æ ‡ã€‚

1. **è§£å†³å¤æ‚æœªçŸ¥ç¯å¢ƒä¸‹çš„å†³ç­–é—®é¢˜**ï¼š
	1. åœ¨å¾ˆå¤šç°å®åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬å¯èƒ½æ— æ³•é¢„å…ˆçŸ¥é“æ‰€æœ‰è§„åˆ™ï¼Œæˆ–è€…ç¯å¢ƒæœ¬èº«å°±éå¸¸å¤æ‚å¤šå˜ã€‚æ¯”å¦‚ï¼Œè®©æœºå™¨äººå­¦ä¼šåœ¨å´å²–ä¸å¹³çš„åœ°é¢ä¸Šè¡Œèµ°ï¼Œæˆ‘ä»¬å¾ˆéš¾æ‰‹åŠ¨ç¼–ç¨‹æ‰€æœ‰å¯èƒ½é‡åˆ°çš„æƒ…å†µå’Œåº”å¯¹ç­–ç•¥ã€‚å¼ºåŒ–å­¦ä¹ å…è®¸æœºå™¨åœ¨ä¸ç¯å¢ƒçš„**äº’åŠ¨**ä¸­è‡ªä¸»å­¦ä¹ ï¼Œæœ€è¿‘æ¯”è¾ƒç«çš„å®‡æ ‘ç§‘æŠ€å’Œæ™ºå…ƒæœºå™¨äººï¼Œå°±å¤§å¹…åº”ç”¨äº†å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚
		
	2. å†æ¯”å¦‚ï¼Œä¸‹å›´æ£‹ï¼ˆAlphaGo å°±æ˜¯ä¸€ä¸ªç»å…¸ä¾‹å­ï¼‰ã€‚æ£‹ç›˜çŠ¶æ€åƒå˜ä¸‡åŒ–ï¼Œä¸å¯èƒ½ç©·ä¸¾æ‰€æœ‰æƒ…å†µã€‚RLå¯ä»¥è®©æ™ºèƒ½ä½“é€šè¿‡è‡ªæˆ‘å¯¹å¼ˆæ¥å­¦ä¹ é«˜è¶…çš„æ£‹è‰ºã€‚
	
2. **å®ç°çœŸæ­£çš„â€œAIâ€**ï¼š
	1. ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ ï¼ˆSFTï¼‰éœ€è¦å¤§é‡çš„â€œæ ‡æ³¨æ•°æ®â€ï¼Œå¾€å¾€è¿™ä¸ªé‡çº§è‡³å°‘æ˜¯ä¸‰ä½æ•°èµ·æ­¥ï¼Œå‘Šè¯‰æ¨¡å‹â€œè¾“å…¥Xå¯¹åº”è¾“å‡ºYâ€ã€‚ä½†åœ¨å¾ˆå¤šä»»åŠ¡ä¸­ï¼Œè·å–è¿™æ ·çš„å®Œç¾æ ‡æ³¨æ•°æ®æˆæœ¬æé«˜ï¼Œç”šè‡³ä¸å¯èƒ½ï¼ˆæ¯”å¦‚ï¼Œå¦‚ä½•æ ‡æ³¨ä¸€ä¸ªâ€œå®Œç¾â€çš„æœºå™¨äººè¡Œèµ°å§¿æ€åºåˆ—ï¼Ÿæ ‡æ³¨ç³ç…æ»¡ç›®çš„å•†å“ä¿¡æ¯ï¼Ÿï¼‰ã€‚
		
	2. å¼ºåŒ–å­¦ä¹ æ›´æ¥è¿‘äººç±»å­¦ä¹ çš„æ–¹å¼â€”â€”é€šè¿‡**è¯•é”™ï¼ˆTrial and Errorï¼‰**ã€‚åšå¾—å¥½å°±ç»™â€œå¥–åŠ±â€ï¼Œåšå¾—ä¸å¥½å°±ç»™â€œæƒ©ç½šâ€ï¼ˆæˆ–è€…ä¸ç»™å¥–åŠ±ï¼‰ï¼Œæ™ºèƒ½ä½“åœ¨ä¸ç¯å¢ƒçš„äº’åŠ¨ä¸­ï¼Œé€šè¿‡æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±æ¥å­¦ä¹ æœ€ä½³ç­–ç•¥ã€‚
		
	3. å»¶è¿Ÿå¥–åŠ±é—®é¢˜ï¼šå¾ˆå¤šæ—¶å€™ï¼Œä¸€ä¸ªè¡Œä¸ºçš„å¥½åå¹¶ä¸èƒ½ç«‹åˆ»æ˜¾ç°å‡ºæ¥ã€‚æ¯”å¦‚ï¼Œåœ¨æ¸¸æˆä¸­ï¼Œä½ ç°åœ¨èµ°çš„ä¸€æ­¥æ£‹ï¼Œå¯èƒ½è¦åˆ°å‡ åæ­¥ä¹‹åæ‰èƒ½çœ‹å‡ºæ˜¯å¥½æ˜¯åã€‚å¼ºåŒ–å­¦ä¹ çš„ç®—æ³•èƒ½å¤Ÿå°†æœ€ç»ˆçš„å¥–åŠ±/æƒ©ç½š**å›æº¯**åˆ†é…ç»™å¯¼è‡´è¿™ä¸€ç»“æœçš„ä¸€ç³»åˆ—è¡Œä¸ºï¼Œä»è€Œå­¦ä¼šé•¿è¿œè§„åˆ’ã€‚
	
3. **å¹¿æ³›çš„åº”ç”¨å‰æ™¯**ï¼š
	1. **æ¸¸æˆ**AI**ï¼šæ¯”å¦‚TEGå’Œå¤©ç¾å¯¹ç‹è€…è£è€€è”åˆå¼€å‘çš„NPCã€ç½‘æ˜“çš„æ°¸åŠ«æ— é—´è¯­éŸ³é˜Ÿå‹ï¼Œéƒ½åº”ç”¨äº†å¼ºåŒ–å­¦ä¹ ç®—æ³•
		
	2. **æœºå™¨äººæ§åˆ¶**ï¼šæœºå™¨äººè¡Œèµ°-å®‡æ ‘ç§‘æŠ€å’Œæ™ºå…ƒæœºå™¨äººã€æ— äººæœºé£è¡Œç®—æ³•-DJIã€‚
		
	3. **è‡ªåŠ¨é©¾é©¶**ï¼šä¸»è¦ä½“ç°åœ¨å†³ç­–ç³»ç»Ÿï¼Œç›®å‰ä¸»æµçš„è‡ªåŠ¨é©¾é©¶å‚å•†å¦‚momentaä¹Ÿåœ¨å°è¯•RLç®—æ³•ã€‚
		
	4. **æ¨èç³»ç»Ÿ**ï¼šå¦‚ä½•ç»™ç”¨æˆ·æ¨èä¸€ç³»åˆ—å†…å®¹ï¼Œå°çº¢ä¹¦ã€å¿«æ‰‹ã€å­—èŠ‚éƒ½åœ¨è¿™ä¸Šé¢æŠ•å…¥å·¨å¤§ã€‚
		

## RLçš„æ ¸å¿ƒæ€æƒ³æ˜¯ä»€ä¹ˆï¼Ÿ

å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒæ€æƒ³å¯ä»¥æ¦‚æ‹¬ä¸ºï¼š**æ™ºèƒ½ä½“ï¼ˆAgentï¼‰é€šè¿‡ä¸ç¯å¢ƒï¼ˆEnvironmentï¼‰çš„äº’åŠ¨ï¼Œæ ¹æ®ç¯å¢ƒåé¦ˆçš„å¥–åŠ±ï¼ˆRewardï¼‰æˆ–æƒ©ç½šï¼ˆPunishmentï¼‰ï¼Œå­¦ä¹ ä¸€ä¸ªæœ€ä¼˜ç­–ç•¥ï¼ˆPolicyï¼‰ï¼Œä»¥æœ€å¤§åŒ–å…¶é•¿æœŸç´¯ç§¯å¥–åŠ±ã€‚** 
å¬èµ·æ¥æœ‰ç‚¹æŠ½è±¡ï¼Ÿæˆ‘ä»¬æŠŠå®ƒæ‹†è§£å¼€æ¥çœ‹ï¼š

1. **æ™ºèƒ½ä½“** **(Agent)** ğŸ¤–ï¼š
	1. è¿™å°±æ˜¯æˆ‘ä»¬è¦è®­ç»ƒçš„å­¦ä¹ è€…å’Œå†³ç­–è€…ã€‚å®ƒå¯ä»¥æ˜¯ä¸€ä¸ªæœºå™¨äººã€æ¸¸æˆä¸­çš„è§’è‰²ã€è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿç­‰ã€‚
		
	2. æ™ºèƒ½ä½“çš„ç›®æ ‡æ˜¯å­¦ä¹ ä¸€ä¸ª**ç­–ç•¥ (Policy)**ã€‚
	
2. **ç¯å¢ƒ (Environment)** ğŸŒ³ï¼š
	1. æ™ºèƒ½ä½“æ‰€å¤„çš„å¤–éƒ¨ä¸–ç•Œï¼Œå®ƒä¼šå“åº”æ™ºèƒ½ä½“çš„è¡Œä¸ºå¹¶ç»™å‡ºåé¦ˆã€‚
		
	2. ç¯å¢ƒæœ‰å…¶è‡ªèº«çš„**çŠ¶æ€ (State)**ã€‚
	
3. **çŠ¶æ€ (State, S)** ğŸ“Šï¼š
	1. å¯¹å½“å‰ç¯å¢ƒçš„ä¸€ä¸ªæè¿°ã€‚æ¯”å¦‚ï¼Œåœ¨æœºå™¨äººè¡Œèµ°ä»»åŠ¡ä¸­ï¼ŒçŠ¶æ€å¯ä»¥æ˜¯æœºå™¨äººå„ä¸ªå…³èŠ‚çš„è§’åº¦ã€é€Ÿåº¦ï¼›åœ¨æ¸¸æˆä¸­ï¼ŒçŠ¶æ€å¯ä»¥æ˜¯å½“å‰çš„æ¸¸æˆç”»é¢ã€‚
		
	2. æ™ºèƒ½ä½“æ ¹æ®å½“å‰çŠ¶æ€æ¥å†³å®šä¸‹ä¸€æ­¥çš„è¡ŒåŠ¨ã€‚
	
4. **åŠ¨ä½œ (Action, A)** ğŸƒï¼š
	1. æ™ºèƒ½ä½“åœ¨æŸä¸ªçŠ¶æ€ä¸‹å¯ä»¥æ‰§è¡Œçš„æ“ä½œã€‚æ¯”å¦‚ï¼Œæœºå™¨äººå¯ä»¥æ§åˆ¶æŸä¸ªå…³èŠ‚è½¬åŠ¨ï¼Œæ¸¸æˆè§’è‰²å¯ä»¥å‘å‰èµ°ã€è·³è·ƒã€æ”»å‡»ã€‚
	
5. **å¥–åŠ± (Reward, R)** ğŸ† / **æƒ©ç½š** ğŸ’”ï¼š
	1. å½“æ™ºèƒ½ä½“åœ¨æŸä¸ªçŠ¶æ€ S æ‰§è¡Œäº†æŸä¸ªåŠ¨ä½œ Aï¼Œè½¬ç§»åˆ°æ–°çš„çŠ¶æ€ Sâ€² åï¼Œç¯å¢ƒä¼šç»™å‡ºä¸€ä¸ªæ ‡é‡ä¿¡å·â€”â€”å¥–åŠ±ï¼ˆæˆ–æƒ©ç½šï¼‰ã€‚
		
	2. è¿™ä¸ªå¥–åŠ±ä¿¡å·æ˜¯æŒ‡å¯¼æ™ºèƒ½ä½“å­¦ä¹ çš„å…³é”®ã€‚æ­£å¥–åŠ±è¡¨ç¤ºè¿™ä¸ªåŠ¨ä½œæ˜¯å¥½çš„ï¼Œè´Ÿå¥–åŠ±ï¼ˆæƒ©ç½šï¼‰è¡¨ç¤ºè¿™ä¸ªåŠ¨ä½œæ˜¯åçš„ã€‚
		
	3. **é‡è¦**ï¼šæ™ºèƒ½ä½“çš„ç›®æ ‡ä¸æ˜¯æœ€å¤§åŒ–ç¬æ—¶å¥–åŠ±ï¼Œè€Œæ˜¯**æœ€å¤§åŒ–æœªæ¥çš„ç´¯ç§¯å¥–åŠ±** (Cumulative Reward)ã€‚æ¯”å¦‚ï¼Œåƒç³–æœï¼ˆç¬æ—¶å¥–åŠ±ï¼‰å¾ˆå¼€å¿ƒï¼Œä½†å¦‚æœå› æ­¤å¯¼è‡´è›€ç‰™ï¼ˆæœªæ¥æƒ©ç½šï¼‰ï¼Œé‚£å¯èƒ½å°±ä¸æ˜¯ä¸€ä¸ªå¥½ç­–ç•¥ã€‚
	
6. **ç­–ç•¥ (Policy, Ï€)** ğŸ—ºï¸ï¼š
	1. è¿™æ˜¯æ™ºèƒ½ä½“å­¦ä¹ çš„æ ¸å¿ƒï¼ç­–ç•¥å®šä¹‰äº†æ™ºèƒ½ä½“åœ¨ç»™å®šçŠ¶æ€ä¸‹åº”è¯¥é‡‡å–ä»€ä¹ˆåŠ¨ä½œã€‚
		
	2. å®ƒå¯ä»¥æ˜¯ç¡®å®šæ€§çš„ï¼ˆåœ¨çŠ¶æ€ sï¼Œæ€»æ˜¯æ‰§è¡ŒåŠ¨ä½œ aï¼‰ï¼Œä¹Ÿå¯ä»¥æ˜¯éšæœºæ€§çš„ï¼ˆåœ¨çŠ¶æ€ sï¼Œä»¥ä¸€å®šæ¦‚ç‡æ‰§è¡ŒåŠ¨ä½œ a1ï¼Œä»¥ä¸€å®šæ¦‚ç‡æ‰§è¡ŒåŠ¨ä½œ a2 ç­‰ï¼‰ã€‚
		
	3. æ•°å­¦ä¸Šé€šå¸¸è¡¨ç¤ºä¸º Ï€(aâˆ£s)=P(At=aâˆ£St=s)ï¼Œå³åœ¨çŠ¶æ€ St æ—¶é‡‡å–åŠ¨ä½œ a çš„æ¦‚ç‡ã€‚
	
7. **ä»·å€¼å‡½æ•°** **(Value Function, V(s) æˆ– Q(s,a))** ğŸ’°ï¼š
	1. é™¤äº†ç­–ç•¥ï¼Œæ™ºèƒ½ä½“é€šå¸¸è¿˜ä¼šå­¦ä¹ ä¸€ä¸ªä»·å€¼å‡½æ•°ã€‚
		
	2. çŠ¶æ€ä»·å€¼å‡½æ•° **VÏ€(s)**ï¼šè¡¨ç¤ºä»çŠ¶æ€ s å¼€å§‹ï¼ŒæŒ‰ç…§ç­–ç•¥ Ï€ ç»§ç»­ä¸‹å»ï¼Œèƒ½å¤Ÿè·å¾—çš„æœŸæœ›ç´¯ç§¯å¥–åŠ±ã€‚å®ƒè¡¡é‡äº†å½“å‰çŠ¶æ€æœ‰å¤šå¥½ã€‚
		
	3. **çŠ¶æ€-åŠ¨ä½œä»·å€¼å‡½æ•°** **QÏ€(s,a)**ï¼šè¡¨ç¤ºåœ¨çŠ¶æ€ s æ‰§è¡ŒåŠ¨ä½œ a åï¼Œå†æŒ‰ç…§ç­–ç•¥ Ï€ ç»§ç»­ä¸‹å»ï¼Œèƒ½å¤Ÿè·å¾—çš„æœŸæœ›ç´¯ç§¯å¥–åŠ±ã€‚å®ƒè¡¡é‡äº†åœ¨å½“å‰çŠ¶æ€ä¸‹æ‰§è¡ŒæŸä¸ªç‰¹å®šåŠ¨ä½œæœ‰å¤šå¥½ã€‚è¿™ä¸ª Q å‡½æ•°ï¼ˆQ-valueï¼‰åœ¨å¾ˆå¤šç®—æ³•ä¸­éå¸¸å…³é”®ã€‚
		

æ€»ç»“æ¥è¯´ï¼Œ**å¼ºåŒ–å­¦ä¹ **çš„æ ¸å¿ƒæ€æƒ³å°±æ˜¯**é€šè¿‡ä¸ç¯å¢ƒçš„äº’åŠ¨å’Œè¯•é”™ï¼Œå­¦ä¹ ä¸€ä¸ªä»çŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„ï¼ˆç­–ç•¥ï¼‰ï¼Œä»¥æœ€å¤§åŒ–æ™ºèƒ½ä½“è·å¾—çš„é•¿æœŸç´¯ç§¯å¥–åŠ±**ã€‚å®ƒæä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶ï¼Œè®©æœºå™¨èƒ½å¤Ÿåƒäººç±»ä¸€æ ·ä»ç»éªŒä¸­å­¦ä¹ å¦‚ä½•åœ¨å¤æ‚ä¸–ç•Œä¸­åšå‡ºæ˜æ™ºçš„å†³ç­–ã€‚

# GRPO

## GRPOç›¸è¾ƒäºå…¶ä»–RLæ–¹æ³•çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ

> å¸¸è§çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åŒ…æ‹¬ï¼šPPOï¼ˆæœ€æ—©çš„ï¼‰ã€DPOï¼ˆæœ€ç®€å•çš„ï¼‰ã€GRPOï¼ˆéšç€deepseekçˆ†ç«çš„ï¼‰ç­‰

**PPO** **(Proximal Policy Optimization,** **è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–)**

- **æ ¸å¿ƒæ€æƒ³**ï¼š PPO æ˜¯ä¸€ç§ç»å…¸çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå±äº**Actor-Critic**æ¶æ„ã€‚
	

**DPO** **(Direct Preference Optimization, ç›´æ¥åå¥½ä¼˜åŒ–)**

- **æ ¸å¿ƒæ€æƒ³**ï¼š DPO æå‡ºäº†ä¸€ç§æ›´ç®€æ´çš„æ–¹æ³•æ¥è¿›è¡ŒåŸºäºäººç±»åå¥½çš„ LLM å¯¹é½ã€‚å®ƒå·§å¦™åœ°å‘ç°ï¼ŒRLHF ä¸­çš„å¥–åŠ±æœ€å¤§åŒ–ç›®æ ‡å¯ä»¥è¢«è½¬åŒ–ä¸ºä¸€ä¸ª**ç›´æ¥åŸºäºåå¥½æ•°æ®çš„åˆ†ç±»é—®é¢˜**ã€‚ å®ƒä¸å†éœ€è¦æ˜¾å¼åœ°è®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹ï¼Œä¹Ÿä¸éœ€è¦å¤æ‚çš„ Actor-Critic å¾ªç¯å’Œåœ¨çº¿é‡‡æ ·ã€‚
	

**GRPO (Group Relative Policy Optimization, ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–)**

- **æ ¸å¿ƒæ€æƒ³**ï¼š GRPO å¯ä»¥çœ‹ä½œæ˜¯ PPO çš„ä¸€ç§å˜ä½“æˆ–æ”¹è¿›ï¼Œæ—¨åœ¨æé«˜è®­ç»ƒæ•ˆç‡å’Œç¨³å®šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹æ¨¡å‹ä¸Šã€‚DeepSeekçš„å·¥ä½œä½¿è¿™ä¸ªæ–¹æ³•å—åˆ°å…³æ³¨ã€‚ å®ƒçš„ä¸€ä¸ªæ ¸å¿ƒç‰¹ç‚¹æ˜¯**å°è¯•çœå»æˆ–ç®€åŒ–ä»·å€¼æ¨¡å‹ (Critic)**ï¼Œå¹¶é€šè¿‡ä¸€ç§â€œç»„å†…ç›¸å¯¹æ¯”è¾ƒâ€çš„æ–¹å¼æ¥ä¼°è®¡åŠ¨ä½œçš„ä¼˜åŠ¿ (Advantage)ã€‚
	
- **å·¥ä½œæ–¹å¼**ï¼š
	- **SFT** **å’Œå¥–åŠ±æ¨¡å‹**ï¼šé€šå¸¸ä»ç„¶éœ€è¦ SFT æ¨¡å‹ä½œä¸ºåŸºç¡€ï¼Œå¹¶ä¸”éœ€è¦ä¸€ä¸ªï¼ˆå¯èƒ½æ˜¯é¢„è®­ç»ƒå¥½çš„ï¼Œæˆ–è€…æ­£åœ¨ä¼˜åŒ–çš„ï¼‰å¥–åŠ±æ¨¡å‹æ¥è¯„ä¼°ç”Ÿæˆå†…å®¹çš„è´¨é‡ã€‚
		
	- **ç»„é‡‡æ ·å’Œç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡**ï¼š
		1. å¯¹äºä¸€ä¸ªç»™å®šçš„è¾“å…¥ (prompt)ï¼Œæ¨¡å‹ä¼šç”Ÿæˆ**ä¸€ç»„ (group)** å¤šä¸ªå€™é€‰è¾“å‡ºã€‚
			
		2. å¥–åŠ±æ¨¡å‹å¯¹è¿™ä¸€ç»„ä¸­çš„æ‰€æœ‰è¾“å‡ºè¿›è¡Œæ‰“åˆ†ã€‚
			
		3. GRPO åˆ©ç”¨è¿™äº›ç»„å†…çš„æ‰“åˆ†ä¿¡æ¯æ¥ä¼°è®¡æ¯ä¸ªè¾“å‡ºçš„ç›¸å¯¹ä¼˜åŠ¿ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥å°†ç»„å†…æ‰€æœ‰è¾“å‡ºçš„å¹³å‡å¥–åŠ±ä½œä¸ºåŸºçº¿ (baseline)ï¼Œé«˜äºå¹³å‡å¥–åŠ±çš„è¾“å‡ºè¢«è®¤ä¸ºå…·æœ‰æ­£ä¼˜åŠ¿ï¼Œä½äºå¹³å‡å¥–åŠ±çš„åˆ™å…·æœ‰è´Ÿä¼˜åŠ¿ã€‚
		
	- **ç­–ç•¥æ›´æ–°**ï¼šåŸºäºè¿™ç§ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡æ¥æ›´æ–°ç­–ç•¥ç½‘ç»œ (Actor)ã€‚
		1. å…³é”®åœ¨äºå®ƒå¯èƒ½**ä¸éœ€è¦ä¸€ä¸ªç‹¬ç«‹çš„ã€éœ€è¦è¿­ä»£è®­ç»ƒçš„ä»·å€¼ç½‘ç»œ (Critic)** æ¥ä¼°è®¡çŠ¶æ€ä»·å€¼æˆ–çŠ¶æ€-åŠ¨ä½œä»·å€¼ï¼Œä»è€Œé™ä½äº†å¤æ‚æ€§å’Œè®¡ç®—é‡ã€‚
	
- **ä¼˜ç‚¹**ï¼š
	- **æ•ˆç‡æå‡**ï¼šé€šè¿‡çœå»æˆ–ç®€åŒ–ä»·å€¼æ¨¡å‹ï¼Œå¯ä»¥å‡å°‘å†…å­˜å ç”¨å’Œè®¡ç®—å¼€é”€ï¼Œè¿™å¯¹äºéå¸¸å¤§çš„æ¨¡å‹å°¤å…¶æœ‰åˆ©ã€‚
		
	- **ç¨³å®šæ€§å¯èƒ½æ›´å¥½**ï¼šç»„å†…ç›¸å¯¹æ¯”è¾ƒå¯èƒ½æä¾›æ›´ç¨³å®šçš„å­¦ä¹ ä¿¡å·ã€‚
		
	- ä¸å¥–åŠ±æ¨¡å‹çš„è®­ç»ƒæ–¹å¼æ›´å¥‘åˆï¼šå¥–åŠ±æ¨¡å‹é€šå¸¸ä¹Ÿæ˜¯é€šè¿‡æ¯”è¾ƒæˆå¯¹è¾“å‡ºæ¥è®­ç»ƒçš„ï¼ŒGRPO çš„ç»„å†…æ¯”è¾ƒæ–¹å¼ä¸æ­¤ç±»ä¼¼ã€‚
		

## GRPOçš„æ ¸å¿ƒæ€æƒ³æ˜¯ä»€ä¹ˆï¼Ÿ

> é‚£ä¹ˆä½ å¯èƒ½ä¼šæ³¨æ„åˆ°RLå¾€å¾€æ˜¯åœ¨SFTä¹‹åä½œä¸ºä¸€ç§post-traningçš„æ–¹æ³•ï¼Œé‚£ä¹ˆç›´æ¥RLå¯ä¸å¯ä»¥å‘¢ï¼Ÿ

DeepSeek çš„ç ”ç©¶äººå‘˜åœ¨ä½¿ç”¨çº¯å¼ºåŒ–å­¦ä¹  (RL) è®­ç»ƒ R1-Zero æ—¶ï¼Œå‘ç°äº†ä¸€ä¸ªâ€œé¡¿æ‚Ÿæ—¶åˆ»â€ã€‚è¯¥æ¨¡å‹å­¦ä¼šäº†é€šè¿‡é‡æ–°è¯„ä¼°å…¶åˆå§‹æ–¹æ³•ï¼Œå»¶é•¿å…¶æ€è€ƒæ—¶é—´ï¼Œè€Œæ— éœ€ä»»ä½•äººå·¥æŒ‡å¯¼æˆ–é¢„å…ˆå®šä¹‰çš„æŒ‡ä»¤ã€‚

ä¸€ä¸ªå®Œæ•´çš„GRPOæµç¨‹å¾€å¾€å¦‚ä¸‹ï¼š

1. æ¨¡å‹ç”Ÿæˆå¤šç»„å“åº”
	
2. æ¯ä¸ªå“åº”éƒ½æ ¹æ®æ­£ç¡®æ€§æˆ–ç”±æŸäº›è®¾å®šçš„å¥–åŠ±å‡½æ•°ï¼ˆè€Œä¸æ˜¯ LLM å¥–åŠ±æ¨¡å‹ï¼‰åˆ›å»ºçš„å…¶ä»–æŒ‡æ ‡è¿›è¡Œè¯„åˆ†
	
3. è®¡ç®—è¯¥ç»„çš„å¹³å‡åˆ†æ•°
	
4. æ¯ä¸ªå›ç­”çš„åˆ†æ•°éƒ½ä¼šä¸å°ç»„å¹³å‡åˆ†æ•°è¿›è¡Œæ¯”è¾ƒ
	
5. æ¨¡å‹å¾—åˆ°å¼ºåŒ–ï¼Œæœ‰åˆ©äºè·å¾—æ›´é«˜å¾—åˆ†çš„å›åº”
	

æœ€åˆï¼Œäººä»¬å¿…é¡»æ”¶é›†å¤§é‡æ•°æ®æ¥å¡«å……æ¨ç†è¿‡ç¨‹/æ€ç»´é“¾ã€‚ä½† GRPOï¼ˆDeepSeek ä½¿ç”¨çš„ç®—æ³•ï¼‰æˆ–å…¶ä»–å¼ºåŒ–å­¦ä¹ ç®—æ³•å¯ä»¥å¼•å¯¼æ¨¡å‹è‡ªåŠ¨å±•ç°æ¨ç†èƒ½åŠ›å¹¶åˆ›å»ºæ¨ç†è½¨è¿¹ã€‚ç›¸åï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºè‰¯å¥½çš„å¥–åŠ±å‡½æ•°æˆ–éªŒè¯å™¨ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå®ƒå¾—åˆ°äº†æ­£ç¡®çš„ç­”æ¡ˆï¼Œå°±ç»™å®ƒ 1 åˆ†ã€‚å¦‚æœæŸäº›å•è¯æ‹¼å†™é”™è¯¯ï¼Œå°±å‡ 0.1 åˆ†ã€‚ç­‰ç­‰ï¼æˆ‘ä»¬å¯ä»¥æä¾›å¾ˆå¤šå‡½æ•°æ¥å¥–åŠ±è¿™ä¸ªè¿‡ç¨‹ï¼Œä¹Ÿå°±ä»£æ›¿äº†ä¼ ç»Ÿçš„LLMå¥–åŠ±æ¨¡å‹ï¼ŒèŠ‚çº¦äº†å¤§é‡çš„è®­ç»ƒæˆæœ¬å’Œèµ„æºæ¶ˆè€—ï¼

# RLæ¡†æ¶çš„é€‰æ‹©

## å¸¸è§çš„RLæ¡†æ¶éƒ½æœ‰ä»€ä¹ˆï¼Ÿ

> TRLã€VeRLã€Unslothç­‰

TRLï¼ˆTransformer Reinforcement Learningï¼‰ æ˜¯ç”± Hugging Face å¼€å‘çš„ä¸€å¥—åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning, RLï¼‰çš„è®­ç»ƒå·¥å…·ï¼Œä¸“é—¨ç”¨äºä¼˜åŒ–å’Œå¾®è°ƒå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ GPTã€LLaMA ç­‰ï¼‰ã€‚å®ƒç»“åˆäº† PPOï¼ˆProximal Policy Optimizationï¼‰ ç­‰å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä½¿å¼€å‘è€…èƒ½å¤Ÿé«˜æ•ˆåœ°å¯¹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹è¿›è¡Œ RL å¾®è°ƒï¼Œä»¥ä¼˜åŒ–ç‰¹å®šç›®æ ‡ï¼ˆå¦‚äººç±»åå¥½å¯¹é½ã€ä»»åŠ¡æ€§èƒ½æå‡ç­‰ï¼‰ã€‚
VeRLæ˜¯å­—èŠ‚è·³åŠ¨åœ¨TRLçš„åŸºç¡€ä¹‹ä¸Šé’ˆå¯¹å¤§è§„æ¨¡åˆ†å¸ƒå¼ï¼ˆrayï¼‰è®­ç»ƒåœºæ™¯ä¼˜åŒ–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ŒTRLèƒ½å¹²çš„VeRLéƒ½èƒ½å¹²ï¼Œç›®å‰å¾ˆå¤šå…¬å¸çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶éƒ½æ˜¯åŸºäºVeRlæ”¹çš„ã€‚

ä¸è¿‡è¿™éƒ½ä¸æ˜¯æœ¬æ•™ç¨‹çš„ä¸»è§’ï¼Œæœ¬æ•™ç¨‹çš„ä¸»è§’æ˜¯Unslothï¼Œä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªæ¡†æ¶å‘¢ï¼Ÿæˆ‘å°†å…¶æ€»ç»“æˆäº†ä»¥ä¸‹å‡ ç‚¹ï¼š

1. VeRLä¼˜åŒ–çš„æ˜¯åˆ†å¸ƒå¼åœºæ™¯ï¼Œä¹Ÿå°±æ˜¯å¡çš„æ•°é‡>=2ç”¨èµ·æ¥æ‰ä¼šæ¯”è¾ƒé¡ºæ‰‹ï¼Œè¿™ä¸ç¬¦åˆé¡¹ç›®çš„åˆè¡·
	
2. rayçš„å­¦ä¹ æˆæœ¬å¯¹äºå°ç™½æ¥è®²è¿‡é«˜
	
3. unslothåœ¨å•å¡åœºæ™¯ä¸‹å³å¯å¯¹8bæ¨¡å‹è¿›è¡ŒRLèµ„æºå ç”¨æä½
	

RLåŸç†å¯ä»¥åˆ©ç”¨æå°çš„å­¦ä¹ æˆæœ¬æŒæ¡ï¼Œè¿™ä¸æ˜¯å¥½äº‹æˆåŒå˜›ï¼Œæ¥ä¸‹æ¥å°±ä¸€èµ·çœ‹çœ‹Unslothå§ï¼

## Unsloth - 0åŸºç¡€å­¦ä¹ æœ€ä½³æ¡†æ¶ï¼

![10-01](./images/10-01.png)
**å®˜æ–¹æ–‡æ¡£ï¼š** https://docs.unsloth.ai/
**Githubï¼š** https://github.com/unslothai/unsloth
Unsloth æ˜¯ä¸€ä¸ªæå…¶å¼ºè°ƒèµ„æºèŠ‚çœçš„æ¡†æ¶ï¼ŒæŠŠæ‰€æœ‰çš„èµ„æºèŠ‚çœåšåˆ°äº†æè‡´ï¼Œå…·ä½“æ¥è®²Unslothèƒ½å¤Ÿå°† Llama-3ã€Mistralã€Phi-4 å’Œ Gemma ç­‰å¤§å‹è¯­è¨€æ¨¡å‹çš„å¾®è°ƒé€Ÿåº¦æå‡ 2 å€ï¼Œå†…å­˜å ç”¨å‡å°‘ 70%ï¼Œå¹¶ä¸”å‡†ç¡®ç‡æ²¡æœ‰ä»»ä½•ä¸‹é™ï¼
å®˜æ–¹æ–‡æ¡£éå¸¸å…¨é¢ï¼Œè¯¦ç»†æŒ‡å¯¼äº†å¦‚ä½•è®­ç»ƒè‡ªå·±çš„å®šåˆ¶æ¨¡å‹ã€‚å…¶ä¸­æ¶µç›–äº†å®‰è£…å’Œæ›´æ–° Unslothã€åˆ›å»ºæ•°æ®é›†ã€è¿è¡Œå’Œéƒ¨ç½²æ¨¡å‹ç­‰åŸºæœ¬è¦ç´ ã€‚ Unsloth è®©å¤§å®¶åœ¨æœ¬åœ°æˆ–åœ¨ Google Colab å’Œ Kaggle ç­‰å¹³å°ä¸Šè®­ç»ƒåƒ Llama 3 è¿™æ ·çš„æ¨¡å‹å˜å¾—æå…¶ç®€å•ã€‚Unslothç®€åŒ–äº†æ•´ä¸ªè®­ç»ƒå·¥ä½œæµç¨‹ï¼ŒåŒ…æ‹¬æ¨¡å‹åŠ è½½ã€é‡åŒ–ã€è®­ç»ƒã€è¯„ä¼°ã€è¿è¡Œã€ä¿å­˜ã€å¯¼å‡ºï¼Œä»¥åŠä¸ Ollamaã€llama.cpp å’Œ vLLM ç­‰æ¨ç†å¼•æ“çš„é›†æˆã€‚
Unslothå®šæœŸä¸ Hugging Faceã€Google å’Œ Meta çš„å›¢é˜Ÿåˆä½œï¼Œä»¥ä¿®å¤ LLM è®­ç»ƒå’Œæ¨¡å‹ä¸­çš„é”™è¯¯ã€‚å› æ­¤ï¼Œå½“ä½¿ç”¨ Unsloth è¿›è¡Œè®­ç»ƒæˆ–ä½¿ç”¨æ¨¡å‹æ—¶ï¼Œå¯ä»¥æœŸå¾…è·å¾—æœ€å‡†ç¡®çš„ç»“æœã€‚ Unsloth å…·æœ‰é«˜åº¦å¯å®šåˆ¶æ€§ï¼Œå…è®¸æ›´æ”¹èŠå¤©æ¨¡æ¿æˆ–æ•°æ®é›†æ ¼å¼ç­‰å†…å®¹ã€‚Unslothè¿˜ä¸ºè§†è§‰ã€æ–‡æœ¬è½¬è¯­éŸ³ (TTS)ã€BERTã€å¼ºåŒ–å­¦ä¹  (RL) ç­‰æä¾›äº†é¢„æ„å»ºçš„è„šæœ¬ï¼æ­¤å¤–ï¼ŒUnslothæ”¯æŒæ‰€æœ‰è®­ç»ƒæ–¹æ³•å’Œæ‰€æœ‰åŸºäº Transformer çš„æ¨¡å‹ã€‚

# GRPOå¾®è°ƒQwen3ï¼

> å’³å’³ï¼Œæœ‰ç‚¹å•°å—¦äº†ï¼Œä¸‹é¢å¼€å§‹æ­¥å…¥æ­£é¢˜

æœ¬æ–‡ä½¿ç”¨çš„æµ‹è¯•ç¯å¢ƒä¸ºå•å¼ A100ï¼Œæ˜¾å­˜80GBï¼Œå¯æ ¹æ®éœ€æ±‚åˆ‡æ¢ä¸åŒå‚æ•°é‡çš„æ¨¡å‹ï¼Œå®æµ‹4B 24Gæ˜¾å­˜ is enoughï¼
unslothä½¿Qwen3(8B)å¾®è°ƒé€Ÿåº¦æé«˜2å€ï¼ŒVRAMä½¿ç”¨å‡å°‘70%ï¼Œå¹¶ä¸”æ¯”æ‰€æœ‰ä½¿ç”¨Flash Attention 2çš„ç¯å¢ƒæ”¯æŒé•¿8å€çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚ä½¿ç”¨unslothï¼ŒQwen3â€“30B-A3Bæ¨¡å‹å¯ä»¥èˆ’é€‚åœ°åœ¨ä»…17.5GB VRAMçš„ç¯å¢ƒä¸­è¿è¡Œã€‚
unslothä¸ºQwen3æä¾›äº†Dynamic 2.0é‡åŒ–æ–¹æ³•ï¼Œåœ¨5-shot MMLUå’ŒKLæ•£åº¦åŸºå‡†æµ‹è¯•ä¸­æä¾›æœ€ä½³æ€§èƒ½ã€‚è¿™æ„å‘³ç€å¯ä»¥è¿è¡Œå’Œå¾®è°ƒé‡åŒ–åçš„Qwen3 LLMï¼ŒåŒæ—¶ä¿æŒæœ€å°çš„ç²¾åº¦æŸå¤±ã€‚unslothè¿˜ä¸Šä¼ äº†æ”¯æŒåŸç”Ÿ128Kä¸Šä¸‹æ–‡é•¿åº¦çš„Qwen3ç‰ˆæœ¬ã€‚

## Qwen3\_8b\_GRPO.ipynb

**å®‰è£…è½¯ä»¶åŒ…**

```Python
pip install unsloth==2025.3.19 vllm==0.8.2
```

**å‡†å¤‡æ¨¡å‹ï¼Œè®¾ç½®å‚æ•°**

```Python
from unsloth import FastLanguageModel
import torch

max_seq_length = 2048  # æœ€å¤§åºåˆ—é•¿åº¦ï¼Œå¯ä»¥å¢åŠ ä»¥æ”¯æŒæ›´é•¿çš„æ¨ç†è½¨è¿¹
lora_rank = 32         # LoRA çš„ç§©ï¼Œç§©è¶Šå¤§æ¨¡å‹å¯èƒ½è¶Šæ™ºèƒ½ï¼Œä½†è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ä¼šå˜æ…¢

# ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="Qwen/Qwen3-8B",  # è¦åŠ è½½çš„é¢„è®­ç»ƒæ¨¡å‹åç§°(å®˜ç½‘ç‰ˆæœ¬/ç¦»çº¿ç‰ˆæœ¬/unslothç‰ˆæœ¬å‡å¯)
    max_seq_length=max_seq_length,        # è®¾ç½®æ¨¡å‹çš„æœ€å¤§åºåˆ—é•¿åº¦
    load_in_4bit=False,                   # æ˜¯å¦ä»¥4ä½åŠ è½½æ¨¡å‹ï¼Œå¯¹äºLoRA 16ä½è®­ç»ƒï¼Œè®¾ç½®ä¸ºFalse
    fast_inference=True,                  # æ˜¯å¦å¯ç”¨ vLLM å¿«é€Ÿæ¨ç†
    max_lora_rank=lora_rank,              # è®¾ç½® LoRA çš„æœ€å¤§ç§©
    gpu_memory_utilization=0.7,           # GPUæ˜¾å­˜ä½¿ç”¨ç‡ï¼Œå¦‚æœæ˜¾å­˜ä¸è¶³ (OOM)ï¼Œå¯ä»¥é™ä½æ­¤å€¼
)

# ä¸ºæ¨¡å‹æ·»åŠ  PEFT (Parameter-Efficient Fine-Tuning) é…ç½®ï¼Œè¿™é‡Œä½¿ç”¨ LoRA
model = FastLanguageModel.get_peft_model(
    model,
    r=lora_rank,  # LoRA çš„ç§© (r)ï¼Œé€‰æ‹©ä»»ä½•å¤§äº0çš„æ•°å­—ï¼å»ºè®®å€¼ä¸º 8, 16, 32, 64, 128
    target_modules=[  # éœ€è¦åº”ç”¨LoRAçš„æ¨¡å—åç§°åˆ—è¡¨
        "q_proj", "k_proj", "v_proj", "o_proj",  # æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æŸ¥è¯¢ã€é”®ã€å€¼ã€è¾“å‡ºæŠ•å½±
        "gate_proj", "up_proj", "down_proj",     # å‰é¦ˆç½‘ç»œä¸­çš„é—¨æ§ã€ä¸Šè¡Œå’Œä¸‹è¡ŒæŠ•å½±
    ],
    lora_alpha=lora_rank * 2,  # LoRA çš„ alpha å‚æ•°ï¼Œè®¾ç½®ä¸ºç§©çš„2å€å¯ä»¥åŠ é€Ÿè®­ç»ƒ
    use_gradient_checkpointing="unsloth",  # æ˜¯å¦ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹æŠ€æœ¯ï¼Œ"unsloth" è¡¨ç¤ºä½¿ç”¨å…¶ä¼˜åŒ–ç‰ˆæœ¬ä»¥å‡å°‘æ˜¾å­˜ä½¿ç”¨
    random_state=3407,                   # éšæœºç§å­ï¼Œç”¨äºç¡®ä¿ç»“æœçš„å¯å¤ç°æ€§
)
```

![10-02](./images/10-02.png)
**è®¾ç½®CoTæ€è€ƒæ¨¡ç‰ˆã€è®©æ¨¡å‹å…·å¤‡æ€è€ƒèƒ½åŠ›çš„å¿…ç»ä¹‹è·¯ã€‘**

```Python
# å®šä¹‰ç”¨äºæ ‡è®°æ¨¡å‹æ¨ç†è¿‡ç¨‹å’Œæœ€ç»ˆè§£çš„å­—ç¬¦ä¸²æ ‡è®°
reasoning_start = "<start_working_out>"     # ç”¨äºåŒ…è£¹æ¨¡å‹çš„â€œæ€è€ƒè¿‡ç¨‹â€å¼€å§‹éƒ¨åˆ†
reasoning_end   = "<end_working_out>"       # ç”¨äºåŒ…è£¹æ¨¡å‹çš„â€œæ€è€ƒè¿‡ç¨‹â€ç»“æŸéƒ¨åˆ†
solution_start  = "<SOLUTION>"              # ç”¨äºåŒ…è£¹æœ€ç»ˆè§£ç­”å¼€å§‹éƒ¨åˆ†
solution_end    = "</SOLUTION>"             # ç”¨äºåŒ…è£¹æœ€ç»ˆè§£ç­”ç»“æŸéƒ¨åˆ†

# å®šä¹‰ç³»ç»Ÿæç¤ºè¯ï¼Œå‘Šè¯‰æ¨¡å‹åº”è¯¥å¦‚ä½•ç»„ç»‡å›ç­”ï¼šå…ˆæ¨ç†ï¼Œå†ç»™å‡ºç­”æ¡ˆ
system_prompt = \
f"""You are given a problem.
Think about the problem and provide your working out.
Place it between {reasoning_start} and {reasoning_end}.
Then, provide your solution between {solution_start}{solution_end}"""

# æ„å»ºchat_templateæ¨¡æ¿ï¼Œæ§åˆ¶å¦‚ä½•æ‹¼æ¥prompt
chat_template = \
    "{% if messages[0]['role'] == 'system' %}"\
        "{{ messages[0]['content'] + eos_token }}"\  # å¦‚æœç¬¬ä¸€æ¡æ˜¯systemæç¤ºï¼Œæ‹¼æ¥å®ƒå¹¶æ·»åŠ eosæ ‡è®°
        "{% set loop_messages = messages[1:] %}"\     # å‰©ä¸‹çš„æ¶ˆæ¯è®¾ä¸ºå¾ªç¯ä½“
    "{% else %}"\
        "{{ '{system_prompt}' + eos_token }}"\        # å¦åˆ™ï¼Œæ’å…¥é»˜è®¤system_promptå¹¶æ·»åŠ eos
        "{% set loop_messages = messages %}"\
    "{% endif %}"\
    "{% for message in loop_messages %}"\             # éå†æ‰€æœ‰å¯¹è¯æ¶ˆæ¯
        "{% if message['role'] == 'user' %}"\
            "{{ message['content'] }}"\               # ç”¨æˆ·æ¶ˆæ¯ç›´æ¥æ·»åŠ 
        "{% elif message['role'] == 'assistant' %}"\
            "{{ message['content'] + eos_token }}"\   # assistantæ¶ˆæ¯ååŠ eos
        "{% endif %}"\
    "{% endfor %}"\
    "{% if add_generation_prompt %}{{ '{reasoning_start}' }}"\  # å¦‚æœéœ€è¦ç”Ÿæˆæç¤ºï¼Œæ·»åŠ å¼€å§‹æ€è€ƒæ ‡è®°
    "{% endif %}"

# å°†æ¨¡æ¿ä¸­ä½œä¸ºå­—ç¬¦ä¸²å­˜åœ¨çš„å˜é‡æ›¿æ¢ä¸ºå®é™…å˜é‡å€¼ï¼ˆé¿å…æ¨¡æ¿ä¸­å¼•å·åŒ…ä½å˜é‡åï¼‰
chat_template = chat_template\
    .replace("'{system_prompt}'",   f"'{system_prompt}'")\
    .replace("'{reasoning_start}'", f"'{reasoning_start}'")

# å°†chat_templateåº”ç”¨åˆ°tokenizerï¼ˆå‡è®¾è¿™æ˜¯ä¸€ä¸ªæ”¯æŒchat_templateçš„tokenizerï¼‰
tokenizer.chat_template = chat_template

# æ¨¡æ‹Ÿä¸€æ¬¡tokenizeråº”ç”¨chat_templateçš„è¿‡ç¨‹ï¼ˆä¸è¿›è¡Œtokenizeï¼Œåªå±•ç¤ºç»“æœï¼‰
tokenizer.apply_chat_template([
    {"role" : "user", "content" : "What is 1+1?"},
    {"role" : "assistant", "content" : f"{reasoning_start}I think it's 2.{reasoning_end}{solution_start}2{solution_end}"},
    {"role" : "user", "content" : "What is 2+2?"},
], tokenize = False, add_generation_prompt = True)
```

**åŠ è½½æ•°æ®é›†ã€è¿™é‡Œä½¿ç”¨ä¸€ä¸ªæ•°å­¦æ¨ç†çš„æ•°æ®é›†ã€‘**

```Python
from datasets import load_dataset
import pandas as pd
import numpy as np

dataset = load_dataset("unsloth/OpenMathReasoning-mini", split = "cot")
dataset = dataset.to_pandas()[
    ["expected_answer", "problem", "generated_solution"]
]

is_number = pd.to_numeric(pd.Series(dataset["expected_answer"]), errors = "coerce").notnull()
dataset = dataset.iloc[np.where(is_number)[0]]

dataset
```

**å¤„ç†æ•°æ®é›†çš„æ ¼å¼ï¼ˆSFTçš„è€ç”Ÿå¸¸è°ˆï¼‰**

```Python
def format_dataset(x):
    # ä»è¾“å…¥æ•°æ®ä¸­æå–æœŸæœ›ç­”æ¡ˆå’Œé—®é¢˜å†…å®¹
    expected_answer = x["expected_answer"]
    problem = x["problem"]

    # è·å–æ¨¡å‹ç”Ÿæˆçš„æ¨ç†å†…å®¹ï¼Œå¹¶ç§»é™¤æ—§æ ¼å¼æ ‡ç­¾ <think> å’Œ </think>
    thoughts = x["generated_solution"]
    thoughts = thoughts.replace("<think>", "").replace("</think>", "")

    # å»é™¤æ¨ç†å†…å®¹å·¦å³ä¸¤ç«¯çš„æ¢è¡Œç¬¦å’Œç©ºæ ¼
    thoughts = thoughts.strip()

    # æŒ‰ç…§è‡ªå®šä¹‰æ ¼å¼æ‹¼æ¥æ¨ç†éƒ¨åˆ†å’Œç­”æ¡ˆéƒ¨åˆ†ï¼Œæ’å…¥æ ‡è®°æ ‡ç­¾
    final_prompt = (
        reasoning_start + thoughts + reasoning_end +
        solution_start + expected_answer + solution_end
    )

    # æ„é€ æ ¼å¼åŒ–åçš„å¤šè½®å¯¹è¯åˆ—è¡¨ï¼Œç”¨äºå¾®è°ƒæˆ–æµ‹è¯•å¯¹è¯æ¨¡å‹
    return [
        {"role": "system",    "content": system_prompt},  # ç³»ç»Ÿæç¤ºè¯ï¼ŒæŒ‡å¯¼æ¨¡å‹è¾“å‡ºæ ¼å¼
        {"role": "user",      "content": problem},        # ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
        {"role": "assistant", "content": final_prompt},   # æ¨¡å‹çš„å›å¤ï¼ŒåŒ…å«æ¨ç†è¿‡ç¨‹å’Œç­”æ¡ˆ
    ]

# å°†æ•´ä¸ªæ•°æ®é›†æŒ‰è¡Œåº”ç”¨æ ¼å¼åŒ–å‡½æ•°ï¼Œç”Ÿæˆ Messages å­—æ®µï¼Œé€‚ç”¨äºå¯¹è¯ç±»å¾®è°ƒ
dataset["Messages"] = dataset.apply(format_dataset, axis=1)

tokenizer.apply_chat_template(dataset["Messages"][0], tokenize = False)

dataset["N"] = dataset["Messages"].apply(lambda x: len(tokenizer.apply_chat_template(x)))

dataset = dataset.loc[dataset["N"] <= max_seq_length/2].copy()
dataset.shape

from datasets import Dataset

dataset["text"] = tokenizer.apply_chat_template(dataset["Messages"].values.tolist(), tokenize = False)
dataset = Dataset.from_pandas(dataset)
dataset
```

**å…ˆåšSFT**

```Python
from trl import SFTTrainer, SFTConfig

# åˆ›å»ºä¸€ä¸ªæœ‰ç›‘ç£å¾®è°ƒçš„è®­ç»ƒå™¨å®ä¾‹
trainer = SFTTrainer(
    model = model,                 # é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚ LLaMAã€Qwenã€Mistral ç­‰ï¼‰
    tokenizer = tokenizer,         # ä¸æ¨¡å‹åŒ¹é…çš„ tokenizerï¼Œéœ€æ”¯æŒ chat_template
    train_dataset = dataset,       # ç”¨äºè®­ç»ƒçš„æ•°æ®é›†ï¼Œè¦æ±‚åŒ…å«"text"å­—æ®µ

    # è®­ç»ƒå‚æ•°é…ç½®
    args = SFTConfig(
        dataset_text_field = "text",               # æ•°æ®é›†ä¸­ç”¨äºè®­ç»ƒè¾“å…¥çš„å­—æ®µåï¼ˆé€šå¸¸ä¸º"text"ï¼‰
        per_device_train_batch_size = 1,           # æ¯å¼  GPU ä¸Šçš„ batch size
        gradient_accumulation_steps = 1,           # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆæ€»æœ‰æ•ˆ batch_size = ä¸Šä¸¤è€…ç›¸ä¹˜ï¼‰

        warmup_steps = 5,                          # å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°ï¼Œé¿å…åˆå§‹è¿‡å¿«ä¸‹é™
        num_train_epochs = 2,                      # è®­ç»ƒè½®æ•°

        learning_rate = 2e-4,                      # åˆå§‹å­¦ä¹ ç‡ï¼ˆå»ºè®®é•¿æœŸè®­ç»ƒç”¨ 2e-5 ~ 5e-5ï¼‰
        logging_steps = 5,                         # æ¯ 5 æ­¥æ‰“å°ä¸€æ¬¡æ—¥å¿—ï¼ˆloss ç­‰ï¼‰

        optim = "adamw_8bit",                      # ä½¿ç”¨ 8-bit AdamW ä¼˜åŒ–å™¨ï¼ˆéœ€è¦ bitsandbytes æ”¯æŒï¼‰
        weight_decay = 0.01,                       # æƒé‡è¡°å‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        lr_scheduler_type = "linear",              # çº¿æ€§å­¦ä¹ ç‡è¡°å‡ç­–ç•¥

        seed = 3407,                               # å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿å®éªŒå¯é‡å¤

        report_to = "none",                        # ä¸å°†è®­ç»ƒæ—¥å¿—æŠ¥å‘Šåˆ° WandB ç­‰å·¥å…·ï¼ˆå¦‚éœ€å¼€å¯æ”¹ä¸º"wandb"ï¼‰
    ),
)

trainer.train()
```

**SFTé˜¶æ®µè¾“å‡º**
![10-03](./images/10-03.png)

```Python
# æ„å»ºè¾“å…¥ promptï¼Œé€‰å–å‰ä¸¤æ¡æ¶ˆæ¯ï¼ˆé€šå¸¸ä¸º system + userï¼‰
text = tokenizer.apply_chat_template(
    dataset[0]["Messages"][:2],       # è¾“å…¥å‰ä¸¤æ¡æ¶ˆæ¯ï¼šsystem å’Œ user ç»„æˆçš„ prompt
    tokenize = False,                 # ä¸è¿›è¡Œ token åŒ–ï¼Œè¿”å›çº¯æ–‡æœ¬å­—ç¬¦ä¸²
    add_generation_prompt = True,     # åœ¨ç»“å°¾æ·»åŠ æ¨ç†å¼€å§‹æ ‡è®°ï¼ˆå¦‚ <start_working_out>ï¼‰
)

# ä½¿ç”¨ transformers çš„æµå¼è¾“å‡ºå·¥å…· TextStreamer å®æ—¶æ‰“å°ç”Ÿæˆå†…å®¹
from transformers import TextStreamer
_ = model.generate(
    **tokenizer(text, return_tensors = "pt").to("cuda"),   # ç¼–ç æ–‡æœ¬å¹¶ç§»åŠ¨åˆ° GPU
    temperature = 0,                # ä½¿ç”¨è´ªå©ªè§£ç ï¼ˆtemperature è¶‹è¿‘äº 0ï¼‰
    max_new_tokens = 1024,         # é™åˆ¶ç”Ÿæˆ token æ•°é‡
    streamer = TextStreamer(tokenizer, skip_prompt = False),  # å®æ—¶æ‰“å°ç”Ÿæˆç»“æœ
)

# æ¸…ç†å†…å­˜ï¼Œé˜²æ­¢æ˜¾å­˜æ³„éœ²
del dataset
torch.cuda.empty_cache()
import gc
gc.collect()

# åŠ è½½ä¸€ä¸ªæ•°å­¦å¾®è°ƒæ•°æ®é›†ï¼ˆHuggingFace hub ä¸Šçš„ DAPO-Math-17kï¼‰
from datasets import load_dataset
dataset = load_dataset("open-r1/DAPO-Math-17k-Processed", "en", split = "train")
dataset

# æŸ¥çœ‹ä¸€ä¸ªæ ·æœ¬çš„ prompt å’Œ solution å­—æ®µ
dataset[0]["prompt"]
dataset[0]["solution"]

# æŠ½å–è§£ç­”å‡½æ•°ï¼ˆå¯å®šåˆ¶ï¼Œæ­¤å¤„æš‚æ—¶åŸæ ·è¿”å›ï¼‰
def extract_hash_answer(text):
    # å¯å¯ç”¨ä»¥ä¸‹ä»£ç ç”¨äºå¤„ç†å¸¦æœ‰ "####" åˆ†éš”çš„ç­”æ¡ˆ
    # if "####" not in text: return None
    # return text.split("####")[1].strip()
    return text
extract_hash_answer(dataset[0]["solution"])

# å°†åŸå§‹æ•°æ®æ ¼å¼è½¬ä¸º messages æ ¼å¼ï¼Œé€‚é… SFTTrainer æ‰€éœ€æ ¼å¼
dataset = dataset.map(lambda x: {
    "prompt": [  # å°†ç³»ç»Ÿæç¤ºå’Œç”¨æˆ·è¾“å…¥è½¬ä¸ºæ¶ˆæ¯æ ¼å¼
        {"role": "system", "content": system_prompt},
        {"role": "user",   "content": x["prompt"]},
    ],
    "answer": extract_hash_answer(x["solution"]),  # ç­”æ¡ˆéƒ¨åˆ†å¤„ç†
})
dataset[0]

# ========================
# æå–ç”Ÿæˆæ–‡æœ¬ä¸­ç­”æ¡ˆéƒ¨åˆ†çš„æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å™¨
# ========================

import re

# æ„é€ åŒ¹é…ç»“æŸæ ‡ç­¾ "</SOLUTION>" å’Œå¯é€‰çš„ eos_tokenï¼ˆä¾‹å¦‚ <|endoftext|>ï¼‰
solution_end_regex = r"</SOLUTION>[\s]{0,}" + \
    "(?:" + re.escape(tokenizer.eos_token) + ")?"

# æ„é€ å®Œæ•´çš„åŒ¹é…æ¨¡æ¿ï¼Œç”¨äºæå–æ¨ç†ç»“æœä¸­çš„ç­”æ¡ˆéƒ¨åˆ†
match_format = re.compile(
    rf"{reasoning_end}.*?"\                   # åŒ¹é…æ¨ç†ç»“æŸæ ‡ç­¾ä»¥åŠå…¶åçš„å†…å®¹ï¼ˆéè´ªå©ªï¼‰
    rf"{solution_start}(.+?){solution_end_regex}"\  # æå– <SOLUTION> ä¸ </SOLUTION> ä¹‹é—´çš„å†…å®¹
    rf"[\s]{{0,}}$",                          # åŒ¹é…æœ«å°¾çš„ç©ºç™½
    flags = re.MULTILINE | re.DOTALL         # å¤šè¡ŒåŒ¹é… + ç‚¹å·åŒ¹é…æ¢è¡Œç¬¦
)

# ç¤ºä¾‹ï¼šéªŒè¯æ ¼å¼åŒ¹é…æ˜¯å¦èƒ½æ­£ç¡®æå–è§£ç­”éƒ¨åˆ†
match_format.findall(
    "<start_working_out>Let me think!<end_working_out>"\
    f"<SOLUTION>  2  </SOLUTION>\n\n",
)
# è¾“å‡ºåº”ä¸ºï¼š["2"]
```

![10-04](./images/10-04.png)
**å¥–åŠ±å‡½æ•°éƒ¨åˆ†**

```Python
def match_format_exactly(completions, **kwargs):
    scores = []  # ç”¨äºä¿å­˜æ¯ä¸ª completion çš„å¾—åˆ†

    for completion in completions:
        score = 0
        response = completion[0]["content"]  # è·å–æ¨¡å‹è¾“å‡ºå†…å®¹ï¼ˆå‡è®¾ä¸º messages åˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ª assistant å›å¤ï¼‰

        # å¦‚æœè¾“å‡ºå†…å®¹èƒ½æˆåŠŸåŒ¹é…æŒ‡å®šæ ¼å¼ï¼ˆå³åŒ…å«å®Œæ•´ <start_working_out>...<SOLUTION>... æ ‡ç­¾ç»“æ„ï¼‰
        if match_format.search(response) is not None:
            score += 3.0  # åŒ¹é…æˆåŠŸå¾— 3 åˆ†ï¼ˆç”¨äºå¥–åŠ±æ ¼å¼æ­£ç¡®çš„è¾“å‡ºï¼‰

        scores.append(score)  # ä¿å­˜è¯¥æ¡ completion çš„å¾—åˆ†

    return scores  # è¿”å›æ‰€æœ‰ completion çš„æ ¼å¼åŒ¹é…å¾—åˆ†åˆ—è¡¨
```

```Python
def match_format_approximately(completions, **kwargs):
    scores = []  # å­˜å‚¨æ¯ä¸ª completion çš„è¿‘ä¼¼æ ¼å¼åŒ¹é…å¾—åˆ†

    for completion in completions:
        score = 0
        response = completion[0]["content"]  # è·å–è¯¥æ¡ç”Ÿæˆç»“æœçš„æ–‡æœ¬å†…å®¹

        # æœ¬å‡½æ•°ä¸æ˜¯ç²¾ç¡®åŒ¹é…æ•´æ®µæ¨¡æ¿ï¼Œè€Œæ˜¯æ£€æŸ¥å…³é”®æ ‡ç­¾æ˜¯å¦æ°å¥½å‡ºç°ä¸€æ¬¡
        # è¯„åˆ†æ ‡å‡†å¦‚ä¸‹ï¼ˆæ¯ä¸ªå…³é”®æ ‡ç­¾å‡ºç°ä¸€æ¬¡åŠ  0.5 åˆ†ï¼Œå‡ºç°å¤šæ¬¡æˆ–æ¼æ‰åˆ™å‡ 1 åˆ†ï¼‰ï¼š

        # <start_working_out> ä¸éœ€è¦åˆ¤æ–­ï¼Œå› ä¸ºä¸€èˆ¬åœ¨ prompt ä¸­å·²åŠ ï¼Œæ— éœ€é‡å¤å¥–åŠ±

        score += 0.5 if response.count(reasoning_end)   == 1 else -1.0  # æ£€æŸ¥ <end_working_out>
        score += 0.5 if response.count(solution_start)  == 1 else -1.0  # æ£€æŸ¥ <SOLUTION>
        score += 0.5 if response.count(solution_end)    == 1 else -1.0  # æ£€æŸ¥ </SOLUTION>

        scores.append(score)  # ä¿å­˜è¯¥æ¡ completion çš„è¯„åˆ†ç»“æœ

    return scores  # è¿”å›æ‰€æœ‰æ ·æœ¬çš„è¯„åˆ†ç»“æœåˆ—è¡¨
```

```Python
def check_answer(prompts, completions, answer, **kwargs):
    # è·å–åŸå§‹é—®é¢˜ï¼ˆä¸€èˆ¬ä¸º prompts ä¸­æœ€åä¸€ä¸ª user æ¶ˆæ¯çš„å†…å®¹ï¼‰
    question = prompts[0][-1]["content"]

    # æå–æ¯ä¸ª completion çš„ç”Ÿæˆç»“æœï¼ˆå‡è®¾ä¸º assistant çš„ç¬¬ä¸€æ¡å›å¤ï¼‰
    responses = [completion[0]["content"] for completion in completions]

    # ä»æ¯ä¸ª response ä¸­æå– <SOLUTION> æ ‡ç­¾å†…çš„ç­”æ¡ˆï¼ˆä½¿ç”¨æ­£åˆ™åŒ¹é…ï¼‰
    extracted_responses = [
        guess.group(1)  # å¦‚æœåŒ¹é…æˆåŠŸï¼Œå–å‡ºæ‹¬å·ä¸­çš„ group(1)
        if (guess := match_format.search(r)) is not None else None  # å¦åˆ™ä¸º None
        for r in responses
    ]

    scores = []  # å­˜å‚¨æ¯ä¸ªæ ·æœ¬çš„è¯„åˆ†ç»“æœ
    for guess, true_answer in zip(extracted_responses, answer):
        score = 0
        if guess is None:
            scores.append(-2.0)  # è‹¥æœªæˆåŠŸæå–ç­”æ¡ˆï¼Œç›´æ¥æ‰£åˆ†
            continue

        # æ­£ç¡®ç­”æ¡ˆå®Œå…¨ä¸€è‡´ï¼Œå¥–åŠ± 5 åˆ†
        if guess == true_answer:
            score += 5.0

        # è‹¥å»é™¤ç©ºæ ¼ååŒ¹é…æˆåŠŸï¼Œå¥–åŠ±ç•¥å°‘ï¼ˆ3.5 åˆ†ï¼‰
        elif guess.strip() == true_answer.strip():
            score += 3.5

        # å¦åˆ™ï¼Œå°è¯•è¿›è¡Œâ€œè¿‘ä¼¼æ•°å€¼â€åŒ¹é…
        else:
            try:
                ratio = float(guess) / float(true_answer)  # è½¬æ¢ä¸º float å¹¶è®¡ç®—æ¯”å€¼
                if   ratio >= 0.9 and ratio <= 1.1:
                    score += 2.0  # è¯¯å·®åœ¨ Â±10% å†…
                elif ratio >= 0.8 and ratio <= 1.2:
                    score += 1.5  # è¯¯å·®åœ¨ Â±20% å†…
                else:
                    score -= 2.5  # åå·®å¤ªå¤§ï¼Œæ‰£åˆ†
            except:
                score -= 4.5  # æ— æ³•è½¬ä¸ºæ•°å€¼ï¼ˆå¦‚åŒ…å«æ–‡æœ¬ã€å•ä½ç­‰ï¼‰ï¼Œä¸¥é‡æ‰£åˆ†

        scores.append(score)  # è®°å½•å½“å‰æ ·æœ¬çš„å¾—åˆ†

    return scores  # è¿”å›æ¯ä¸ª completion çš„åˆ†æ•°
```

```Python
match_numbers = re.compile(
    solution_start + r".*?[\s]{0,}([-]?[\d\.\,]{1,})",
    flags = re.MULTILINE | re.DOTALL
)
print(match_numbers.findall("<SOLUTION>  0.34  </SOLUTION>"))
print(match_numbers.findall("<SOLUTION>  123,456  </SOLUTION>"))
print(match_numbers.findall("<SOLUTION>  -0.234  </SOLUTION>"))
print(match_numbers.findall("<SOLUTION>17</SOLUTION>"))
```

```Python
# å…¨å±€æ‰“å°æ§åˆ¶å˜é‡ï¼Œæ¯ N æ­¥æ‰“å°ä¸€æ¬¡æ—¥å¿—ï¼ˆç”¨äºè°ƒè¯•æ—¶æŸ¥çœ‹éƒ¨åˆ†è¾“å‡ºï¼‰
global PRINTED_TIMES
PRINTED_TIMES = 0  # å½“å‰å·²æ‰“å°æ¬¡æ•°

global PRINT_EVERY_STEPS
PRINT_EVERY_STEPS = 5  # æ¯é—´éš”å¤šå°‘æ­¥æ‰“å°ä¸€æ¬¡

# æ•°å€¼åŒ¹é…å‡½æ•°ï¼šä»ç”Ÿæˆç»“æœä¸­æå–æ•°å­—ï¼Œå¹¶ä¸æ­£ç¡®ç­”æ¡ˆè¿›è¡Œæ¯”è¾ƒ
def check_numbers(prompts, completions, answer, **kwargs):
    # è·å–é—®é¢˜æ–‡æœ¬ï¼ˆé€šå¸¸ä¸º prompts ä¸­æœ€åä¸€ä¸ª user æ¶ˆæ¯ï¼‰
    question = prompts[0][-1]["content"]

    # æå–æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹ï¼ˆå‡è®¾æ¯ä¸ª completion æ˜¯ä¸€ä¸ªæ¶ˆæ¯åˆ—è¡¨ï¼Œå–ç¬¬ä¸€æ¡ï¼‰
    responses = [completion[0]["content"] for completion in completions]

    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ match_numbers æå–æ•°å­—
    extracted_responses = [
        guess.group(1)
        if (guess := match_numbers.search(r)) is not None else None \
        for r in responses
    ]

    scores = []  # å­˜å‚¨å¾—åˆ†ç»“æœ

    # æ§åˆ¶æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼ˆæ¯éš” N æ¬¡æ‰“å°ä¸€æ¬¡ï¼Œç”¨äºæŸ¥çœ‹ sample åŒ¹é…ç»“æœï¼‰
    global PRINTED_TIMES
    global PRINT_EVERY_STEPS
    if PRINTED_TIMES % PRINT_EVERY_STEPS == 0:
        print(
            '*'*20 + f"Question:\n{question}",  # æ‰“å°é—®é¢˜
            f"\nAnswer:\n{answer[0]}",           # æ‰“å°å‚è€ƒç­”æ¡ˆ
            f"\nResponse:\n{responses[0]}",      # æ‰“å°æ¨¡å‹ç”Ÿæˆ
            f"\nExtracted:\n{extracted_responses[0]}"  # æ‰“å°æå–ç»“æœ
        )
    PRINTED_TIMES += 1  # æ‰“å°æ¬¡æ•°å¢åŠ 

    # æ ¸å¿ƒè¯„åˆ†é€»è¾‘
    for guess, true_answer in zip(extracted_responses, answer):
        if guess is None:
            scores.append(-2.5)  # æ²¡æå–å‡ºæ•°å­—ï¼Œç›´æ¥æ‰£åˆ†
            continue

        try:
            # å»é™¤ç©ºæ ¼ï¼Œè½¬æ¢ä¸º floatï¼›guess å…ˆå»æ‰åƒä½åˆ†éš”ç¬¦ï¼ˆä¾‹å¦‚ 123,456ï¼‰
            true_answer = float(true_answer.strip())
            guess = float(guess.strip().replace(",", ""))

            # å¦‚æœå®Œå…¨æ•°å€¼ä¸€è‡´ï¼Œå¾—åˆ† 3.5ï¼›å¦åˆ™æ‰£åˆ†
            scores.append(3.5 if guess == true_answer else -1.5)

        except:
            scores.append(0)  # è§£æå¤±è´¥ä¸ç»™åˆ†ä¹Ÿä¸æ‰£åˆ†
            continue

    return scores  # è¿”å›æ‰€æœ‰æ ·æœ¬çš„å¾—åˆ†åˆ—è¡¨
```

```Python
# å¯¹æ•°æ®é›†ä¸­çš„æ¯æ¡æ ·æœ¬ï¼Œåº”ç”¨ tokenizer çš„ chat æ¨¡æ¿å¹¶è¿›è¡Œåˆ†è¯
tokenized = dataset.map(
    lambda x: {
        "tokens": tokenizer.apply_chat_template(
            x["prompt"],                        # è¾“å…¥ä¸º prompt å­—æ®µï¼Œå³æ¶ˆæ¯åˆ—è¡¨ [{"role": ..., "content": ...}, ...]
            add_generation_prompt=True,         # æ·»åŠ ç”Ÿæˆæç¤ºç¬¦ï¼ˆå¦‚ <start_working_out>ï¼‰ï¼Œç”¨äº instruct-style ç”Ÿæˆä»»åŠ¡
            tokenize=True                       # è¿”å› token id åˆ—è¡¨ï¼Œè€Œéå­—ç¬¦ä¸²
        )
    },
    batched=True,  # å¯ç”¨æ‰¹å¤„ç†ï¼Œæé«˜ map çš„æ•ˆç‡
)

# æ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬çš„è§£ç ç»“æœï¼ˆä» token id è½¬æ¢å›å­—ç¬¦ä¸²ï¼‰ï¼Œç”¨äºéªŒè¯æ¨¡æ¿å’Œ tokenization æ˜¯å¦æ­£ç¡®
print(tokenizer.decode(tokenized[0]["tokens"]))

# ä¸ºæ¯æ¡æ ·æœ¬æ·»åŠ  token åºåˆ—é•¿åº¦å­—æ®µ "L"ï¼Œä¾¿äºåç»­é•¿åº¦åˆ†å¸ƒåˆ†æ
tokenized = tokenized.map(lambda x: {"L": len(x["tokens"])})

import numpy as np
# è®¡ç®— token é•¿åº¦çš„ 90% åˆ†ä½æ•°ï¼Œä½œä¸ºè®­ç»ƒæ—¶çš„æœ€å¤§ token é™åˆ¶ï¼ˆé˜²æ­¢æç«¯é•¿æ ·æœ¬å¯¼è‡´ OOMï¼‰
maximum_length = int(np.quantile(tokenized["L"], 0.9))
print("Max Length = ", maximum_length)

# è¿‡æ»¤æ‰ token é•¿åº¦è¶…è¿‡æœ€å¤§é•¿åº¦é˜ˆå€¼çš„æ ·æœ¬ï¼Œä»…ä¿ç•™è¾ƒçŸ­çš„ 90% æ ·æœ¬
dataset = dataset.select(
    np.where(np.array(tokenized["L"]) <= maximum_length)[0]
)

# åˆ é™¤ä¸­é—´å˜é‡ tokenizedï¼Œé‡Šæ”¾å†…å­˜
del tokenized
```

**GRPOéƒ¨åˆ†**

```Python
# è®¡ç®—æç¤ºé•¿åº¦ä¸Šé™ï¼ˆåŠ 1æ˜¯ä¿é™©æªæ–½ï¼Œé˜²æ­¢è¾¹ç•Œé—®é¢˜ï¼‰
max_prompt_length = maximum_length + 1  # +1 æ˜¯ä¸ºäº†é¿å… max_length æˆªæ–­æ—¶è¯¯ä¼¤
max_completion_length = max_seq_length - max_prompt_length  # å‰©ä½™ token ç”¨äºç”Ÿæˆ

# é…ç½® vLLM çš„é‡‡æ ·å‚æ•°ï¼ˆç”¨äºç”Ÿæˆè®­ç»ƒæ ·æœ¬ï¼‰
from vllm import SamplingParams
vllm_sampling_params = SamplingParams(
    min_p = 0.1,                    # nucleus sampling çš„æˆªæ–­ä¸‹ç•Œ
    top_p = 1.0,                    # nucleus sampling çš„ä¸Šé™ï¼ˆtop-p samplingï¼‰
    top_k = -1,                     # ä¸å¯ç”¨ top-k æˆªæ–­ï¼ˆ-1 è¡¨ç¤ºå…³é—­ï¼‰
    seed = 3407,                    # å›ºå®šéšæœºç§å­ï¼Œä¿è¯ç”Ÿæˆç»“æœå¯å¤ç°
    stop = [tokenizer.eos_token],   # ç”Ÿæˆåœæ­¢æ ‡å¿—ï¼ˆé€šå¸¸æ˜¯ <|endoftext|>ï¼‰
    include_stop_str_in_output = True,  # æ˜¯å¦å°† stop token ä¹ŸåŒ…å«åœ¨è¾“å‡ºä¸­
)
# é…ç½® GRPOï¼ˆGeneralized Reinforcement Preference Optimizationï¼‰è®­ç»ƒå‚æ•°
from trl import GRPOConfig, GRPOTrainer
training_args = GRPOConfig(
    vllm_sampling_params = vllm_sampling_params,  # ç”¨äºç”Ÿæˆ completion çš„é‡‡æ ·ç­–ç•¥
    temperature = 1.0,               # ç”Ÿæˆçš„å¤šæ ·æ€§æ§åˆ¶ï¼ˆé€šå¸¸è®¾ä¸º 0.7 ~ 1.0ï¼‰
    learning_rate = 5e-6,            # è®­ç»ƒçš„å­¦ä¹ ç‡ï¼ˆè¾ƒå°ä»¥ä¿è¯ç¨³å®šæ”¶æ•›ï¼‰
    weight_decay = 0.01,             # æƒé‡è¡°å‡ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ
    warmup_ratio = 0.1,              # warmup æ­¥æ•°å æ€»è®­ç»ƒæ­¥æ•°çš„æ¯”ä¾‹ï¼ˆé€šå¸¸ä¸º 0.05 ~ 0.1ï¼‰
    lr_scheduler_type = "linear",    # å­¦ä¹ ç‡è°ƒåº¦æ–¹å¼ä¸ºçº¿æ€§ä¸‹é™
    optim = "adamw_8bit",            # ä½¿ç”¨ bitsandbytes çš„ 8bit AdamW ä¼˜åŒ–å™¨ï¼ˆçœæ˜¾å­˜ï¼‰

    logging_steps = 1,               # æ¯ä¸€æ­¥æ‰“å°ä¸€æ¬¡æ—¥å¿—ï¼ˆé€‚åˆ debugï¼‰
    per_device_train_batch_size = 1, # æ¯å¼  GPU çš„ batch size
    gradient_accumulation_steps = 1, # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆè®¾ä¸º 4 å¯ç­‰æ•ˆ batch size=4ï¼‰

    num_generations = 4,             # æ¯ä¸ª prompt ç”Ÿæˆå¤šå°‘ä¸ª responseï¼ˆè¶Šå¤šè¶Šå¥½ï¼Œä½†æ˜¾å­˜æ¶ˆè€—æ›´å¤§ï¼‰
    max_prompt_length = max_prompt_length,         # æç¤º token æœ€å¤§é•¿åº¦ï¼ˆå‰é¢è®¡ç®—å¾—å‡ºï¼‰
    max_completion_length = max_completion_length, # å›ç­” token æœ€å¤§é•¿åº¦ï¼ˆç¡®ä¿æ€»é•¿ä¸è¶…è¿‡æ¨¡å‹é™åˆ¶ï¼‰

    # num_train_epochs = 1,          # å¯é€‰å‚æ•°ï¼šè®­ç»ƒè½®æ¬¡ï¼Œè®¾ç½®åå¯æŒ‰ epoch æ§åˆ¶è®­ç»ƒç»ˆæ­¢
    max_steps = 100,                 # æœ€å¤§è®­ç»ƒæ­¥æ•°ï¼ˆé€‚åˆè°ƒè¯•ç”¨ï¼Œæ­£å¼è®­ç»ƒæ—¶å¯åŠ å¤§ï¼‰
    save_steps = 100,                # æ¯ 100 æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹ï¼ˆè®­ç»ƒçŸ­æ—¶å¯ä»¥ä¸å­˜ï¼‰

    report_to = "none",              # ä¸ä¸Šä¼ è®­ç»ƒæ—¥å¿—ï¼ˆå¯è®¾ç½®ä¸º "wandb" ä½¿ç”¨å¯è§†åŒ–å·¥å…·ï¼‰
    output_dir = "outputs",          # æ¨¡å‹è¾“å‡ºè·¯å¾„
```

```Python
# åˆ›å»º GRPOTrainer å®ä¾‹ï¼Œç”¨äºæ‰§è¡Œå¼ºåŒ–å­¦ä¹ å¼åå¥½ä¼˜åŒ–è®­ç»ƒï¼ˆGeneralized RPOï¼‰
trainer = GRPOTrainer(
    model = model,  # éœ€è¦è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼ˆå¿…é¡»æ”¯æŒ causal LM æ ¼å¼ï¼Œå¦‚ GPTã€LLaMA ç­‰ï¼‰

    processing_class = tokenizer,  # ç”¨äºç”Ÿæˆ promptã€è§£ç  response çš„ tokenizerï¼ˆéœ€æ”¯æŒ chat_templateï¼‰

    reward_funcs = [  # è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°åˆ—è¡¨ï¼Œç”¨äºè®¡ç®—æ¯æ¡æ ·æœ¬çš„å¾—åˆ†
        match_format_exactly,       # æ£€æŸ¥æ˜¯å¦ä¸¥æ ¼ç¬¦åˆ <start_working_out>...<SOLUTION> æ ¼å¼ï¼ŒåŒ¹é…å¾—åˆ†é«˜
        match_format_approximately, # æ£€æŸ¥æ˜¯å¦å¤§è‡´æœ‰æ ¼å¼æ ‡ç­¾ï¼Œå®½æ¾è¯„åˆ†
        check_answer,               # ä¸å‚è€ƒç­”æ¡ˆå¯¹æ¯”ï¼Œè¿›è¡Œç²¾ç¡®åŒ¹é…ã€æ¨¡ç³ŠåŒ¹é…ã€è¿‘ä¼¼æ•°å€¼åŒ¹é…
        check_numbers,              # æå–æ•°å€¼è¿›è¡Œæ¯”è¾ƒï¼ˆç”¨äºæ•°å­¦é¢˜ï¼‰
    ],

    args = training_args,  # è®­ç»ƒå‚æ•°é…ç½®ï¼ˆä½¿ç”¨å‰é¢å®šä¹‰å¥½çš„ GRPOConfig å¯¹è±¡ï¼‰

    train_dataset = dataset,  # å®é™…ç”¨äºè®­ç»ƒçš„æ•°æ®é›†

    # å¯é€‰ï¼šå¦‚å¯ç”¨è®­ç»ƒ + éªŒè¯è¯„ä¼°ï¼Œå¯æ›¿æ¢ä¸ºå¦‚ä¸‹é…ç½®
    # train_dataset = new_dataset["train"],
    # eval_dataset = new_dataset["test"],
)
```

![10-05](./images/10-05.png)
**è®­ç»ƒå®Œæ¯•åè°ƒç”¨æ¨¡å‹**

```Python
# Step 1: æ„é€ åˆå§‹è¾“å…¥å¹¶ç”Ÿæˆè¾“å‡ºï¼ˆä¸åŠ è½½ LoRAï¼‰
text = "What is the sqrt of 101?"

from vllm import SamplingParams

# è®¾ç½®ç”Ÿæˆå‚æ•°ï¼ˆé€‚åº¦éšæœºï¼Œé™åˆ¶é•¿åº¦ï¼‰
sampling_params = SamplingParams(
    temperature = 1.0,
    top_k = 50,
    max_tokens = 1024,
)

# ä½¿ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œå¿«é€Ÿæ¨ç†ï¼ˆæœªåŠ è½½ LoRAï¼‰
output = model.fast_generate(
    [text],  # è¾“å…¥ä¸ºå•æ¡æ–‡æœ¬
    sampling_params=sampling_params,
    lora_request=None,  # ä¸åŠ è½½ä»»ä½• LoRA æƒé‡
)[0].outputs[0].text

print("Original model output:\n", output)

# Step 2: ä¿å­˜ GRPO å¾®è°ƒå¾—åˆ°çš„ LoRA æƒé‡
model.save_lora("grpo_saved_lora")

# Step 3: æ£€æŸ¥ä¿å­˜çš„ safetensors æƒé‡ä¸ä¸ºå…¨é›¶
from safetensors import safe_open

with safe_open("grpo_saved_lora/adapter_model.safetensors", framework="pt") as f:
    for key in f.keys():
        tensor = f.get_tensor(key)
        n_zeros = (tensor == 0).sum() / tensor.numel()
        assert n_zeros.item() != 1.0, f"Tensor {key} is entirely zero!"

print("LoRA weights saved and verified.")

# Step 4: æ„é€ æ¶ˆæ¯æ ¼å¼è¾“å…¥å¹¶åº”ç”¨ tokenizer çš„ chat_template
messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": "What is the sqrt of 101?"},
]

# æ„é€ å¯¹è¯å¼æ–‡æœ¬è¾“å…¥ï¼Œç”¨äº instruct-style æ¨ç†
text = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,  # æ·»åŠ æ¨ç†èµ·å§‹æ ‡è®°
    tokenize=False               # è¿”å›å­—ç¬¦ä¸²è€Œé token ids
)

# Step 5: åŠ è½½å¾®è°ƒåçš„ LoRA å¹¶ç”Ÿæˆè¾“å‡º
sampling_params = SamplingParams(
    temperature=1.0,
    top_k=50,
    max_tokens=2048,
)

# åŠ è½½ LoRA å¹¶è¿›è¡Œæ¨ç†
output = model.fast_generate(
    text,
    sampling_params=sampling_params,
    lora_request=model.load_lora("grpo_saved_lora"),  # åŠ è½½ GRPO å¾®è°ƒæƒé‡
)[0].outputs[0].text

print("GRPO-tuned model output:\n", output)
```

**ä¿å­˜æ¨¡å‹**

```Python
# åˆå¹¶ä¸º 16bit æƒé‡å¹¶ä¿å­˜æœ¬åœ°ï¼ˆé€‚ç”¨äºå…¨ç²¾åº¦éƒ¨ç½²ï¼‰
if False:
    model.save_pretrained_merged("model", tokenizer, save_method="merged_16bit")
# åˆå¹¶ä¸º 16bit æƒé‡å¹¶ä¸Šä¼ è‡³ HuggingFace Hubï¼ˆéœ€å¡«å†™ tokenï¼‰
if False:
    model.push_to_hub_merged("hf/model", tokenizer, save_method="merged_16bit", token="")
# åˆå¹¶ä¸º 4bit é‡åŒ–æƒé‡å¹¶ä¿å­˜æœ¬åœ°ï¼ˆé€‚ç”¨äºèŠ‚çœæ˜¾å­˜çš„éƒ¨ç½²åœºæ™¯ï¼Œå¦‚ QLoRA æ¨ç†ï¼‰
if False:
    model.save_pretrained_merged("model", tokenizer, save_method="merged_4bit")
# åˆå¹¶ä¸º 4bit é‡åŒ–æƒé‡å¹¶ä¸Šä¼ è‡³ HuggingFace Hubï¼ˆéœ€å¡«å†™ tokenï¼‰
if False:
    model.push_to_hub_merged("hf/model", tokenizer, save_method="merged_4bit", token="")
# ä»…ä¿å­˜ LoRA Adapter å‚æ•°ï¼ˆé€‚ç”¨äºåªä¸Šä¼ å¾®è°ƒéƒ¨åˆ†ä»¥èŠ‚çœç©ºé—´æˆ–ç”¨äº PEFT åŠ è½½ï¼‰
if False:
    model.save_pretrained_merged("model", tokenizer, save_method="lora")
# ä»…ä¸Šä¼  LoRA Adapter å‚æ•°è‡³ HuggingFace Hubï¼ˆéœ€å¡«å†™ tokenï¼‰
if False:
    model.push_to_hub_merged("hf/model", tokenizer, save_method="lora", token="")
```

```Python
# ä¿å­˜ä¸º 8bit Q8_0 GGUF æ ¼å¼ï¼ˆé€‚ç”¨äº GGUF é‡åŒ–æ¨¡å‹æ¨ç†ï¼Œå¦‚ llama.cppï¼‰
if False:
    model.save_pretrained_gguf("model", tokenizer)
# ä¸Šä¼  Q8_0 é‡åŒ–æ¨¡å‹åˆ° HuggingFace Hub
# è¯·å‰å¾€ https://huggingface.co/settings/tokens è·å–è®¿é—®ä»¤ç‰Œï¼ˆtokenï¼‰
# å¹¶å°† "hf" æ›¿æ¢ä¸ºä½ çš„ç”¨æˆ·å
if False:
    model.push_to_hub_gguf("hf/model", tokenizer, token="")
# ä¿å­˜ä¸º 16bit GGUF æ ¼å¼ï¼ˆå³æœªé‡åŒ–ç‰ˆæœ¬ï¼Œä¿ç•™å®Œæ•´ç²¾åº¦ï¼Œé€‚ç”¨äºç²¾åº¦æ•æ„Ÿä»»åŠ¡ï¼‰
if False:
    model.save_pretrained_gguf("model", tokenizer, quantization_method="f16")
# ä¸Šä¼  16bit GGUF æ¨¡å‹åˆ° HuggingFace Hub
if False:
    model.push_to_hub_gguf("hf/model", tokenizer, quantization_method="f16", token="")
# ä¿å­˜ä¸º q4_k_mï¼ˆ4bit K-typeï¼‰GGUF æ ¼å¼ï¼Œé€‚ç”¨äºæ¨ç†æ•ˆç‡ä¸ç²¾åº¦ä¹‹é—´çš„å¹³è¡¡
if False:
    model.save_pretrained_gguf("model", tokenizer, quantization_method="q4_k_m")
# ä¸Šä¼  q4_k_m æ¨¡å‹åˆ° HuggingFace Hub
if False:
    model.push_to_hub_gguf("hf/model", tokenizer, quantization_method="q4_k_m", token="")
# ä¸€æ¬¡æ€§ä¸Šä¼ å¤šä¸ª GGUF æ ¼å¼ç‰ˆæœ¬ï¼Œé€Ÿåº¦æ›´å¿«ï¼ˆé€‚åˆéƒ¨ç½²å¤šä¸ªç²¾åº¦ç‰ˆæœ¬ï¼‰
if False:
    model.push_to_hub_gguf(
        "hf/model",  # æ›¿æ¢ "hf" ä¸ºä½ çš„ HuggingFace ç”¨æˆ·å
        tokenizer,
        quantization_method=["q4_k_m", "q8_0", "q5_k_m"],  # å¯æ ¹æ®éœ€è¦è°ƒæ•´æ ¼å¼
        token=""
    )
```

## Swanlab

![10-06](./images/10-06.png)

> ++[SwanLab](https://github.com/swanhubx/swanlab)++ æ˜¯ä¸€ä¸ªå¼€æºçš„æ¨¡å‹è®­ç»ƒè®°å½•å·¥å…·ï¼Œé¢å‘ AI ç ”ç©¶è€…ï¼Œæä¾›äº†è®­ç»ƒå¯è§†åŒ–ã€è‡ªåŠ¨æ—¥å¿—è®°å½•ã€è¶…å‚æ•°è®°å½•ã€å®éªŒå¯¹æ¯”ã€å¤šäººååŒç­‰åŠŸèƒ½ã€‚åœ¨ `SwanLab` ä¸Šï¼Œç ”ç©¶è€…èƒ½åŸºäºç›´è§‚çš„å¯è§†åŒ–å›¾è¡¨å‘ç°è®­ç»ƒé—®é¢˜ï¼Œå¯¹æ¯”å¤šä¸ªå®éªŒæ‰¾åˆ°ç ”ç©¶çµæ„Ÿï¼Œå¹¶é€šè¿‡åœ¨çº¿é“¾æ¥çš„åˆ†äº«ä¸åŸºäºç»„ç»‡çš„å¤šäººååŒè®­ç»ƒï¼Œæ‰“ç ´å›¢é˜Ÿæ²Ÿé€šçš„å£å’ã€‚

### ä¸ºä»€ä¹ˆè¦è®°å½•è®­ç»ƒï¼Ÿ

ç›¸è¾ƒäºè½¯ä»¶å¼€å‘ï¼Œæ¨¡å‹è®­ç»ƒæ›´åƒä¸€ä¸ªå®éªŒç§‘å­¦ã€‚ä¸€ä¸ªå“è´¨ä¼˜ç§€çš„æ¨¡å‹èƒŒåï¼Œå¾€å¾€æ˜¯æˆåƒä¸Šä¸‡æ¬¡å®éªŒã€‚ç ”ç©¶è€…éœ€è¦ä¸æ–­å°è¯•ã€è®°å½•ã€å¯¹æ¯”ï¼Œç§¯ç´¯ç»éªŒï¼Œæ‰èƒ½æ‰¾åˆ°æœ€ä½³çš„æ¨¡å‹ç»“æ„ã€è¶…å‚æ•°ä¸æ•°æ®é…æ¯”ã€‚åœ¨è¿™ä¹‹ä¸­ï¼Œå¦‚ä½•é«˜æ•ˆè¿›è¡Œè®°å½•ä¸å¯¹æ¯”ï¼Œå¯¹äºç ”ç©¶æ•ˆç‡çš„æå‡è‡³å…³é‡è¦ã€‚

### åœ¨å“ªé‡Œç”¨ï¼Ÿ

å»ºè®®å…ˆåœ¨ ++[SwanLab å®˜ç½‘](https://swanlab.cn/)++ æ³¨å†Œè´¦å·ï¼Œç„¶ååœ¨SFTå’ŒGRPOè®­ç»ƒåˆå§‹åŒ–é˜¶æ®µé€‰æ‹©

```Python
from trl import SFTTrainer, SFTConfig
trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    args = SFTConfig(
        dataset_text_field = "text",
        per_device_train_batch_size = 1,
        gradient_accumulation_steps = 1, 
        warmup_steps = 5,
        num_train_epochs = 2,
        learning_rate = 2e-4, 
        logging_steps = 5,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        report_to = "swanlab", # è¿™é‡Œæ”¹æˆswanlab
    ),
)
```

```Python
max_prompt_length = maximum_length + 1
max_completion_length = max_seq_length - max_prompt_length

from vllm import SamplingParams
vllm_sampling_params = SamplingParams(
    min_p = 0.1,
    top_p = 1.0,
    top_k = -1,
    seed = 3407,
    stop = [tokenizer.eos_token],
    include_stop_str_in_output = True,
)

from trl import GRPOConfig, GRPOTrainer
training_args = GRPOConfig(
    vllm_sampling_params = vllm_sampling_params,
    temperature = 1.0,
    learning_rate = 5e-6,
    weight_decay = 0.01,
    warmup_ratio = 0.1,
    lr_scheduler_type = "linear",
    optim = "adamw_8bit",
    logging_steps = 1,
    per_device_train_batch_size = 1,
    gradient_accumulation_steps = 1, # Increase to 4 for smoother training
    num_generations = 4, # Decrease if out of memory
    max_prompt_length = max_prompt_length,
    max_completion_length = max_completion_length,
    # num_train_epochs = 1, # Set to 1 for a full training run
    max_steps = 100,
    save_steps = 100,
    report_to = "swanlab", # è¿™é‡Œæ”¹æˆswanlab
    output_dir = "outputs",

    # å¯ä»¥é€‰æ‹©è®­ç»ƒ+è¯„ä¼°
    # fp16_full_eval = True,
    # per_device_eval_batch_size = 4,
    # eval_accumulation_steps = 1,
    # eval_strategy = "steps",
    # eval_steps = 1,
)
```

### æœ¬è¯•éªŒçš„è¯•éªŒè®°å½•

#### SFTé˜¶æ®µ

![10-07](./images/10-07.png)

#### GRPOé˜¶æ®µ

![10-08](./images/10-08.png)
400ä¸ªstepä¹‹ålossä¼šæœ‰æ˜æ˜¾å˜åŒ–

# æ€»ç»“

Congratulationsï¼çœ‹åˆ°äº†è¿™ï¼Œä½ å·²ç»åˆæ­¥å®ç°äº†ä¸€ä¸ªç®€å•çš„RLå®æˆ˜ï¼ŒæŒæ¡äº†ä½¿ç”¨ Unsloth å¯¹ Qwen3 è¿™ç±»å¤§æ¨¡å‹è¿›è¡Œ GRPO å¾®è°ƒçš„å…·ä½“æ“ä½œæ­¥éª¤ï¼Œæ›´èƒ½ä½“ä¼šåˆ° Unsloth åœ¨å¤§å¹…æå‡è®­ç»ƒé€Ÿåº¦ã€æ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨æ–¹é¢çš„å¼ºå¤§ä¼˜åŠ¿ï¼Œä»è€Œä½¿åœ¨æœ‰é™èµ„æºä¸‹è¿›è¡Œå¤æ‚å¼ºåŒ–å­¦ä¹ å®éªŒæˆä¸ºå¯èƒ½ï¼å¦‚æœæ”¯æŒæˆ‘ä»¬çš„å·¥ä½œå¸Œæœ›å¾—åˆ°ä½ çš„starï¼ï¼è¿™æ˜¯æˆ‘ä»¬æŒç»­æ›´æ–°çš„æœ€å¤§åŠ¨åŠ›ï¼ï¼ï¼

# ç›¸å…³é“¾æ¥

- å®Œæ•´å¯è¿è¡Œçš„ä»£ç ï¼š[Github](https://github.com/datawhalechina/self-llm/blob/master/models/Qwen3/10-Qwen3_8B_GRPO.ipynb)
- ç»¼è¿°ï¼šhttps://arxiv.org/abs/2001.06921
- deepseek-r1ï¼šhttps://arxiv.org/abs/2501.12948
- æ•°å­¦åŸç†ï¼šhttps://blog.csdn.net/weixin\_38991876/article/details/146474767
- Unslothï¼šhttps://docs.unsloth.ai/
