# 06-Qwen2.5-7B-Instruct o1-like æ¨ç†é“¾å®ç°

## **OpenAI o1 model ç®€ä»‹**

**OpenAI o1** ç³»åˆ—æ¨¡å‹æ˜¯ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹ï¼Œç”¨äºæ‰§è¡Œå¤æ‚æ¨ç†ã€‚o1 æ¨¡å‹åœ¨å›ç­”ä¹‹å‰ä¼šæ€è€ƒï¼Œåœ¨å‘ç”¨æˆ·åšå‡ºå›åº”ä¹‹å‰å¯ä»¥äº§ç”Ÿä¸€ä¸ªé•¿çš„å†…éƒ¨æ€ç»´é“¾ã€‚ o1 æ¨¡å‹åœ¨ç§‘å­¦æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œåœ¨ç¼–ç¨‹ç«èµ›ï¼ˆCodeforcesï¼‰ä¸­æ’å 89%ï¼Œåœ¨ç¾å›½æ•°å­¦å¥¥æ—åŒ¹å…‹ç«èµ›ï¼ˆAIMEï¼‰çš„é¢„é€‰ä¸­ä½åˆ—å‰ 500 åï¼Œåœ¨ç‰©ç†ã€ç”Ÿç‰©å­¦å’ŒåŒ–å­¦é—®é¢˜åŸºå‡†æµ‹è¯•ï¼ˆGPQAï¼‰ä¸­è¶…è¶Šäº†äººç±»åšå£«æ°´å¹³ã€‚

![06-1](./images/06-1.png)

**OpenAI o1** ç³»åˆ—æ¨¡å‹ä¸ä»…æé«˜äº†æ¨¡å‹çš„å®ç”¨æ€§ï¼Œè¿˜ä¸ºæœªæ¥AIæŠ€æœ¯çš„å‘å±•å¼€è¾Ÿäº†æ–°çš„é“è·¯ã€‚ç›®å‰ï¼Œo1æ¨¡å‹åŒ…æ‹¬ `o1-preview` å’Œ` o1-mini` ä¸¤ä¸ªç‰ˆæœ¬ï¼Œå…¶ä¸­ `o1-preview` é€‚ç”¨äºè§£å†³å„ä¸ªé¢†åŸŸçš„å¤æ‚é—®é¢˜ï¼Œè€Œ `o1-mini` åˆ™é€Ÿåº¦æ›´å¿«ï¼Œæ€§ä»·æ¯”æ›´é«˜ï¼Œä¸”æ›´æ“…é•¿ä»£ç é¢†åŸŸã€‚

![06-2](./images/06-2.png)

å‚è€ƒæ–‡æ¡£ï¼š 

[Reasoning - OpenAI API](https://platform.openai.com/docs/guides/reasoning/quickstart)

[o1 System Card | OpenAI](https://openai.com/index/openai-o1-system-card/)



## ç¯å¢ƒå‡†å¤‡  

æœ¬æ–‡åŸºç¡€ç¯å¢ƒå¦‚ä¸‹ï¼š

```
----------------
ubuntu 22.04
python 3.12
cuda 12.1
pytorch 2.3.0
----------------
```

> æœ¬æ–‡é»˜è®¤å­¦ä¹ è€…å·²é…ç½®å¥½ä»¥ä¸Š `Pytorch (cuda)` ç¯å¢ƒï¼Œå¦‚æœªé…ç½®è¯·å…ˆè‡ªè¡Œå®‰è£…ã€‚

é¦–å…ˆ `pip` æ¢æºåŠ é€Ÿä¸‹è½½å¹¶å®‰è£…ä¾èµ–åŒ…

```bash
python -m pip install --upgrade pip
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

pip install modelscope==1.18.0
pip install openai==1.46.0
pip install tqdm==4.66.2
pip install transformers==4.44.2
pip install vllm==0.6.1.post2
pip install streamlit==1.38.0
```

> è€ƒè™‘åˆ°éƒ¨åˆ†åŒå­¦é…ç½®ç¯å¢ƒå¯èƒ½ä¼šé‡åˆ°ä¸€äº›é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨AutoDLå¹³å°å‡†å¤‡äº† `Qwen2.5` çš„ç¯å¢ƒé•œåƒï¼Œç‚¹å‡»ä¸‹æ–¹é“¾æ¥å¹¶ç›´æ¥åˆ›å»º `AutoDL` ç¤ºä¾‹å³å¯ã€‚
> ***https://www.codewithgpu.com/i/datawhalechina/self-llm/Qwen2.5-self-llm***



## æ¨¡å‹ä¸‹è½½  

ä½¿ç”¨ `modelscope` ä¸­çš„ `snapshot_download` å‡½æ•°ä¸‹è½½æ¨¡å‹ï¼Œç¬¬ä¸€ä¸ªå‚æ•°ä¸ºæ¨¡å‹åç§°ï¼Œå‚æ•° `cache_dir`ä¸ºæ¨¡å‹çš„ä¸‹è½½è·¯å¾„ã€‚

å…ˆåˆ‡æ¢åˆ° `autodl-tmp` ç›®å½•ï¼Œ`cd /root/autodl-tmp` 

ç„¶åæ–°å»ºåä¸º `model_download.py` çš„ `python` è„šæœ¬ï¼Œå¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹å¹¶ä¿å­˜

```python
# model_download.py
from modelscope import snapshot_download
model_dir = snapshot_download('qwen/Qwen2.5-7B-Instruct', cache_dir='/root/autodl-tmp', revision='master')
```

ç„¶ååœ¨ç»ˆç«¯ä¸­è¾“å…¥ `python model_download.py` æ‰§è¡Œä¸‹è½½ï¼Œè¿™é‡Œéœ€è¦è€å¿ƒç­‰å¾…ä¸€æ®µæ—¶é—´ç›´åˆ°æ¨¡å‹ä¸‹è½½å®Œæˆã€‚

> æ³¨æ„ï¼šè®°å¾—ä¿®æ”¹ `cache_dir` ä¸ºä½ çš„æ¨¡å‹ä¸‹è½½è·¯å¾„å“¦~

![03-1](./images/03-1.png)



## **ä»£ç å‡†å¤‡**

#### **æ ¸å¿ƒä»£ç **

åœ¨ `/root/autodl-tmp` è·¯å¾„ä¸‹æ–°å»º `app_qwen.py` æ–‡ä»¶å¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼Œç²˜è´´ä»£ç åè¯·åŠæ—¶ä¿å­˜æ–‡ä»¶ã€‚

```python
# app_qwen.py
import os
import json
import time
import streamlit as st
from openai import OpenAI

client = OpenAI(
    api_key="sk-xxx",
    base_url="http://localhost:8000/v1",
)

def make_api_call(messages, max_tokens, is_final_answer=False):
    for attempt in range(3):
        try:
            response = client.chat.completions.create(
                model="Qwen2.5-7B-Instruct",
                messages=messages,
                max_tokens=max_tokens,
                temperature=0.2,
                response_format={"type": "json_object"}
            )
            content = response.choices[0].message.content
            print(f"Raw API response: {content}")  # æ·»åŠ æ­¤è¡Œæ¥æ‰“å°åŸå§‹å“åº”
            try:
                return json.loads(content)
            except json.JSONDecodeError as json_error:
                print(f"JSONè§£æé”™è¯¯: {json_error}")
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªåŒ…å«åŸå§‹å†…å®¹çš„å­—å…¸
                return {
                    "title": "API Response",
                    "content": content,
                    "next_action": "final_answer" if is_final_answer else "continue"
                }
        except Exception as e:
            if attempt == 2:
                return {
                    "title": "Error",
                    "content": f"Failed after 3 attempts. Error: {str(e)}",
                    "next_action": "final_answer"
                }
            time.sleep(1)  # é‡è¯•å‰ç­‰å¾…1ç§’

def generate_response(prompt):
    messages = [
        {"role": "system", "content": """
        ä½ æ˜¯ä¸€ä½å…·æœ‰é«˜çº§æ¨ç†èƒ½åŠ›çš„ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æä¾›è¯¦ç»†çš„ã€é€æ­¥çš„æ€ç»´è¿‡ç¨‹è§£é‡Šã€‚å¯¹äºæ¯ä¸€æ­¥:
        1. æä¾›ä¸€ä¸ªæ¸…æ™°ã€ç®€æ´çš„æ ‡é¢˜,æè¿°å½“å‰çš„æ¨ç†é˜¶æ®µã€‚
        2. åœ¨å†…å®¹éƒ¨åˆ†è¯¦ç»†é˜è¿°ä½ çš„æ€ç»´è¿‡ç¨‹ã€‚
        3. å†³å®šæ˜¯ç»§ç»­æ¨ç†è¿˜æ˜¯æä¾›æœ€ç»ˆç­”æ¡ˆã€‚

        è¾“å‡ºæ ¼å¼è¯´æ˜:
        è¾“å‡ºè¯·ä¸¥æ ¼éµå¾ªJSONæ ¼å¼, åŒ…å«ä»¥ä¸‹é”®: 'title', 'content', 'next_action'(å€¼åªèƒ½ä¸º'continue' æˆ– 'final_answer'äºŒè€…ä¹‹ä¸€)

        å…³é”®æŒ‡ç¤º:
        - è‡³å°‘ä½¿ç”¨5ä¸ªä¸åŒçš„æ¨ç†æ­¥éª¤ã€‚
        - æ‰¿è®¤ä½ ä½œä¸ºAIçš„å±€é™æ€§,æ˜ç¡®è¯´æ˜ä½ èƒ½åšä»€ä¹ˆå’Œä¸èƒ½åšä»€ä¹ˆã€‚
        - ä¸»åŠ¨æ¢ç´¢å’Œè¯„ä¼°æ›¿ä»£ç­”æ¡ˆæˆ–æ–¹æ³•ã€‚
        - æ‰¹åˆ¤æ€§åœ°è¯„ä¼°ä½ è‡ªå·±çš„æ¨ç†;è¯†åˆ«æ½œåœ¨çš„ç¼ºé™·æˆ–åè§ã€‚
        - å½“é‡æ–°å®¡è§†æ—¶,é‡‡ç”¨æ ¹æœ¬ä¸åŒçš„æ–¹æ³•æˆ–è§†è§’ã€‚
        - è‡³å°‘ä½¿ç”¨3ç§ä¸åŒçš„æ–¹æ³•æ¥å¾—å‡ºæˆ–éªŒè¯ä½ çš„ç­”æ¡ˆã€‚
        - åœ¨ä½ çš„æ¨ç†ä¸­èå…¥ç›¸å…³çš„é¢†åŸŸçŸ¥è¯†å’Œæœ€ä½³å®è·µã€‚
        - åœ¨é€‚ç”¨çš„æƒ…å†µä¸‹,é‡åŒ–æ¯ä¸ªæ­¥éª¤å’Œæœ€ç»ˆç»“è®ºçš„ç¡®å®šæ€§æ°´å¹³ã€‚
        - è€ƒè™‘ä½ æ¨ç†ä¸­å¯èƒ½å­˜åœ¨çš„è¾¹ç¼˜æƒ…å†µæˆ–ä¾‹å¤–ã€‚
        - ä¸ºæ’é™¤æ›¿ä»£å‡è®¾æä¾›æ¸…æ™°çš„ç†ç”±ã€‚

        ç¤ºä¾‹JSONè¾“å‡º:
        {
            "title": "åˆæ­¥é—®é¢˜åˆ†æ",
            "content": "ä¸ºäº†æœ‰æ•ˆåœ°è§£å†³è¿™ä¸ªé—®é¢˜,æˆ‘é¦–å…ˆä¼šå°†ç»™å®šçš„ä¿¡æ¯åˆ†è§£ä¸ºå…³é”®ç»„æˆéƒ¨åˆ†ã€‚è¿™æ¶‰åŠåˆ°è¯†åˆ«...[è¯¦ç»†è§£é‡Š]...é€šè¿‡è¿™æ ·æ„å»ºé—®é¢˜,æˆ‘ä»¬å¯ä»¥ç³»ç»Ÿåœ°è§£å†³æ¯ä¸ªæ–¹é¢ã€‚",
            "next_action": "continue"
        }

        è®°ä½: å…¨é¢æ€§å’Œæ¸…æ™°åº¦è‡³å…³é‡è¦ã€‚æ¯ä¸€æ­¥éƒ½åº”è¯¥ä¸ºæœ€ç»ˆè§£å†³æ–¹æ¡ˆæä¾›æœ‰æ„ä¹‰çš„è¿›å±•ã€‚
        å†æ¬¡æé†’: è¾“å‡ºè¯·åŠ¡å¿…ä¸¥æ ¼éµå¾ªJSONæ ¼å¼, åŒ…å«ä»¥ä¸‹é”®: 'title', 'content', 'next_action'(å€¼åªèƒ½ä¸º'continue' æˆ– 'final_answer'äºŒè€…ä¹‹ä¸€)
        """},
        {"role": "user", "content": prompt},
        {"role": "assistant", "content": "ç°åœ¨æˆ‘å°†ä¸€æ­¥æ­¥æ€è€ƒï¼Œä»åˆ†æé—®é¢˜å¼€å§‹å¹¶å°†é—®é¢˜åˆ†è§£ã€‚"}
    ]
    
    steps = []
    step_count = 1
    total_thinking_time = 0
    
    while True:
        start_time = time.time()
        step_data = make_api_call(messages, 1000)
        end_time = time.time()
        thinking_time = end_time - start_time
        total_thinking_time += thinking_time
        
        title = step_data.get('title', f'Step {step_count}')
        content = step_data.get('content', 'No content provided')
        next_action = step_data.get('next_action', 'continue')
        
        steps.append((f"Step {step_count}: {title}", content, thinking_time))
        
        messages.append({"role": "assistant", "content": json.dumps(step_data)})
        
        if next_action == 'final_answer' or step_count > 25: # æœ€å¤š25æ­¥ï¼Œä»¥é˜²æ­¢æ— é™çš„æ€è€ƒã€‚å¯ä»¥é€‚å½“è°ƒæ•´ã€‚
            break
        
        step_count += 1

        yield steps, None  # åœ¨ç»“æŸæ—¶ç”Ÿæˆæ€»æ—¶é—´

    # ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    messages.append({"role": "user", "content": "è¯·æ ¹æ®ä½ ä¸Šé¢çš„æ¨ç†æä¾›æœ€ç»ˆç­”æ¡ˆã€‚"})
    
    start_time = time.time()
    final_data = make_api_call(messages, 1000, is_final_answer=True)
    end_time = time.time()
    thinking_time = end_time - start_time
    total_thinking_time += thinking_time
    
    final_content = final_data.get('content', 'æ²¡æœ‰æ¨ç†å‡ºæœ€ç»ˆç»“æœ')
    steps.append(("æœ€ç»ˆæ¨ç†ç»“æœ", final_content, thinking_time))

    yield steps, total_thinking_time

def main():
    st.set_page_config(page_title="Qwen2.5 o1-like Reasoning Chain", page_icon="ğŸ’¬", layout="wide")
    
    st.title("Qwen2.5å®ç°ç±»ä¼¼o1 modelçš„æ¨ç†é“¾")
    st.caption("ğŸš€ A streamlit implementation powered by [å¼€æºå¤§æ¨¡å‹é£Ÿç”¨æŒ‡å— self-llm](https://github.com/datawhalechina/self-llm)")
    
    st.markdown("""
    é€šè¿‡vLLMéƒ¨ç½²è°ƒç”¨Qwen2.5-7B-Instructå¹¶å®ç°ç±»ä¼¼OpenAI o1 modelçš„é•¿æ¨ç†é“¾æ•ˆæœä»¥æé«˜å¯¹å¤æ‚é—®é¢˜çš„æ¨ç†å‡†ç¡®æ€§ã€‚
    """)
    
    # ç”¨æˆ·è¾“å…¥æŸ¥è¯¢
    user_query = st.text_input("è¾“å…¥é—®é¢˜:", placeholder="ç¤ºä¾‹ï¼šstrawberryä¸­æœ‰å¤šå°‘ä¸ªå­—æ¯rï¼Ÿ")
    
    if user_query:
        st.write("æ­£åœ¨ç”Ÿæˆæ¨ç†é“¾ä¸­...")
        
        # åˆ›å»ºç©ºå…ƒç´ ä»¥ä¿å­˜ç”Ÿæˆçš„æ–‡æœ¬å’Œæ€»æ—¶é—´
        response_container = st.empty()
        time_container = st.empty()
        
        # ç”Ÿæˆå¹¶æ˜¾ç¤ºå“åº”
        for steps, total_thinking_time in generate_response(user_query):
            with response_container.container():
                for i, (title, content, thinking_time) in enumerate(steps):
                    if title.startswith("æœ€ç»ˆæ¨ç†ç»“æœ"):
                        st.markdown(f"### {title}")
                        st.markdown(content.replace('\n', '<br>'), unsafe_allow_html=True)
                    else:
                        with st.expander(title, expanded=True):
                            st.markdown(content.replace('\n', '<br>'), unsafe_allow_html=True)
            
            # ä»…åœ¨ç»“æŸæ—¶æ˜¾ç¤ºæ€»æ—¶é—´
            if total_thinking_time is not None:
                time_container.markdown(f"**æ€»æ¨ç†æ—¶é—´: {total_thinking_time:.2f} ç§’**")

if __name__ == "__main__":
    main()
```



#### å®ç°åŸç†

![06-3](./images/06-3.png)

æ³¨æ„ï¼šä»£ç ä¸­åªæ˜¯å°è¯•å®ç°äº† **`o1-like`** çš„ `Reasoning Chain` çš„æ•ˆæœï¼Œè€Œå¹¶éæ˜¯åœ¨ `pretrain` è¿‡ç¨‹ä¸­è®­ç»ƒå¾—åˆ° `Chain of Thought` å†…ç½®èƒ½åŠ›çš„ o1 æ¨¡å‹ã€‚



#### åˆ›å»ºå…¼å®¹ OpenAI API æ¥å£çš„æœåŠ¡

`Qwen` å…¼å®¹ `OpenAI API` åè®®ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨ `vLLM` åˆ›å»º `OpenAI API` æœåŠ¡å™¨ã€‚`vLLM` éƒ¨ç½²å®ç° `OpenAI API` åè®®çš„æœåŠ¡å™¨éå¸¸æ–¹ä¾¿ã€‚é»˜è®¤ä¼šåœ¨ http://localhost:8000 å¯åŠ¨æœåŠ¡å™¨ã€‚æœåŠ¡å™¨å½“å‰ä¸€æ¬¡æ‰˜ç®¡ä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶å®ç°åˆ—è¡¨æ¨¡å‹ã€`completions` å’Œ `chat completions` ç«¯å£ã€‚

- `completions`ï¼šæ˜¯åŸºæœ¬çš„æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œæ¨¡å‹ä¼šåœ¨ç»™å®šçš„æç¤ºåç”Ÿæˆä¸€æ®µæ–‡æœ¬ã€‚è¿™ç§ç±»å‹çš„ä»»åŠ¡é€šå¸¸ç”¨äºç”Ÿæˆæ–‡ç« ã€æ•…äº‹ã€é‚®ä»¶ç­‰ã€‚
- `chat completions`ï¼šæ˜¯é¢å‘å¯¹è¯çš„ä»»åŠ¡ï¼Œæ¨¡å‹éœ€è¦ç†è§£å’Œç”Ÿæˆå¯¹è¯ã€‚è¿™ç§ç±»å‹çš„ä»»åŠ¡é€šå¸¸ç”¨äºæ„å»ºèŠå¤©æœºå™¨äººæˆ–è€…å¯¹è¯ç³»ç»Ÿã€‚

åœ¨åˆ›å»ºæœåŠ¡å™¨æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‡å®šæ¨¡å‹åç§°ã€æ¨¡å‹è·¯å¾„ã€èŠå¤©æ¨¡æ¿ç­‰å‚æ•°ã€‚

- `--host` å’Œ `--port` å‚æ•°æŒ‡å®šåœ°å€ã€‚
- `--model` å‚æ•°æŒ‡å®šæ¨¡å‹åç§°ã€‚
- `--chat-template` å‚æ•°æŒ‡å®šèŠå¤©æ¨¡æ¿ã€‚
- `--served-model-name` æŒ‡å®šæœåŠ¡æ¨¡å‹çš„åç§°ã€‚
- `--max-model-len` æŒ‡å®šæ¨¡å‹çš„æœ€å¤§é•¿åº¦ã€‚

```bash
python -m vllm.entrypoints.openai.api_server --model /root/autodl-tmp/qwen/Qwen2.5-7B-Instruct  --served-model-name Qwen2.5-7B-Instruct --max-model-len=32768 --port 8000
```

åŠ è½½å®Œæ¯•åå‡ºç°å¦‚ä¸‹ä¿¡æ¯è¯´æ˜æœåŠ¡æˆåŠŸå¯åŠ¨

![03-3](./images/03-3.png)



#### å¯åŠ¨ Streamlit ç•Œé¢

```shell
streamlit run /root/autodl-tmp/app_qwen.py --server.address 127.0.0.1 --server.port 6004
```

åœ¨æœ¬åœ°æµè§ˆå™¨ä¸­æ‰“å¼€é“¾æ¥ http://127.0.0.1:6004/ ï¼Œå³å¯è¿›å…¥éƒ¨ç½²çš„ `Streamlit` ç•Œé¢ã€‚è¿è¡Œæ•ˆæœå¦‚ä¸‹ï¼š

![06-4](./images/06-4.png)

æˆ‘ä»¬å¯ä»¥å°è¯•ç¤ºä¾‹ä¸­çš„é—®é¢˜ï¼Œ`strawberryä¸­æœ‰å¤šå°‘ä¸ªå­—æ¯rï¼Ÿ` æ¥è®©å®ç°äº†æ¨ç†é“¾çš„ `Qwen2.5-7B-Instruct` ç»™å‡ºæ¨ç†ç»“æœ

![06-5](./images/06-5.png)

æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ç»è¿‡4æ­¥æ¨ç†å’Œæ€è€ƒï¼Œæœ€ç»ˆçš„æ¨ç†ç»“æœæ˜¯**æ­£ç¡®çš„**ï¼Œä¸”æ€»æ¨ç†è€—æ—¶2è¿˜æ˜¯å¾ˆé•¿çš„ï¼Œè¿™ä¹Ÿç¬¦åˆäº† o1 æ¨¡å‹çš„åŸºæœ¬ç‰¹å¾ï¼Œç”¨æ¨ç†æ—¶é—´çš„å¢åŠ æ¢å–æ›´é«˜çš„æ¨ç†å‡†ç¡®ç‡ã€‚

**è¯´æ˜ï¼š**æœ‰è¯»è€…å¯èƒ½ä¼šè®¤ä¸ºè¿™ä¸ªé—®é¢˜æ ¹æœ¬æ²¡æœ‰éš¾åº¦ï¼Œä½†æ˜¯å®é™…ä¸Šå¯¹äºç°é˜¶æ®µçš„å¤§è¯­è¨€æ¨¡å‹æ¥è¯´ï¼Œå¾ˆå°‘æœ‰èƒ½å¤Ÿå›ç­”æ­£ç¡®çš„ï¼Œæ¯”å¦‚æˆ‘ä»¬å¯ä»¥å›åˆ° [04-Qwen2.5-7B-Instruct WebDemo éƒ¨ç½²](./04-Qwen2_5-7B-Instruct WebDemoéƒ¨ç½².md) æ¥å°è¯•ä¸€ä¸‹æé—®ç›¸åŒçš„é—®é¢˜ï¼Œæœ€åå¾—åˆ°çš„ç»“æœæ˜¾ç„¶æ˜¯**é”™è¯¯çš„**ã€‚

![06-6](./images/06-6.png) 