# Qwen2.5-7B-Instruct Lora å¾®è°ƒ SwanLabå¯è§†åŒ–è®°å½•ç‰ˆ

æœ¬èŠ‚æˆ‘ä»¬ç®€è¦ä»‹ç»åŸºäº transformersã€peft ç­‰æ¡†æ¶ï¼Œä½¿ç”¨ Qwen2.5-7B-Instruct æ¨¡å‹åœ¨**ä¸­æ–‡æ³•å¾‹é—®ç­”æ•°æ®é›† DISC-Law-SFT** ä¸Šè¿›è¡ŒLoraå¾®è°ƒè®­ç»ƒï¼ŒåŒæ—¶ä½¿ç”¨ [SwanLab](https://github.com/swanhubx/swanlab) ç›‘æ§è®­ç»ƒè¿‡ç¨‹ä¸è¯„ä¼°æ¨¡å‹æ•ˆæœï¼Œå¹¶æ¯”è¾ƒ0.5B/1.5B/7B/14Bæ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

Lora æ˜¯ä¸€ç§é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œæ·±å…¥äº†è§£å…¶åŸç†å¯å‚è§åšå®¢ï¼š[çŸ¥ä¹|æ·±å…¥æµ…å‡º Lora](https://zhuanlan.zhihu.com/p/650197598)ã€‚

è®­ç»ƒè¿‡ç¨‹ï¼š<a href="https://swanlab.cn/@ZeyiLin/Qwen2.5-LoRA-Law/charts" target="_blank">Qwen2.5-LoRA-Law</a>

## ç›®å½•

- [SwanLabç®€ä»‹](#-SwanLabç®€ä»‹)
- [ç¯å¢ƒé…ç½®](#-ç¯å¢ƒé…ç½®)
- [å‡†å¤‡æ•°æ®é›†](#-å‡†å¤‡æ•°æ®é›†)
- [æ¨¡å‹ä¸‹è½½ä¸åŠ è½½](#-æ¨¡å‹ä¸‹è½½ä¸åŠ è½½)
- [é›†æˆSwanLab](#-é›†æˆSwanLab)
- [å¼€å§‹å¾®è°ƒï¼ˆå®Œæ•´ä»£ç ï¼‰](#-å¼€å§‹å¾®è°ƒ)
- [è®­ç»ƒç»“æœæ¼”ç¤º](#-è®­ç»ƒç»“æœæ¼”ç¤º)
- [è¡¥å……](#è¡¥å……)

## ğŸ‘‹ SwanLabç®€ä»‹

![07-1](./images/07-1.jpg)

[SwanLab](https://github.com/swanhubx/swanlab) æ˜¯ä¸€ä¸ªå¼€æºçš„æ¨¡å‹è®­ç»ƒè®°å½•å·¥å…·ï¼Œé¢å‘AIç ”ç©¶è€…ï¼Œæä¾›äº†è®­ç»ƒå¯è§†åŒ–ã€è‡ªåŠ¨æ—¥å¿—è®°å½•ã€è¶…å‚æ•°è®°å½•ã€å®éªŒå¯¹æ¯”ã€å¤šäººååŒç­‰åŠŸèƒ½ã€‚åœ¨SwanLabä¸Šï¼Œç ”ç©¶è€…èƒ½åŸºäºç›´è§‚çš„å¯è§†åŒ–å›¾è¡¨å‘ç°è®­ç»ƒé—®é¢˜ï¼Œå¯¹æ¯”å¤šä¸ªå®éªŒæ‰¾åˆ°ç ”ç©¶çµæ„Ÿï¼Œå¹¶é€šè¿‡åœ¨çº¿é“¾æ¥çš„åˆ†äº«ä¸åŸºäºç»„ç»‡çš„å¤šäººååŒè®­ç»ƒï¼Œæ‰“ç ´å›¢é˜Ÿæ²Ÿé€šçš„å£å’ã€‚

**ä¸ºä»€ä¹ˆè¦è®°å½•è®­ç»ƒ**

ç›¸è¾ƒäºè½¯ä»¶å¼€å‘ï¼Œæ¨¡å‹è®­ç»ƒæ›´åƒä¸€ä¸ªå®éªŒç§‘å­¦ã€‚ä¸€ä¸ªå“è´¨ä¼˜ç§€çš„æ¨¡å‹èƒŒåï¼Œå¾€å¾€æ˜¯æˆåƒä¸Šä¸‡æ¬¡å®éªŒã€‚ç ”ç©¶è€…éœ€è¦ä¸æ–­å°è¯•ã€è®°å½•ã€å¯¹æ¯”ï¼Œç§¯ç´¯ç»éªŒï¼Œæ‰èƒ½æ‰¾åˆ°æœ€ä½³çš„æ¨¡å‹ç»“æ„ã€è¶…å‚æ•°ä¸æ•°æ®é…æ¯”ã€‚åœ¨è¿™ä¹‹ä¸­ï¼Œå¦‚ä½•é«˜æ•ˆè¿›è¡Œè®°å½•ä¸å¯¹æ¯”ï¼Œå¯¹äºç ”ç©¶æ•ˆç‡çš„æå‡è‡³å…³é‡è¦ã€‚



## ğŸŒ ç¯å¢ƒé…ç½®

ç¯å¢ƒé…ç½®åˆ†ä¸ºä¸‰æ­¥ï¼š

1. ç¡®ä¿ä½ çš„ç”µè„‘ä¸Šè‡³å°‘æœ‰ä¸€å¼ è‹±ä¼Ÿè¾¾æ˜¾å¡ï¼Œå¹¶å·²å®‰è£…å¥½äº†CUDAç¯å¢ƒã€‚

2. å®‰è£…Pythonï¼ˆç‰ˆæœ¬>=3.8ï¼‰ä»¥åŠèƒ½å¤Ÿè°ƒç”¨CUDAåŠ é€Ÿçš„PyTorchã€‚

3. å®‰è£…å¾®è°ƒç›¸å…³çš„ç¬¬ä¸‰æ–¹åº“ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š

```bash
python -m pip install --upgrade pip
# æ›´æ¢ pypi æºåŠ é€Ÿåº“çš„å®‰è£…
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

pip install modelscope==1.18.0
pip install transformers==4.44.2
pip install sentencepiece==0.2.0
pip install accelerate==0.34.2
pip install datasets==2.20.0
pip install peft==0.11.1
pip install swanlab==0.3.23
```

## ğŸ“š å‡†å¤‡æ•°æ®é›†

æœ¬èŠ‚ä½¿ç”¨çš„æ˜¯ [DISC-Law-SFT](https://huggingface.co/datasets/ShengbinYue/DISC-Law-SFT) æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä¸»è¦ç”¨äºä¸­æ–‡æ³•å¾‹å¤§è¯­è¨€æ¨¡å‹çš„å¾®è°ƒã€‚

> HF READMEï¼šåœ¨ä¸­å›½ï¼Œæ³•å¾‹æ™ºèƒ½ç³»ç»Ÿéœ€è¦ç»“åˆå„ç§èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ³•å¾‹æ–‡æœ¬ç†è§£å’Œç”Ÿæˆã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„ç›‘ç£å¾®è°ƒæ•°æ®é›†ï¼Œåä¸ºDISC-Law-SFTï¼Œæ¶µç›–äº†ä¸åŒçš„æ³•å¾‹åœºæ™¯ï¼Œå¦‚æ³•å¾‹ä¿¡æ¯æå–ã€æ³•å¾‹åˆ¤å†³é¢„æµ‹ã€æ³•å¾‹æ–‡ä»¶æ‘˜è¦å’Œæ³•å¾‹é—®é¢˜å›ç­”ã€‚DISC-Law-SFTåŒ…æ‹¬ä¸¤ä¸ªå­é›†ï¼ŒDISC-Law-SFT-Pairå’ŒDISC-Law-SFT-Tripletã€‚å‰è€…æ—¨åœ¨å‘LLMå¼•å…¥æ³•å¾‹æ¨ç†èƒ½åŠ›ï¼Œè€Œåè€…æœ‰åŠ©äºå¢å¼ºæ¨¡å‹åˆ©ç”¨å¤–éƒ¨æ³•å¾‹çŸ¥è¯†çš„èƒ½åŠ›ã€‚

![07-2](./images/07-2.png)

åœ¨æœ¬èŠ‚çš„ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬ä¸»è¦ä½¿ç”¨å…¶ä¸­çš„ [DISC-Law-SFT-Pair-QA-released](https://huggingface.co/datasets/ShengbinYue/DISC-Law-SFT/blob/main/DISC-Law-SFT-Pair-QA-released.jsonl) å­é›†ï¼Œå¹¶å¯¹å®ƒè¿›è¡Œæ ¼å¼è°ƒæ•´ï¼Œç»„åˆæˆå¦‚ä¸‹æ ¼å¼çš„jsonæ–‡ä»¶ï¼š

```json
{
  "instruction": "ä½ æ˜¯ä¸€ä¸ªæ³•å¾‹ä¸“å®¶ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç»™å‡ºä¸“ä¸šçš„å›ç­”",
  "input": "è¯ˆéª—ç½ªé‡åˆ‘æ ‡å‡†æ˜¯ä»€ä¹ˆ?",
  "output": "è¯ˆéª—ç½ªæŒ‡çš„æ˜¯ä»¥éæ³•å æœ‰ä¸ºç›®çš„ï¼Œä½¿ç”¨æ¬ºéª—æ–¹æ³•ï¼Œéª—å–æ•°é¢è¾ƒå¤§çš„å…¬ç§è´¢ç‰©çš„è¡Œä¸º..."
}
```

å…¶ä¸­ï¼Œ`instruction` æ˜¯ç”¨æˆ·æŒ‡ä»¤ï¼Œå‘ŠçŸ¥æ¨¡å‹å…¶éœ€è¦å®Œæˆçš„ä»»åŠ¡ï¼›`input` æ˜¯ç”¨æˆ·è¾“å…¥ï¼Œå³æ¨¡æ‹ŸçœŸå®åœºæ™¯ä¸‹ç”¨æˆ·çš„é—®é¢˜ï¼›`output` æ˜¯æ¨¡å‹åº”è¯¥ç»™å‡ºçš„è¾“å‡ºï¼Œå³æ¨¡å‹çš„å›ç­”ã€‚

**æ•°æ®é›†ä¸‹è½½ä¸å¤„ç†æ–¹å¼**

1. åœ¨HuggingFaceä¸Šç›´æ¥ä¸‹è½½[DISC-Law-SFT-Pair-QA-released.jsonl](https://huggingface.co/datasets/ShengbinYue/DISC-Law-SFT/blob/main/DISC-Law-SFT-Pair-QA-released.jsonl)ï¼Œæ”¾åˆ°é¡¹ç›®æ ¹ç›®å½•ä¸‹
2. åœ¨åŒä¸€ç›®å½•ä¸‹ï¼Œç”¨ä»¥ä¸‹ä»£ç å¤„ç†æ•°æ®é›†ï¼Œå¾—åˆ°æ–°æ•°æ®é›†æ–‡ä»¶ï¼š

```python
import json

# å®šä¹‰å›ºå®šçš„instruction
INSTRUCTION = "ä½ æ˜¯ä¸€ä¸ªæ³•å¾‹ä¸“å®¶ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç»™å‡ºä¸“ä¸šçš„å›ç­”"

def process_jsonl(input_file, output_file):
    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:
        for line in infile:
            # è¯»å–æ¯ä¸€è¡Œå¹¶è§£æJSON
            data = json.loads(line)
            
            # åˆ›å»ºæ–°çš„å­—å…¸ï¼ŒåŒ…å«instruction, inputå’Œoutput
            new_data = {
                "instruction": INSTRUCTION,
                "input": data["input"],
                "output": data["output"]
            }
            
            # å°†æ–°çš„å­—å…¸å†™å…¥è¾“å‡ºæ–‡ä»¶
            json.dump(new_data, outfile, ensure_ascii=False)
            outfile.write('\n')

# ä½¿ç”¨ç¤ºä¾‹
input_file = "DISC-Law-SFT-Pair-QA-released.jsonl"
output_file = "DISC-Law-SFT-Pair-QA-released-new.jsonl"

process_jsonl(input_file, output_file)
print(f"å¤„ç†å®Œæˆã€‚è¾“å‡ºæ–‡ä»¶ï¼š{output_file}")
```
3. è‡³æ­¤æˆ‘ä»¬å®Œæˆäº†æ•°æ®é›†å‡†å¤‡


## ğŸ¤– æ¨¡å‹ä¸‹è½½ä¸åŠ è½½

è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨modelscopeä¸‹è½½Qwen2.5-7B-Instructæ¨¡å‹ï¼Œç„¶åæŠŠå®ƒåŠ è½½åˆ°Transformersä¸­è¿›è¡Œè®­ç»ƒï¼š

```python
from modelscope import snapshot_download, AutoTokenizer
from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq
import torch

# åœ¨modelscopeä¸Šä¸‹è½½Qwenæ¨¡å‹åˆ°æœ¬åœ°ç›®å½•ä¸‹
model_dir = snapshot_download("qwen/Qwen2.5-7B-Instruct", cache_dir="./", revision="master")

# TransformersåŠ è½½æ¨¡å‹æƒé‡
tokenizer = AutoTokenizer.from_pretrained("./qwen/Qwen2___5-7B-Instruct/", use_fast=False,trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("./qwen/Qwen2___5-7B-Instruct/", device_map="auto",torch_dtype=torch.bfloat16)
model.enable_input_require_grads()  # å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹æ—¶ï¼Œè¦æ‰§è¡Œè¯¥æ–¹æ³•
```

æ¨¡å‹å¤§å°ä¸º 15GBï¼Œä¸‹è½½æ¨¡å‹å¤§æ¦‚éœ€è¦ 5 åˆ†é’Ÿã€‚

## ğŸ¦â€ é›†æˆSwanLab

SwanLabä¸Transformerså·²ç»åšå¥½äº†é›†æˆï¼Œç”¨æ³•æ˜¯åœ¨Trainerçš„`callbacks`å‚æ•°ä¸­æ·»åŠ `SwanLabCallback`å®ä¾‹ï¼Œå°±å¯ä»¥è‡ªåŠ¨è®°å½•è¶…å‚æ•°å’Œè®­ç»ƒæŒ‡æ ‡ï¼Œç®€åŒ–ä»£ç å¦‚ä¸‹ï¼š

```python
from swanlab.integration.transformers import SwanLabCallback
from transformers import Trainer

swanlab_callback = SwanLabCallback()

trainer = Trainer(
    ...
    callbacks=[swanlab_callback],
)
```

é¦–æ¬¡ä½¿ç”¨SwanLabï¼Œéœ€è¦å…ˆåœ¨[å®˜ç½‘](https://swanlab.cn)æ³¨å†Œä¸€ä¸ªè´¦å·ï¼Œç„¶ååœ¨ç”¨æˆ·è®¾ç½®é¡µé¢å¤åˆ¶ä½ çš„API Keyï¼Œç„¶ååœ¨è®­ç»ƒå¼€å§‹æç¤ºç™»å½•æ—¶ç²˜è´´å³å¯ï¼Œåç»­æ— éœ€å†æ¬¡ç™»å½•ï¼š

![07-3](./images/07-3.png)

æ›´å¤šç”¨æ³•å¯å‚è€ƒ[å¿«é€Ÿå¼€å§‹](https://docs.swanlab.cn/zh/guide_cloud/general/quick-start.html)ã€[Transformersé›†æˆ](https://docs.swanlab.cn/zh/guide_cloud/integration/integration-huggingface-transformers.html)ã€‚


## ğŸš€ å¼€å§‹å¾®è°ƒ

æŸ¥çœ‹å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹ï¼š<a href="https://swanlab.cn/@ZeyiLin/Qwen2.5-LoRA-Law/charts" target="_blank">Qwen2.5-LoRA-Law</a>

**æœ¬èŠ‚ä»£ç åšäº†ä»¥ä¸‹å‡ ä»¶äº‹ï¼š**
1. ä¸‹è½½å¹¶åŠ è½½Qwen2.5-7B-Instructæ¨¡å‹
2. åŠ è½½æ•°æ®é›†ï¼Œå–å‰5000æ¡æ•°æ®å‚ä¸è®­ç»ƒï¼Œ5æ¡æ•°æ®è¿›è¡Œä¸»è§‚è¯„æµ‹
3. é…ç½®Loraï¼Œå‚æ•°ä¸ºr=64, lora_alpha=16, lora_dropout=0.1
4. ä½¿ç”¨SwanLabè®°å½•è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬è¶…å‚æ•°ã€æŒ‡æ ‡å’Œæ¯ä¸ªepochçš„æ¨¡å‹è¾“å‡ºç»“æœ
5. è®­ç»ƒ2ä¸ªepoch

**å®Œæ•´ä»£ç å¦‚ä¸‹**

```python
import json
import pandas as pd
import torch
from datasets import Dataset
from modelscope import snapshot_download, AutoTokenizer
from swanlab.integration.transformers import SwanLabCallback
from peft import LoraConfig, TaskType, get_peft_model
from transformers import (
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
)
import swanlab
        
        
def process_func(example):
    """
    å°†æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†
    """
    MAX_LENGTH = 384
    input_ids, attention_mask, labels = [], [], []
    instruction = tokenizer(
        f"<|im_start|>system\n{example['instruction']}<|im_end|>\n<|im_start|>user\n{example['input']}<|im_end|>\n<|im_start|>assistant\n",
        add_special_tokens=False,
    )
    response = tokenizer(f"{example['output']}", add_special_tokens=False)
    input_ids = (
        instruction["input_ids"] + response["input_ids"] + [tokenizer.pad_token_id]
    )
    attention_mask = instruction["attention_mask"] + response["attention_mask"] + [1]
    labels = (
        [-100] * len(instruction["input_ids"])
        + response["input_ids"]
        + [tokenizer.pad_token_id]
    )
    if len(input_ids) > MAX_LENGTH:  # åšä¸€ä¸ªæˆªæ–­
        input_ids = input_ids[:MAX_LENGTH]
        attention_mask = attention_mask[:MAX_LENGTH]
        labels = labels[:MAX_LENGTH]
        
    return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": labels}


def predict(messages, model, tokenizer):
    device = "cuda"
    text = tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    model_inputs = tokenizer([text], return_tensors="pt").to(device)

    generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512)
    generated_ids = [
        output_ids[len(input_ids) :]
        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]

    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

    return response


# åœ¨modelscopeä¸Šä¸‹è½½Qwenæ¨¡å‹åˆ°æœ¬åœ°ç›®å½•ä¸‹
model_dir = snapshot_download("qwen/Qwen2.5-7B-Instruct", cache_dir="./", revision="master")

# TransformersåŠ è½½æ¨¡å‹æƒé‡
tokenizer = AutoTokenizer.from_pretrained("./qwen/Qwen2___5-7B-Instruct/", use_fast=False, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("./qwen/Qwen2___5-7B-Instruct/", device_map="auto", torch_dtype=torch.bfloat16)
model.enable_input_require_grads()  # å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹æ—¶ï¼Œè¦æ‰§è¡Œè¯¥æ–¹æ³•

# å¤„ç†æ•°æ®é›†
train_jsonl_path = "DISC-Law-SFT-Pair-QA-released-new.jsonl"
train_df = pd.read_json(train_jsonl_path, lines=True)[5:5000]
train_ds = Dataset.from_pandas(train_df)
train_dataset = train_ds.map(process_func, remove_columns=train_ds.column_names)
test_df = pd.read_json(train_jsonl_path, lines=True)[:5]

config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
    ],
    inference_mode=False,  # è®­ç»ƒæ¨¡å¼
    r=64,  # Lora ç§©
    lora_alpha=16,  # Lora alaphï¼Œå…·ä½“ä½œç”¨å‚è§ Lora åŸç†
    lora_dropout=0.1,  # Dropout æ¯”ä¾‹
)

peft_model = get_peft_model(model, config)

args = TrainingArguments(
    output_dir="./output/Qwen2.5-7b",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    logging_steps=10,
    num_train_epochs=2,
    save_steps=100,
    learning_rate=1e-4,
    save_on_each_node=True,
    gradient_checkpointing=True,
    report_to="none",
)

class HuanhuanSwanLabCallback(SwanLabCallback):   
    def on_train_begin(self, args, state, control, model=None, **kwargs):
        if not self._initialized:
            self.setup(args, state, model, **kwargs)
            
        print("è®­ç»ƒå¼€å§‹")
        print("æœªå¼€å§‹å¾®è°ƒï¼Œå…ˆå–3æ¡ä¸»è§‚è¯„æµ‹ï¼š")
        test_text_list = []
        for index, row in test_df[:3].iterrows():
            instruction = row["instruction"]
            input_value = row["input"]

            messages = [
                {"role": "system", "content": f"{instruction}"},
                {"role": "user", "content": f"{input_value}"},
            ]

            response = predict(messages, peft_model, tokenizer)
            messages.append({"role": "assistant", "content": f"{response}"})
                
            result_text = f"ã€Qã€‘{messages[1]['content']}\nã€LLMã€‘{messages[2]['content']}\n"
            print(result_text)
            
            test_text_list.append(swanlab.Text(result_text, caption=response))

        swanlab.log({"Prediction": test_text_list}, step=0)
    
    def on_epoch_end(self, args, state, control, **kwargs):
        # ===================æµ‹è¯•é˜¶æ®µ======================
        test_text_list = []
        for index, row in test_df.iterrows():
            instruction = row["instruction"]
            input_value = row["input"]
            ground_truth = row["output"]

            messages = [
                {"role": "system", "content": f"{instruction}"},
                {"role": "user", "content": f"{input_value}"},
            ]

            response = predict(messages, peft_model, tokenizer)
            messages.append({"role": "assistant", "content": f"{response}"})
            
            if index == 0:
                print("epoch", round(state.epoch), "ä¸»è§‚è¯„æµ‹ï¼š")
                
            result_text = f"ã€Qã€‘{messages[1]['content']}\nã€LLMã€‘{messages[2]['content']}\nã€GTã€‘ {ground_truth}"
            print(result_text)
            
            test_text_list.append(swanlab.Text(result_text, caption=response))

        swanlab.log({"Prediction": test_text_list}, step=round(state.epoch))
        
        
swanlab_callback = HuanhuanSwanLabCallback(
    project="Qwen2.5-LoRA-Law",
    experiment_name="7b",
    config={
        "model": "https://modelscope.cn/models/Qwen/Qwen2.5-7B-Instruct",
        "dataset": "https://huggingface.co/datasets/ShengbinYue/DISC-Law-SFT",
        "github": "https://github.com/datawhalechina/self-llm",
        "system_prompt": "ä½ æ˜¯ä¸€ä¸ªæ³•å¾‹ä¸“å®¶ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç»™å‡ºä¸“ä¸šçš„å›ç­”",
        "lora_rank": 64,
        "lora_alpha": 16,
        "lora_dropout": 0.1,
    },
)

trainer = Trainer(
    model=peft_model,
    args=args,
    train_dataset=train_dataset,
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),
    callbacks=[swanlab_callback],
)

trainer.train()

# åœ¨Jupyter Notebookä¸­è¿è¡Œæ—¶è¦åœæ­¢SwanLabè®°å½•ï¼Œéœ€è¦è°ƒç”¨swanlab.finish()
swanlab.finish()
```

çœ‹åˆ°ä¸‹é¢çš„è¿›åº¦æ¡å³ä»£è¡¨è®­ç»ƒå¼€å§‹ï¼š

![07-4](./images/07-4.png)


## ğŸ’» è®­ç»ƒç»“æœæ¼”ç¤º

![07-5](./images/07-5.png)

ä»å›¾è¡¨ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œlrçš„ä¸‹é™ç­–ç•¥æ˜¯çº¿æ€§ä¸‹é™ï¼Œlosséšepochå‘ˆç°ä¸‹é™è¶‹åŠ¿ï¼Œè€Œgrad_normåˆ™åœ¨ä¸Šå‡ã€‚è¿™ç§å½¢æ€å¾€å¾€åæ˜ äº†æ¨¡å‹æœ‰è¿‡æ‹Ÿåˆçš„é£é™©ï¼Œè®­ç»ƒä¸è¦è¶…è¿‡2ä¸ªepochã€‚

åœ¨`Prediction`å›¾è¡¨ä¸­è®°å½•ç€æ¯ä¸ªepochçš„æ¨¡å‹è¾“å‡ºç»“æœï¼Œå¯ä»¥ç›´è§‚çœ‹åˆ°åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹å¯¹åŒä¸€è¾“å…¥æ–‡æœ¬çš„è¾“å‡ºå˜åŒ–ï¼š

æ²¡æœ‰è¢«å¾®è°ƒçš„æ¨¡å‹ï¼š
![07-6](./images/07-6.png)

å¾®è°ƒåï¼š
![07-7](./images/07-7.png)

å¯ä»¥çœ‹åˆ°Loraå¾®è°ƒåçš„æ¨¡å‹ï¼Œåœ¨å›ç­”çš„é£æ ¼ä¸Šè¦æ›´è´´è¿‘æ•°æ®é›†ï¼Œæ›´ç¬¦åˆé¢„æœŸã€‚


## è¡¥å……

### 0.5B/1.5B/7B/14Bæ¨¡å‹åœ¨æ³•å¾‹é—®ç­”ä»»åŠ¡ä¸Šçš„è¡¨ç°å¯¹æ¯”

è¿™é‡Œæˆ‘ä»¬è¡¥å……ä¸€ç»„å®éªŒï¼Œæ¢ç©¶åŒä¸€ä¸ªæ•°æ®é›†çš„loraå¾®è°ƒä»»åŠ¡ï¼Œä¸åŒè§„æ¨¡å¤§å°çš„æ¨¡å‹çš„è¡¨ç°ã€‚

![07-8](./images/07-8.png)

å¯ä»¥çœ‹åˆ°ï¼Œè¶Šå¤§è§„æ¨¡çš„æ¨¡å‹ï¼Œlossæ˜æ˜¾è¦è¶Šä½ï¼Œåæ˜ äº†æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚

### è¯¦ç»†ç¡¬ä»¶é…ç½®å’Œå‚æ•°è¯´æ˜

ä½¿ç”¨4å¼ A100 40GBæ˜¾å¡ï¼Œbatch sizeä¸º4ï¼Œgradient accumulation stepsä¸º4ï¼Œè®­ç»ƒ2ä¸ªepochçš„ç”¨æ—¶ä¸º29åˆ†é’Ÿ6ç§’ã€‚

![07-9](./images/07-9.png)
