# Llama-3-8B-Instruct FastApi éƒ¨ç½²è°ƒç”¨

## ç¯å¢ƒå‡†å¤‡

åœ¨ Autodl å¹³å°ä¸­ç§Ÿèµä¸€ä¸ª 3090 ç­‰ 24G æ˜¾å­˜çš„æ˜¾å¡æœºå™¨ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºé•œåƒé€‰æ‹© `PyTorch-->2.1.0-->3.10(ubuntu22.04)-->12.1`ã€‚
æ¥ä¸‹æ¥æ‰“å¼€åˆšåˆšç§Ÿç”¨æœåŠ¡å™¨çš„ JupyterLabï¼Œå¹¶ä¸”æ‰“å¼€å…¶ä¸­çš„ç»ˆç«¯å¼€å§‹ç¯å¢ƒé…ç½®ã€æ¨¡å‹ä¸‹è½½å’Œè¿è¡Œæ¼”ç¤ºã€‚

![å¼€å¯æœºå™¨é…ç½®é€‰æ‹©](images/image-1.png)

pip æ¢æºåŠ é€Ÿä¸‹è½½å¹¶å®‰è£…ä¾èµ–åŒ…

```shell
# å‡çº§pip
python -m pip install --upgrade pip
# æ›´æ¢ pypi æºåŠ é€Ÿåº“çš„å®‰è£…
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

pip install fastapi==0.110.2
pip install uvicorn==0.29.0
pip install requests==2.31.0
pip install modelscope==1.11.0
pip install transformers==4.40.0
pip install accelerate==0.29.3
```

fastapi==0.110.2
langchain==0.1.16
modelscope==1.11.0
streamlit==1.33.0
torch==2.1.2+cu121
transformers==4.40.0
uvicorn==0.29.0

> è€ƒè™‘åˆ°éƒ¨åˆ†åŒå­¦é…ç½®ç¯å¢ƒå¯èƒ½ä¼šé‡åˆ°ä¸€äº›é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨ AutoDL å¹³å°å‡†å¤‡äº† LLaMA3 çš„ç¯å¢ƒé•œåƒï¼Œè¯¥é•œåƒé€‚ç”¨äºè¯¥ä»“åº“çš„æ‰€æœ‰éƒ¨ç½²ç¯å¢ƒã€‚ç‚¹å‡»ä¸‹æ–¹é“¾æ¥å¹¶ç›´æ¥åˆ›å»º Autodl ç¤ºä¾‹å³å¯ã€‚
> ***https://www.codewithgpu.com/i/datawhalechina/self-llm/self-llm-LLaMA3***

## æ¨¡å‹ä¸‹è½½

ä½¿ç”¨ modelscope ä¸­çš„ snapshot_download å‡½æ•°ä¸‹è½½æ¨¡å‹ï¼Œç¬¬ä¸€ä¸ªå‚æ•°ä¸ºæ¨¡å‹åç§°ï¼Œå‚æ•° cache_dir ä¸ºæ¨¡å‹çš„ä¸‹è½½è·¯å¾„ã€‚

åœ¨ /root/autodl-tmp è·¯å¾„ä¸‹æ–°å»º model_download.py æ–‡ä»¶å¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼Œç²˜è´´ä»£ç åè¯·åŠæ—¶ä¿å­˜æ–‡ä»¶ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å¹¶è¿è¡Œ `python /root/autodl-tmp/model_download.py` æ‰§è¡Œä¸‹è½½ï¼Œæ¨¡å‹å¤§å°ä¸º 15GBï¼Œä¸‹è½½æ¨¡å‹å¤§æ¦‚éœ€è¦ 2 åˆ†é’Ÿã€‚

```python
import torch
from modelscope import snapshot_download, AutoModel, AutoTokenizer
import os
model_dir = snapshot_download('LLM-Research/Meta-Llama-3-8B-Instruct', cache_dir='/root/autodl-tmp', revision='master')
```

## ä»£ç å‡†å¤‡

åœ¨ /root/autodl-tmp è·¯å¾„ä¸‹æ–°å»º api.py æ–‡ä»¶å¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼Œç²˜è´´ä»£ç åè¯·åŠæ—¶ä¿å­˜æ–‡ä»¶ã€‚ä¸‹é¢çš„ä»£ç æœ‰å¾ˆè¯¦ç»†çš„æ³¨é‡Šï¼Œå¤§å®¶å¦‚æœ‰ä¸ç†è§£çš„åœ°æ–¹ï¼Œæ¬¢è¿æå‡º issueã€‚

```python
from fastapi import FastAPI, Request
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
import uvicorn
import json
import datetime
import torch

# è®¾ç½®è®¾å¤‡å‚æ•°
DEVICE = "cuda"  # ä½¿ç”¨CUDA
DEVICE_ID = "0"  # CUDAè®¾å¤‡IDï¼Œå¦‚æœæœªè®¾ç½®åˆ™ä¸ºç©º
CUDA_DEVICE = f"{DEVICE}:{DEVICE_ID}" if DEVICE_ID else DEVICE  # ç»„åˆCUDAè®¾å¤‡ä¿¡æ¯

# æ¸…ç†GPUå†…å­˜å‡½æ•°
def torch_gc():
    if torch.cuda.is_available():  # æ£€æŸ¥æ˜¯å¦å¯ç”¨CUDA
        with torch.cuda.device(CUDA_DEVICE):  # æŒ‡å®šCUDAè®¾å¤‡
            torch.cuda.empty_cache()  # æ¸…ç©ºCUDAç¼“å­˜
            torch.cuda.ipc_collect()  # æ”¶é›†CUDAå†…å­˜ç¢ç‰‡

# æ„å»º chat æ¨¡ç‰ˆ
def bulid_input(prompt, history=[]):
    system_format='<|start_header_id|>system<|end_header_id|>\n\n{content}<|eot_id|>'
    user_format='<|start_header_id|>user<|end_header_id|>\n\n{content}<|eot_id|>'
    assistant_format='<|start_header_id|>assistant<|end_header_id|>\n\n{content}<|eot_id|>\n'
    history.append({'role':'user','content':prompt})
    prompt_str = ''
    # æ‹¼æ¥å†å²å¯¹è¯
    for item in history:
        if item['role']=='user':
            prompt_str+=user_format.format(content=item['content'])
        else:
            prompt_str+=assistant_format.format(content=item['content'])
    return prompt_str

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI()

# å¤„ç†POSTè¯·æ±‚çš„ç«¯ç‚¹
@app.post("/")
async def create_item(request: Request):
    global model, tokenizer  # å£°æ˜å…¨å±€å˜é‡ä»¥ä¾¿åœ¨å‡½æ•°å†…éƒ¨ä½¿ç”¨æ¨¡å‹å’Œåˆ†è¯å™¨
    json_post_raw = await request.json()  # è·å–POSTè¯·æ±‚çš„JSONæ•°æ®
    json_post = json.dumps(json_post_raw)  # å°†JSONæ•°æ®è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    json_post_list = json.loads(json_post)  # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºPythonå¯¹è±¡
    prompt = json_post_list.get('prompt')  # è·å–è¯·æ±‚ä¸­çš„æç¤º
    history = json_post_list.get('history', [])  # è·å–è¯·æ±‚ä¸­çš„å†å²è®°å½•

    messages = [
            # {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
    ]

    # è°ƒç”¨æ¨¡å‹è¿›è¡Œå¯¹è¯ç”Ÿæˆ
    input_str = bulid_input(prompt=prompt, history=history)
    input_ids = tokenizer.encode(input_str, add_special_tokens=False, return_tensors='pt').cuda()

    generated_ids = model.generate(
    input_ids=input_ids, max_new_tokens=512, do_sample=True,
    top_p=0.9, temperature=0.5, repetition_penalty=1.1, eos_token_id=tokenizer.encode('<|eot_id|>')[0]
    )
    outputs = generated_ids.tolist()[0][len(input_ids[0]):]
    response = tokenizer.decode(outputs)
    response = response.strip().replace('<|eot_id|>', "").replace('<|start_header_id|>assistant<|end_header_id|>\n\n', '').strip() # è§£æ chat æ¨¡ç‰ˆ


    now = datetime.datetime.now()  # è·å–å½“å‰æ—¶é—´
    time = now.strftime("%Y-%m-%d %H:%M:%S")  # æ ¼å¼åŒ–æ—¶é—´ä¸ºå­—ç¬¦ä¸²
    # æ„å»ºå“åº”JSON
    answer = {
        "response": response,
        "status": 200,
        "time": time
    }
    # æ„å»ºæ—¥å¿—ä¿¡æ¯
    log = "[" + time + "] " + '", prompt:"' + prompt + '", response:"' + repr(response) + '"'
    print(log)  # æ‰“å°æ—¥å¿—
    torch_gc()  # æ‰§è¡ŒGPUå†…å­˜æ¸…ç†
    return answer  # è¿”å›å“åº”

# ä¸»å‡½æ•°å…¥å£
if __name__ == '__main__':
    # åŠ è½½é¢„è®­ç»ƒçš„åˆ†è¯å™¨å’Œæ¨¡å‹
    model_name_or_path = '/root/autodl-tmp/LLM-Research/Meta-Llama-3-8B-Instruct'
    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=False)
    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map="auto", torch_dtype=torch.bfloat16).cuda()

    # å¯åŠ¨FastAPIåº”ç”¨
    # ç”¨6006ç«¯å£å¯ä»¥å°†autodlçš„ç«¯å£æ˜ å°„åˆ°æœ¬åœ°ï¼Œä»è€Œåœ¨æœ¬åœ°ä½¿ç”¨api
    uvicorn.run(app, host='0.0.0.0', port=6006, workers=1)  # åœ¨æŒ‡å®šç«¯å£å’Œä¸»æœºä¸Šå¯åŠ¨åº”ç”¨
```

## Api éƒ¨ç½²

åœ¨ç»ˆç«¯è¾“å…¥ä»¥ä¸‹å‘½ä»¤å¯åŠ¨ api æœåŠ¡ï¼š

```shell
cd /root/autodl-tmp
python api.py
```

åŠ è½½å®Œæ¯•åå‡ºç°å¦‚ä¸‹ä¿¡æ¯è¯´æ˜æˆåŠŸã€‚

![Alt text](images/api_start.png)

é»˜è®¤éƒ¨ç½²åœ¨ 6006 ç«¯å£ï¼Œé€šè¿‡ POST æ–¹æ³•è¿›è¡Œè°ƒç”¨ï¼Œå¯ä»¥ä½¿ç”¨ curl è°ƒç”¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```shell
curl -X POST "http://127.0.0.1:6006" \
     -H 'Content-Type: application/json' \
     -d '{"prompt": "ä½ å¥½"}'
```

å¾—åˆ°çš„è¿”å›å€¼å¦‚ä¸‹æ‰€ç¤ºï¼š

```json
{
  "response": "ğŸ˜Š ä½ å¥½ï¼æˆ‘ä¹Ÿå¾ˆé«˜å…´è§åˆ°ä½ ï¼æœ‰ä»€ä¹ˆé—®é¢˜æˆ–è¯é¢˜æƒ³èŠå¤©å—ï¼Ÿ",
  "status": 200,
  "time": "2024-04-20 23:11:00"
}
```

ä¹Ÿå¯ä»¥ä½¿ç”¨ python ä¸­çš„ requests åº“è¿›è¡Œè°ƒç”¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
import requests
import json

def get_completion(prompt):
    headers = {'Content-Type': 'application/json'}
    data = {"prompt": prompt}
    response = requests.post(url='http://127.0.0.1:6006', headers=headers, data=json.dumps(data))
    return response.json()['response']

if __name__ == '__main__':
    print(get_completion('ä½ å¥½'))
```

![Alt text](images/api_resp.png)
