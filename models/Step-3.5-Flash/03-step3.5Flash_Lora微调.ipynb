{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ee3c193",
   "metadata": {},
   "source": [
    "# 调包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb8fbc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/loraa/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoConfig\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import swanlab\n",
    "from swanlab.integration.transformers import SwanLabCallback\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241e1645",
   "metadata": {},
   "source": [
    "# 训练前的准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4f6cd3",
   "metadata": {},
   "source": [
    "## 变量定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f07d6036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from '/autodl-fs/data/stepfun-ai/Step-3___5-Flash' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='/autodl-fs/data/stepfun-ai/Step-3___5-Flash', vocab_size=128000, model_max_length=131072, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<｜begin▁of▁sentence｜>', 'eos_token': '<|im_end|>', 'pad_token': '<｜end▁of▁sentence｜>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<｜begin▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<｜end▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<｜▁pad▁｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128000: AddedToken(\"<im_start>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128001: AddedToken(\"<im_patch>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128002: AddedToken(\"<im_end>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128003: AddedToken(\"<patch_start>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128004: AddedToken(\"<patch_newline>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128005: AddedToken(\"<patch_end>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128006: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128007: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128008: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t128009: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t128010: AddedToken(\"<tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t128011: AddedToken(\"</tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t128012: AddedToken(\"<tool_calls>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128013: AddedToken(\"</tool_calls>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128014: AddedToken(\"<｜place▁holder▁no▁14｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128015: AddedToken(\"<｜place▁holder▁no▁15｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128016: AddedToken(\"<｜place▁holder▁no▁16｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128017: AddedToken(\"<｜place▁holder▁no▁17｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128018: AddedToken(\"<｜place▁holder▁no▁18｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128019: AddedToken(\"<｜place▁holder▁no▁19｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128020: AddedToken(\"<｜place▁holder▁no▁20｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128021: AddedToken(\"<｜place▁holder▁no▁21｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128022: AddedToken(\"<｜place▁holder▁no▁22｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128023: AddedToken(\"<｜place▁holder▁no▁23｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128024: AddedToken(\"<｜place▁holder▁no▁24｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128025: AddedToken(\"<｜place▁holder▁no▁25｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128026: AddedToken(\"<｜place▁holder▁no▁26｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128027: AddedToken(\"<｜place▁holder▁no▁27｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128028: AddedToken(\"<｜place▁holder▁no▁28｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128029: AddedToken(\"<｜place▁holder▁no▁29｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128030: AddedToken(\"<｜place▁holder▁no▁30｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128031: AddedToken(\"<｜place▁holder▁no▁31｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128032: AddedToken(\"<｜place▁holder▁no▁32｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128033: AddedToken(\"<｜place▁holder▁no▁33｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128034: AddedToken(\"<｜place▁holder▁no▁34｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128035: AddedToken(\"<｜place▁holder▁no▁35｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128036: AddedToken(\"<｜place▁holder▁no▁36｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128037: AddedToken(\"<｜place▁holder▁no▁37｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128038: AddedToken(\"<｜place▁holder▁no▁38｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128039: AddedToken(\"<｜place▁holder▁no▁39｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128040: AddedToken(\"<｜place▁holder▁no▁40｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128041: AddedToken(\"<｜place▁holder▁no▁41｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128042: AddedToken(\"<｜place▁holder▁no▁42｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128043: AddedToken(\"<｜place▁holder▁no▁43｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128044: AddedToken(\"<｜place▁holder▁no▁44｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128045: AddedToken(\"<｜place▁holder▁no▁45｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128046: AddedToken(\"<｜place▁holder▁no▁46｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128047: AddedToken(\"<｜place▁holder▁no▁47｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128048: AddedToken(\"<｜place▁holder▁no▁48｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128049: AddedToken(\"<｜place▁holder▁no▁49｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128050: AddedToken(\"<｜place▁holder▁no▁50｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128051: AddedToken(\"<｜place▁holder▁no▁51｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128052: AddedToken(\"<｜place▁holder▁no▁52｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128053: AddedToken(\"<｜place▁holder▁no▁53｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128054: AddedToken(\"<｜place▁holder▁no▁54｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128055: AddedToken(\"<｜place▁holder▁no▁55｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128056: AddedToken(\"<｜place▁holder▁no▁56｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128057: AddedToken(\"<｜place▁holder▁no▁57｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128058: AddedToken(\"<｜place▁holder▁no▁58｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128059: AddedToken(\"<｜place▁holder▁no▁59｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128060: AddedToken(\"<｜place▁holder▁no▁60｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128061: AddedToken(\"<｜place▁holder▁no▁61｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128062: AddedToken(\"<｜place▁holder▁no▁62｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128063: AddedToken(\"<｜place▁holder▁no▁63｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128064: AddedToken(\"<｜place▁holder▁no▁64｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128065: AddedToken(\"<｜place▁holder▁no▁65｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128066: AddedToken(\"<｜place▁holder▁no▁66｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128067: AddedToken(\"<｜place▁holder▁no▁67｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128068: AddedToken(\"<｜place▁holder▁no▁68｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128069: AddedToken(\"<｜place▁holder▁no▁69｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128070: AddedToken(\"<｜place▁holder▁no▁70｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128071: AddedToken(\"<｜place▁holder▁no▁71｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128072: AddedToken(\"<｜place▁holder▁no▁72｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128073: AddedToken(\"<｜place▁holder▁no▁73｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128074: AddedToken(\"<｜place▁holder▁no▁74｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128075: AddedToken(\"<｜place▁holder▁no▁75｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128076: AddedToken(\"<｜place▁holder▁no▁76｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128077: AddedToken(\"<｜place▁holder▁no▁77｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128078: AddedToken(\"<｜place▁holder▁no▁78｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128079: AddedToken(\"<｜place▁holder▁no▁79｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128080: AddedToken(\"<｜place▁holder▁no▁80｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128081: AddedToken(\"<｜place▁holder▁no▁81｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128082: AddedToken(\"<｜place▁holder▁no▁82｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128083: AddedToken(\"<｜place▁holder▁no▁83｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128084: AddedToken(\"<｜place▁holder▁no▁84｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128085: AddedToken(\"<｜place▁holder▁no▁85｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128086: AddedToken(\"<｜place▁holder▁no▁86｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128087: AddedToken(\"<｜place▁holder▁no▁87｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128088: AddedToken(\"<｜place▁holder▁no▁88｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128089: AddedToken(\"<｜place▁holder▁no▁89｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128090: AddedToken(\"<｜place▁holder▁no▁90｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128091: AddedToken(\"<｜place▁holder▁no▁91｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128092: AddedToken(\"<｜place▁holder▁no▁92｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128093: AddedToken(\"<｜place▁holder▁no▁93｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128094: AddedToken(\"<｜place▁holder▁no▁94｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128095: AddedToken(\"<｜place▁holder▁no▁95｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128096: AddedToken(\"<｜place▁holder▁no▁96｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128097: AddedToken(\"<｜place▁holder▁no▁97｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128098: AddedToken(\"<｜place▁holder▁no▁98｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128099: AddedToken(\"<｜place▁holder▁no▁99｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128100: AddedToken(\"<｜place▁holder▁no▁100｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128101: AddedToken(\"<｜place▁holder▁no▁101｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128102: AddedToken(\"<｜place▁holder▁no▁102｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128103: AddedToken(\"<｜place▁holder▁no▁103｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128104: AddedToken(\"<｜place▁holder▁no▁104｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128105: AddedToken(\"<｜place▁holder▁no▁105｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128106: AddedToken(\"<｜place▁holder▁no▁106｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128107: AddedToken(\"<｜place▁holder▁no▁107｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128108: AddedToken(\"<｜place▁holder▁no▁108｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128109: AddedToken(\"<｜place▁holder▁no▁109｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128110: AddedToken(\"<｜place▁holder▁no▁110｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128111: AddedToken(\"<｜place▁holder▁no▁111｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128112: AddedToken(\"<｜place▁holder▁no▁112｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128113: AddedToken(\"<｜place▁holder▁no▁113｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128114: AddedToken(\"<｜place▁holder▁no▁114｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128115: AddedToken(\"<｜place▁holder▁no▁115｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128116: AddedToken(\"<｜place▁holder▁no▁116｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128117: AddedToken(\"<｜place▁holder▁no▁117｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128118: AddedToken(\"<｜place▁holder▁no▁118｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128119: AddedToken(\"<｜place▁holder▁no▁119｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128120: AddedToken(\"<｜place▁holder▁no▁120｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128121: AddedToken(\"<｜place▁holder▁no▁121｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128122: AddedToken(\"<｜place▁holder▁no▁122｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128123: AddedToken(\"<｜place▁holder▁no▁123｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128124: AddedToken(\"<｜place▁holder▁no▁124｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128125: AddedToken(\"<｜place▁holder▁no▁125｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128126: AddedToken(\"<｜place▁holder▁no▁126｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128127: AddedToken(\"<｜place▁holder▁no▁127｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128128: AddedToken(\"<｜place▁holder▁no▁128｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128129: AddedToken(\"<｜place▁holder▁no▁129｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128130: AddedToken(\"<｜place▁holder▁no▁130｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128131: AddedToken(\"<｜place▁holder▁no▁131｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128132: AddedToken(\"<｜place▁holder▁no▁132｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128133: AddedToken(\"<｜place▁holder▁no▁133｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128134: AddedToken(\"<｜place▁holder▁no▁134｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128135: AddedToken(\"<｜place▁holder▁no▁135｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128136: AddedToken(\"<｜place▁holder▁no▁136｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128137: AddedToken(\"<｜place▁holder▁no▁137｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128138: AddedToken(\"<｜place▁holder▁no▁138｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128139: AddedToken(\"<｜place▁holder▁no▁139｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128140: AddedToken(\"<｜place▁holder▁no▁140｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128141: AddedToken(\"<｜place▁holder▁no▁141｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128142: AddedToken(\"<｜place▁holder▁no▁142｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128143: AddedToken(\"<｜place▁holder▁no▁143｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128144: AddedToken(\"<｜place▁holder▁no▁144｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128145: AddedToken(\"<｜place▁holder▁no▁145｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128146: AddedToken(\"<｜place▁holder▁no▁146｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128147: AddedToken(\"<｜place▁holder▁no▁147｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128148: AddedToken(\"<｜place▁holder▁no▁148｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128149: AddedToken(\"<｜place▁holder▁no▁149｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128150: AddedToken(\"<｜place▁holder▁no▁150｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128151: AddedToken(\"<｜place▁holder▁no▁151｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128152: AddedToken(\"<｜place▁holder▁no▁152｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128153: AddedToken(\"<｜place▁holder▁no▁153｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128154: AddedToken(\"<｜place▁holder▁no▁154｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128155: AddedToken(\"<｜place▁holder▁no▁155｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128156: AddedToken(\"<｜place▁holder▁no▁156｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128157: AddedToken(\"<｜place▁holder▁no▁157｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128158: AddedToken(\"<｜place▁holder▁no▁158｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128159: AddedToken(\"<｜place▁holder▁no▁159｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128160: AddedToken(\"<｜place▁holder▁no▁160｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128161: AddedToken(\"<｜place▁holder▁no▁161｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128162: AddedToken(\"<｜place▁holder▁no▁162｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128163: AddedToken(\"<｜place▁holder▁no▁163｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128164: AddedToken(\"<｜place▁holder▁no▁164｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128165: AddedToken(\"<｜place▁holder▁no▁165｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128166: AddedToken(\"<｜place▁holder▁no▁166｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128167: AddedToken(\"<｜place▁holder▁no▁167｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128168: AddedToken(\"<｜place▁holder▁no▁168｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128169: AddedToken(\"<｜place▁holder▁no▁169｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128170: AddedToken(\"<｜place▁holder▁no▁170｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128171: AddedToken(\"<｜place▁holder▁no▁171｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128172: AddedToken(\"<｜place▁holder▁no▁172｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128173: AddedToken(\"<｜place▁holder▁no▁173｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128174: AddedToken(\"<｜place▁holder▁no▁174｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128175: AddedToken(\"<｜place▁holder▁no▁175｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128176: AddedToken(\"<｜place▁holder▁no▁176｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128177: AddedToken(\"<｜place▁holder▁no▁177｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128178: AddedToken(\"<｜place▁holder▁no▁178｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128179: AddedToken(\"<｜place▁holder▁no▁179｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128180: AddedToken(\"<｜place▁holder▁no▁180｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128181: AddedToken(\"<｜place▁holder▁no▁181｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128182: AddedToken(\"<｜place▁holder▁no▁182｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128183: AddedToken(\"<｜place▁holder▁no▁183｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128184: AddedToken(\"<｜place▁holder▁no▁184｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128185: AddedToken(\"<｜place▁holder▁no▁185｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128186: AddedToken(\"<｜place▁holder▁no▁186｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128187: AddedToken(\"<｜place▁holder▁no▁187｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128188: AddedToken(\"<｜place▁holder▁no▁188｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128189: AddedToken(\"<｜place▁holder▁no▁189｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128190: AddedToken(\"<｜place▁holder▁no▁190｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128191: AddedToken(\"<｜place▁holder▁no▁191｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128192: AddedToken(\"<｜place▁holder▁no▁192｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128193: AddedToken(\"<｜place▁holder▁no▁193｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128194: AddedToken(\"<｜place▁holder▁no▁194｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128195: AddedToken(\"<｜place▁holder▁no▁195｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128196: AddedToken(\"<｜place▁holder▁no▁196｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128197: AddedToken(\"<｜place▁holder▁no▁197｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128198: AddedToken(\"<｜place▁holder▁no▁198｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128199: AddedToken(\"<｜place▁holder▁no▁199｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128200: AddedToken(\"<｜place▁holder▁no▁200｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128201: AddedToken(\"<｜place▁holder▁no▁201｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128202: AddedToken(\"<｜place▁holder▁no▁202｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128203: AddedToken(\"<｜place▁holder▁no▁203｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128204: AddedToken(\"<｜place▁holder▁no▁204｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128205: AddedToken(\"<｜place▁holder▁no▁205｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128206: AddedToken(\"<｜place▁holder▁no▁206｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128207: AddedToken(\"<｜place▁holder▁no▁207｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128208: AddedToken(\"<｜place▁holder▁no▁208｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128209: AddedToken(\"<｜place▁holder▁no▁209｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128210: AddedToken(\"<｜place▁holder▁no▁210｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128211: AddedToken(\"<｜place▁holder▁no▁211｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128212: AddedToken(\"<｜place▁holder▁no▁212｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128213: AddedToken(\"<｜place▁holder▁no▁213｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128214: AddedToken(\"<｜place▁holder▁no▁214｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128215: AddedToken(\"<｜place▁holder▁no▁215｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128216: AddedToken(\"<｜place▁holder▁no▁216｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128217: AddedToken(\"<｜place▁holder▁no▁217｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128218: AddedToken(\"<｜place▁holder▁no▁218｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128219: AddedToken(\"<｜place▁holder▁no▁219｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128220: AddedToken(\"<｜place▁holder▁no▁220｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128221: AddedToken(\"<｜place▁holder▁no▁221｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128222: AddedToken(\"<｜place▁holder▁no▁222｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128223: AddedToken(\"<｜place▁holder▁no▁223｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128224: AddedToken(\"<｜place▁holder▁no▁224｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128225: AddedToken(\"<｜place▁holder▁no▁225｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128226: AddedToken(\"<｜place▁holder▁no▁226｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128227: AddedToken(\"<｜place▁holder▁no▁227｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128228: AddedToken(\"<｜place▁holder▁no▁228｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128229: AddedToken(\"<｜place▁holder▁no▁229｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128230: AddedToken(\"<｜place▁holder▁no▁230｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128231: AddedToken(\"<｜place▁holder▁no▁231｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128232: AddedToken(\"<｜place▁holder▁no▁232｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128233: AddedToken(\"<｜place▁holder▁no▁233｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128234: AddedToken(\"<｜place▁holder▁no▁234｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128235: AddedToken(\"<｜place▁holder▁no▁235｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128236: AddedToken(\"<｜place▁holder▁no▁236｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128237: AddedToken(\"<｜place▁holder▁no▁237｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128238: AddedToken(\"<｜place▁holder▁no▁238｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128239: AddedToken(\"<｜place▁holder▁no▁239｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128240: AddedToken(\"<｜place▁holder▁no▁240｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128241: AddedToken(\"<｜place▁holder▁no▁241｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128242: AddedToken(\"<｜place▁holder▁no▁242｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128243: AddedToken(\"<｜place▁holder▁no▁243｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128244: AddedToken(\"<｜place▁holder▁no▁244｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128245: AddedToken(\"<｜place▁holder▁no▁245｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128246: AddedToken(\"<｜place▁holder▁no▁246｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128247: AddedToken(\"<｜place▁holder▁no▁247｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128248: AddedToken(\"<｜place▁holder▁no▁248｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128249: AddedToken(\"<｜place▁holder▁no▁249｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128250: AddedToken(\"<｜place▁holder▁no▁250｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128251: AddedToken(\"<｜place▁holder▁no▁251｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128252: AddedToken(\"<｜place▁holder▁no▁252｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128253: AddedToken(\"<｜place▁holder▁no▁253｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128254: AddedToken(\"<｜place▁holder▁no▁254｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128255: AddedToken(\"<｜place▁holder▁no▁255｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128256: AddedToken(\"<｜place▁holder▁no▁256｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128257: AddedToken(\"<｜place▁holder▁no▁257｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128258: AddedToken(\"<｜place▁holder▁no▁258｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128259: AddedToken(\"<｜place▁holder▁no▁259｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128260: AddedToken(\"<｜place▁holder▁no▁260｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128261: AddedToken(\"<｜place▁holder▁no▁261｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128262: AddedToken(\"<｜place▁holder▁no▁262｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128263: AddedToken(\"<｜place▁holder▁no▁263｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128264: AddedToken(\"<｜place▁holder▁no▁264｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128265: AddedToken(\"<｜place▁holder▁no▁265｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128266: AddedToken(\"<｜place▁holder▁no▁266｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128267: AddedToken(\"<｜place▁holder▁no▁267｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128268: AddedToken(\"<｜place▁holder▁no▁268｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128269: AddedToken(\"<｜place▁holder▁no▁269｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128270: AddedToken(\"<｜place▁holder▁no▁270｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128271: AddedToken(\"<｜place▁holder▁no▁271｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128272: AddedToken(\"<｜place▁holder▁no▁272｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128273: AddedToken(\"<｜place▁holder▁no▁273｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128274: AddedToken(\"<｜place▁holder▁no▁274｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128275: AddedToken(\"<｜place▁holder▁no▁275｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128276: AddedToken(\"<｜place▁holder▁no▁276｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128277: AddedToken(\"<｜place▁holder▁no▁277｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128278: AddedToken(\"<｜place▁holder▁no▁278｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128279: AddedToken(\"<｜place▁holder▁no▁279｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128280: AddedToken(\"<｜place▁holder▁no▁280｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128281: AddedToken(\"<｜place▁holder▁no▁281｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128282: AddedToken(\"<｜place▁holder▁no▁282｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128283: AddedToken(\"<｜place▁holder▁no▁283｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128284: AddedToken(\"<｜place▁holder▁no▁284｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128285: AddedToken(\"<｜place▁holder▁no▁285｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128286: AddedToken(\"<｜place▁holder▁no▁286｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128287: AddedToken(\"<｜place▁holder▁no▁287｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128288: AddedToken(\"<｜place▁holder▁no▁288｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128289: AddedToken(\"<｜place▁holder▁no▁289｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128290: AddedToken(\"<｜place▁holder▁no▁290｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128291: AddedToken(\"<｜place▁holder▁no▁291｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128292: AddedToken(\"<｜place▁holder▁no▁292｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128293: AddedToken(\"<｜place▁holder▁no▁293｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128294: AddedToken(\"<｜place▁holder▁no▁294｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128295: AddedToken(\"<｜place▁holder▁no▁295｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128296: AddedToken(\"<｜place▁holder▁no▁296｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128297: AddedToken(\"<｜place▁holder▁no▁297｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128298: AddedToken(\"<｜place▁holder▁no▁298｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128299: AddedToken(\"<｜place▁holder▁no▁299｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128300: AddedToken(\"<｜place▁holder▁no▁300｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128301: AddedToken(\"<｜place▁holder▁no▁301｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128302: AddedToken(\"<｜place▁holder▁no▁302｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128303: AddedToken(\"<｜place▁holder▁no▁303｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128304: AddedToken(\"<｜place▁holder▁no▁304｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128305: AddedToken(\"<｜place▁holder▁no▁305｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128306: AddedToken(\"<｜place▁holder▁no▁306｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128307: AddedToken(\"<｜place▁holder▁no▁307｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128308: AddedToken(\"<｜place▁holder▁no▁308｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128309: AddedToken(\"<｜place▁holder▁no▁309｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128310: AddedToken(\"<｜place▁holder▁no▁310｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128311: AddedToken(\"<｜place▁holder▁no▁311｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128312: AddedToken(\"<｜place▁holder▁no▁312｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128313: AddedToken(\"<｜place▁holder▁no▁313｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128314: AddedToken(\"<｜place▁holder▁no▁314｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128315: AddedToken(\"<｜place▁holder▁no▁315｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128316: AddedToken(\"<｜place▁holder▁no▁316｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128317: AddedToken(\"<｜place▁holder▁no▁317｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128318: AddedToken(\"<｜place▁holder▁no▁318｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128319: AddedToken(\"<｜place▁holder▁no▁319｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128320: AddedToken(\"<｜place▁holder▁no▁320｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128321: AddedToken(\"<｜place▁holder▁no▁321｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128322: AddedToken(\"<｜place▁holder▁no▁322｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128323: AddedToken(\"<｜place▁holder▁no▁323｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128324: AddedToken(\"<｜place▁holder▁no▁324｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128325: AddedToken(\"<｜place▁holder▁no▁325｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128326: AddedToken(\"<｜place▁holder▁no▁326｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128327: AddedToken(\"<｜place▁holder▁no▁327｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128328: AddedToken(\"<｜place▁holder▁no▁328｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128329: AddedToken(\"<｜place▁holder▁no▁329｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128330: AddedToken(\"<｜place▁holder▁no▁330｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128331: AddedToken(\"<｜place▁holder▁no▁331｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128332: AddedToken(\"<｜place▁holder▁no▁332｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128333: AddedToken(\"<｜place▁holder▁no▁333｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128334: AddedToken(\"<｜place▁holder▁no▁334｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128335: AddedToken(\"<｜place▁holder▁no▁335｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128336: AddedToken(\"<｜place▁holder▁no▁336｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128337: AddedToken(\"<｜place▁holder▁no▁337｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128338: AddedToken(\"<｜place▁holder▁no▁338｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128339: AddedToken(\"<｜place▁holder▁no▁339｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128340: AddedToken(\"<｜place▁holder▁no▁340｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128341: AddedToken(\"<｜place▁holder▁no▁341｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128342: AddedToken(\"<｜place▁holder▁no▁342｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128343: AddedToken(\"<｜place▁holder▁no▁343｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128344: AddedToken(\"<｜place▁holder▁no▁344｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128345: AddedToken(\"<｜place▁holder▁no▁345｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128346: AddedToken(\"<｜place▁holder▁no▁346｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128347: AddedToken(\"<｜place▁holder▁no▁347｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128348: AddedToken(\"<｜place▁holder▁no▁348｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128349: AddedToken(\"<｜place▁holder▁no▁349｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128350: AddedToken(\"<｜place▁holder▁no▁350｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128351: AddedToken(\"<｜place▁holder▁no▁351｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128352: AddedToken(\"<｜place▁holder▁no▁352｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128353: AddedToken(\"<｜place▁holder▁no▁353｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128354: AddedToken(\"<｜place▁holder▁no▁354｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128355: AddedToken(\"<｜place▁holder▁no▁355｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128356: AddedToken(\"<｜place▁holder▁no▁356｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128357: AddedToken(\"<｜place▁holder▁no▁357｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128358: AddedToken(\"<｜place▁holder▁no▁358｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128359: AddedToken(\"<｜place▁holder▁no▁359｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128360: AddedToken(\"<｜place▁holder▁no▁360｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128361: AddedToken(\"<｜place▁holder▁no▁361｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128362: AddedToken(\"<｜place▁holder▁no▁362｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128363: AddedToken(\"<｜place▁holder▁no▁363｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128364: AddedToken(\"<｜place▁holder▁no▁364｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128365: AddedToken(\"<｜place▁holder▁no▁365｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128366: AddedToken(\"<｜place▁holder▁no▁366｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128367: AddedToken(\"<｜place▁holder▁no▁367｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128368: AddedToken(\"<｜place▁holder▁no▁368｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128369: AddedToken(\"<｜place▁holder▁no▁369｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128370: AddedToken(\"<｜place▁holder▁no▁370｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128371: AddedToken(\"<｜place▁holder▁no▁371｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128372: AddedToken(\"<｜place▁holder▁no▁372｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128373: AddedToken(\"<｜place▁holder▁no▁373｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128374: AddedToken(\"<｜place▁holder▁no▁374｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128375: AddedToken(\"<｜place▁holder▁no▁375｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128376: AddedToken(\"<｜place▁holder▁no▁376｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128377: AddedToken(\"<｜place▁holder▁no▁377｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128378: AddedToken(\"<｜place▁holder▁no▁378｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128379: AddedToken(\"<｜place▁holder▁no▁379｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128380: AddedToken(\"<｜place▁holder▁no▁380｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128381: AddedToken(\"<｜place▁holder▁no▁381｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128382: AddedToken(\"<｜place▁holder▁no▁382｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128383: AddedToken(\"<｜place▁holder▁no▁383｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128384: AddedToken(\"<｜place▁holder▁no▁384｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128385: AddedToken(\"<｜place▁holder▁no▁385｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128386: AddedToken(\"<｜place▁holder▁no▁386｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128387: AddedToken(\"<｜place▁holder▁no▁387｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128388: AddedToken(\"<｜place▁holder▁no▁388｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128389: AddedToken(\"<｜place▁holder▁no▁389｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128390: AddedToken(\"<｜place▁holder▁no▁390｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128391: AddedToken(\"<｜place▁holder▁no▁391｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128392: AddedToken(\"<｜place▁holder▁no▁392｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128393: AddedToken(\"<｜place▁holder▁no▁393｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128394: AddedToken(\"<｜place▁holder▁no▁394｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128395: AddedToken(\"<｜place▁holder▁no▁395｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128396: AddedToken(\"<｜place▁holder▁no▁396｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128397: AddedToken(\"<｜place▁holder▁no▁397｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128398: AddedToken(\"<｜place▁holder▁no▁398｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128399: AddedToken(\"<｜place▁holder▁no▁399｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128400: AddedToken(\"<｜place▁holder▁no▁400｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128401: AddedToken(\"<｜place▁holder▁no▁401｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128402: AddedToken(\"<｜place▁holder▁no▁402｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128403: AddedToken(\"<｜place▁holder▁no▁403｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128404: AddedToken(\"<｜place▁holder▁no▁404｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128405: AddedToken(\"<｜place▁holder▁no▁405｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128406: AddedToken(\"<｜place▁holder▁no▁406｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128407: AddedToken(\"<｜place▁holder▁no▁407｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128408: AddedToken(\"<｜place▁holder▁no▁408｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128409: AddedToken(\"<｜place▁holder▁no▁409｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128410: AddedToken(\"<｜place▁holder▁no▁410｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128411: AddedToken(\"<｜place▁holder▁no▁411｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128412: AddedToken(\"<｜place▁holder▁no▁412｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128413: AddedToken(\"<｜place▁holder▁no▁413｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128414: AddedToken(\"<｜place▁holder▁no▁414｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128415: AddedToken(\"<｜place▁holder▁no▁415｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128416: AddedToken(\"<｜place▁holder▁no▁416｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128417: AddedToken(\"<｜place▁holder▁no▁417｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128418: AddedToken(\"<｜place▁holder▁no▁418｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128419: AddedToken(\"<｜place▁holder▁no▁419｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128420: AddedToken(\"<｜place▁holder▁no▁420｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128421: AddedToken(\"<｜place▁holder▁no▁421｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128422: AddedToken(\"<｜place▁holder▁no▁422｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128423: AddedToken(\"<｜place▁holder▁no▁423｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128424: AddedToken(\"<｜place▁holder▁no▁424｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128425: AddedToken(\"<｜place▁holder▁no▁425｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128426: AddedToken(\"<｜place▁holder▁no▁426｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128427: AddedToken(\"<｜place▁holder▁no▁427｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128428: AddedToken(\"<｜place▁holder▁no▁428｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128429: AddedToken(\"<｜place▁holder▁no▁429｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128430: AddedToken(\"<｜place▁holder▁no▁430｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128431: AddedToken(\"<｜place▁holder▁no▁431｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128432: AddedToken(\"<｜place▁holder▁no▁432｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128433: AddedToken(\"<｜place▁holder▁no▁433｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128434: AddedToken(\"<｜place▁holder▁no▁434｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128435: AddedToken(\"<｜place▁holder▁no▁435｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128436: AddedToken(\"<｜place▁holder▁no▁436｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128437: AddedToken(\"<｜place▁holder▁no▁437｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128438: AddedToken(\"<｜place▁holder▁no▁438｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128439: AddedToken(\"<｜place▁holder▁no▁439｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128440: AddedToken(\"<｜place▁holder▁no▁440｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128441: AddedToken(\"<｜place▁holder▁no▁441｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128442: AddedToken(\"<｜place▁holder▁no▁442｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128443: AddedToken(\"<｜place▁holder▁no▁443｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128444: AddedToken(\"<｜place▁holder▁no▁444｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128445: AddedToken(\"<｜place▁holder▁no▁445｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128446: AddedToken(\"<｜place▁holder▁no▁446｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128447: AddedToken(\"<｜place▁holder▁no▁447｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128448: AddedToken(\"<｜place▁holder▁no▁448｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128449: AddedToken(\"<｜place▁holder▁no▁449｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128450: AddedToken(\"<｜place▁holder▁no▁450｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128451: AddedToken(\"<｜place▁holder▁no▁451｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128452: AddedToken(\"<｜place▁holder▁no▁452｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128453: AddedToken(\"<｜place▁holder▁no▁453｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128454: AddedToken(\"<｜place▁holder▁no▁454｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128455: AddedToken(\"<｜place▁holder▁no▁455｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128456: AddedToken(\"<｜place▁holder▁no▁456｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128457: AddedToken(\"<｜place▁holder▁no▁457｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128458: AddedToken(\"<｜place▁holder▁no▁458｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128459: AddedToken(\"<｜place▁holder▁no▁459｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128460: AddedToken(\"<｜place▁holder▁no▁460｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128461: AddedToken(\"<｜place▁holder▁no▁461｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128462: AddedToken(\"<｜place▁holder▁no▁462｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128463: AddedToken(\"<｜place▁holder▁no▁463｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128464: AddedToken(\"<｜place▁holder▁no▁464｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128465: AddedToken(\"<｜place▁holder▁no▁465｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128466: AddedToken(\"<｜place▁holder▁no▁466｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128467: AddedToken(\"<｜place▁holder▁no▁467｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128468: AddedToken(\"<｜place▁holder▁no▁468｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128469: AddedToken(\"<｜place▁holder▁no▁469｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128470: AddedToken(\"<｜place▁holder▁no▁470｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128471: AddedToken(\"<｜place▁holder▁no▁471｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128472: AddedToken(\"<｜place▁holder▁no▁472｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128473: AddedToken(\"<｜place▁holder▁no▁473｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128474: AddedToken(\"<｜place▁holder▁no▁474｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128475: AddedToken(\"<｜place▁holder▁no▁475｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128476: AddedToken(\"<｜place▁holder▁no▁476｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128477: AddedToken(\"<｜place▁holder▁no▁477｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128478: AddedToken(\"<｜place▁holder▁no▁478｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128479: AddedToken(\"<｜place▁holder▁no▁479｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128480: AddedToken(\"<｜place▁holder▁no▁480｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128481: AddedToken(\"<｜place▁holder▁no▁481｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128482: AddedToken(\"<｜place▁holder▁no▁482｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128483: AddedToken(\"<｜place▁holder▁no▁483｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128484: AddedToken(\"<｜place▁holder▁no▁484｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128485: AddedToken(\"<｜place▁holder▁no▁485｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128486: AddedToken(\"<｜place▁holder▁no▁486｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128487: AddedToken(\"<｜place▁holder▁no▁487｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128488: AddedToken(\"<｜place▁holder▁no▁488｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128489: AddedToken(\"<｜place▁holder▁no▁489｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128490: AddedToken(\"<｜place▁holder▁no▁490｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128491: AddedToken(\"<｜place▁holder▁no▁491｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128492: AddedToken(\"<｜place▁holder▁no▁492｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128493: AddedToken(\"<｜place▁holder▁no▁493｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128494: AddedToken(\"<｜place▁holder▁no▁494｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128495: AddedToken(\"<｜place▁holder▁no▁495｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128496: AddedToken(\"<｜place▁holder▁no▁496｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128497: AddedToken(\"<｜place▁holder▁no▁497｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128498: AddedToken(\"<｜place▁holder▁no▁498｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128499: AddedToken(\"<｜place▁holder▁no▁499｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128500: AddedToken(\"<｜place▁holder▁no▁500｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128501: AddedToken(\"<｜place▁holder▁no▁501｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128502: AddedToken(\"<｜place▁holder▁no▁502｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128503: AddedToken(\"<｜place▁holder▁no▁503｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128504: AddedToken(\"<｜place▁holder▁no▁504｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128505: AddedToken(\"<｜place▁holder▁no▁505｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128506: AddedToken(\"<｜place▁holder▁no▁506｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128507: AddedToken(\"<｜place▁holder▁no▁507｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128508: AddedToken(\"<｜place▁holder▁no▁508｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128509: AddedToken(\"<｜place▁holder▁no▁509｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128510: AddedToken(\"<｜place▁holder▁no▁510｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128511: AddedToken(\"<｜place▁holder▁no▁511｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128512: AddedToken(\"<｜place▁holder▁no▁512｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128513: AddedToken(\"<｜place▁holder▁no▁513｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128514: AddedToken(\"<｜place▁holder▁no▁514｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128515: AddedToken(\"<｜place▁holder▁no▁515｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128516: AddedToken(\"<｜place▁holder▁no▁516｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128517: AddedToken(\"<｜place▁holder▁no▁517｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128518: AddedToken(\"<｜place▁holder▁no▁518｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128519: AddedToken(\"<｜place▁holder▁no▁519｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128520: AddedToken(\"<｜place▁holder▁no▁520｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128521: AddedToken(\"<｜place▁holder▁no▁521｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128522: AddedToken(\"<｜place▁holder▁no▁522｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128523: AddedToken(\"<｜place▁holder▁no▁523｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128524: AddedToken(\"<｜place▁holder▁no▁524｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128525: AddedToken(\"<｜place▁holder▁no▁525｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128526: AddedToken(\"<｜place▁holder▁no▁526｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128527: AddedToken(\"<｜place▁holder▁no▁527｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128528: AddedToken(\"<｜place▁holder▁no▁528｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128529: AddedToken(\"<｜place▁holder▁no▁529｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128530: AddedToken(\"<｜place▁holder▁no▁530｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128531: AddedToken(\"<｜place▁holder▁no▁531｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128532: AddedToken(\"<｜place▁holder▁no▁532｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128533: AddedToken(\"<｜place▁holder▁no▁533｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128534: AddedToken(\"<｜place▁holder▁no▁534｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128535: AddedToken(\"<｜place▁holder▁no▁535｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128536: AddedToken(\"<｜place▁holder▁no▁536｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128537: AddedToken(\"<｜place▁holder▁no▁537｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128538: AddedToken(\"<｜place▁holder▁no▁538｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128539: AddedToken(\"<｜place▁holder▁no▁539｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128540: AddedToken(\"<｜place▁holder▁no▁540｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128541: AddedToken(\"<｜place▁holder▁no▁541｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128542: AddedToken(\"<｜place▁holder▁no▁542｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128543: AddedToken(\"<｜place▁holder▁no▁543｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128544: AddedToken(\"<｜place▁holder▁no▁544｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128545: AddedToken(\"<｜place▁holder▁no▁545｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128546: AddedToken(\"<｜place▁holder▁no▁546｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128547: AddedToken(\"<｜place▁holder▁no▁547｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128548: AddedToken(\"<｜place▁holder▁no▁548｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128549: AddedToken(\"<｜place▁holder▁no▁549｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128550: AddedToken(\"<｜place▁holder▁no▁550｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128551: AddedToken(\"<｜place▁holder▁no▁551｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128552: AddedToken(\"<｜place▁holder▁no▁552｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128553: AddedToken(\"<｜place▁holder▁no▁553｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128554: AddedToken(\"<｜place▁holder▁no▁554｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128555: AddedToken(\"<｜place▁holder▁no▁555｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128556: AddedToken(\"<｜place▁holder▁no▁556｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128557: AddedToken(\"<｜place▁holder▁no▁557｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128558: AddedToken(\"<｜place▁holder▁no▁558｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128559: AddedToken(\"<｜place▁holder▁no▁559｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128560: AddedToken(\"<｜place▁holder▁no▁560｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128561: AddedToken(\"<｜place▁holder▁no▁561｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128562: AddedToken(\"<｜place▁holder▁no▁562｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128563: AddedToken(\"<｜place▁holder▁no▁563｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128564: AddedToken(\"<｜place▁holder▁no▁564｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128565: AddedToken(\"<｜place▁holder▁no▁565｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128566: AddedToken(\"<｜place▁holder▁no▁566｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128567: AddedToken(\"<｜place▁holder▁no▁567｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128568: AddedToken(\"<｜place▁holder▁no▁568｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128569: AddedToken(\"<｜place▁holder▁no▁569｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128570: AddedToken(\"<｜place▁holder▁no▁570｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128571: AddedToken(\"<｜place▁holder▁no▁571｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128572: AddedToken(\"<｜place▁holder▁no▁572｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128573: AddedToken(\"<｜place▁holder▁no▁573｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128574: AddedToken(\"<｜place▁holder▁no▁574｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128575: AddedToken(\"<｜place▁holder▁no▁575｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128576: AddedToken(\"<｜place▁holder▁no▁576｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128577: AddedToken(\"<｜place▁holder▁no▁577｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128578: AddedToken(\"<｜place▁holder▁no▁578｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128579: AddedToken(\"<｜place▁holder▁no▁579｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128580: AddedToken(\"<｜place▁holder▁no▁580｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128581: AddedToken(\"<｜place▁holder▁no▁581｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128582: AddedToken(\"<｜place▁holder▁no▁582｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128583: AddedToken(\"<｜place▁holder▁no▁583｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128584: AddedToken(\"<｜place▁holder▁no▁584｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128585: AddedToken(\"<｜place▁holder▁no▁585｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128586: AddedToken(\"<｜place▁holder▁no▁586｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128587: AddedToken(\"<｜place▁holder▁no▁587｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128588: AddedToken(\"<｜place▁holder▁no▁588｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128589: AddedToken(\"<｜place▁holder▁no▁589｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128590: AddedToken(\"<｜place▁holder▁no▁590｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128591: AddedToken(\"<｜place▁holder▁no▁591｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128592: AddedToken(\"<｜place▁holder▁no▁592｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128593: AddedToken(\"<｜place▁holder▁no▁593｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128594: AddedToken(\"<｜place▁holder▁no▁594｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128595: AddedToken(\"<｜place▁holder▁no▁595｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128596: AddedToken(\"<｜place▁holder▁no▁596｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128597: AddedToken(\"<｜place▁holder▁no▁597｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128598: AddedToken(\"<｜place▁holder▁no▁598｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128599: AddedToken(\"<｜place▁holder▁no▁599｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128600: AddedToken(\"<｜place▁holder▁no▁600｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128601: AddedToken(\"<｜place▁holder▁no▁601｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128602: AddedToken(\"<｜place▁holder▁no▁602｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128603: AddedToken(\"<｜place▁holder▁no▁603｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128604: AddedToken(\"<｜place▁holder▁no▁604｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128605: AddedToken(\"<｜place▁holder▁no▁605｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128606: AddedToken(\"<｜place▁holder▁no▁606｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128607: AddedToken(\"<｜place▁holder▁no▁607｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128608: AddedToken(\"<｜place▁holder▁no▁608｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128609: AddedToken(\"<｜place▁holder▁no▁609｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128610: AddedToken(\"<｜place▁holder▁no▁610｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128611: AddedToken(\"<｜place▁holder▁no▁611｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128612: AddedToken(\"<｜place▁holder▁no▁612｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128613: AddedToken(\"<｜place▁holder▁no▁613｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128614: AddedToken(\"<｜place▁holder▁no▁614｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128615: AddedToken(\"<｜place▁holder▁no▁615｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128616: AddedToken(\"<｜place▁holder▁no▁616｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128617: AddedToken(\"<｜place▁holder▁no▁617｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128618: AddedToken(\"<｜place▁holder▁no▁618｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128619: AddedToken(\"<｜place▁holder▁no▁619｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128620: AddedToken(\"<｜place▁holder▁no▁620｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128621: AddedToken(\"<｜place▁holder▁no▁621｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128622: AddedToken(\"<｜place▁holder▁no▁622｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128623: AddedToken(\"<｜place▁holder▁no▁623｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128624: AddedToken(\"<｜place▁holder▁no▁624｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128625: AddedToken(\"<｜place▁holder▁no▁625｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128626: AddedToken(\"<｜place▁holder▁no▁626｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128627: AddedToken(\"<｜place▁holder▁no▁627｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128628: AddedToken(\"<｜place▁holder▁no▁628｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128629: AddedToken(\"<｜place▁holder▁no▁629｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128630: AddedToken(\"<｜place▁holder▁no▁630｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128631: AddedToken(\"<｜place▁holder▁no▁631｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128632: AddedToken(\"<｜place▁holder▁no▁632｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128633: AddedToken(\"<｜place▁holder▁no▁633｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128634: AddedToken(\"<｜place▁holder▁no▁634｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128635: AddedToken(\"<｜place▁holder▁no▁635｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128636: AddedToken(\"<｜place▁holder▁no▁636｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128637: AddedToken(\"<｜place▁holder▁no▁637｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128638: AddedToken(\"<｜place▁holder▁no▁638｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128639: AddedToken(\"<｜place▁holder▁no▁639｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128640: AddedToken(\"<｜place▁holder▁no▁640｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128641: AddedToken(\"<｜place▁holder▁no▁641｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128642: AddedToken(\"<｜place▁holder▁no▁642｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128643: AddedToken(\"<｜place▁holder▁no▁643｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128644: AddedToken(\"<｜place▁holder▁no▁644｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128645: AddedToken(\"<｜place▁holder▁no▁645｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128646: AddedToken(\"<｜place▁holder▁no▁646｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128647: AddedToken(\"<｜place▁holder▁no▁647｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128648: AddedToken(\"<｜place▁holder▁no▁648｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128649: AddedToken(\"<｜place▁holder▁no▁649｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128650: AddedToken(\"<｜place▁holder▁no▁650｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128651: AddedToken(\"<｜place▁holder▁no▁651｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128652: AddedToken(\"<｜place▁holder▁no▁652｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128653: AddedToken(\"<｜place▁holder▁no▁653｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128654: AddedToken(\"<｜place▁holder▁no▁654｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128655: AddedToken(\"<｜place▁holder▁no▁655｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128656: AddedToken(\"<｜place▁holder▁no▁656｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128657: AddedToken(\"<｜place▁holder▁no▁657｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128658: AddedToken(\"<｜place▁holder▁no▁658｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128659: AddedToken(\"<｜place▁holder▁no▁659｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128660: AddedToken(\"<｜place▁holder▁no▁660｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128661: AddedToken(\"<｜place▁holder▁no▁661｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128662: AddedToken(\"<｜place▁holder▁no▁662｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128663: AddedToken(\"<｜place▁holder▁no▁663｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128664: AddedToken(\"<｜place▁holder▁no▁664｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128665: AddedToken(\"<｜place▁holder▁no▁665｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128666: AddedToken(\"<｜place▁holder▁no▁666｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128667: AddedToken(\"<｜place▁holder▁no▁667｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128668: AddedToken(\"<｜place▁holder▁no▁668｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128669: AddedToken(\"<｜place▁holder▁no▁669｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128670: AddedToken(\"<｜place▁holder▁no▁670｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128671: AddedToken(\"<｜place▁holder▁no▁671｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128672: AddedToken(\"<｜place▁holder▁no▁672｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128673: AddedToken(\"<｜place▁holder▁no▁673｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128674: AddedToken(\"<｜place▁holder▁no▁674｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128675: AddedToken(\"<｜place▁holder▁no▁675｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128676: AddedToken(\"<｜place▁holder▁no▁676｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128677: AddedToken(\"<｜place▁holder▁no▁677｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128678: AddedToken(\"<｜place▁holder▁no▁678｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128679: AddedToken(\"<｜place▁holder▁no▁679｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128680: AddedToken(\"<｜place▁holder▁no▁680｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128681: AddedToken(\"<｜place▁holder▁no▁681｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128682: AddedToken(\"<｜place▁holder▁no▁682｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128683: AddedToken(\"<｜place▁holder▁no▁683｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128684: AddedToken(\"<｜place▁holder▁no▁684｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128685: AddedToken(\"<｜place▁holder▁no▁685｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128686: AddedToken(\"<｜place▁holder▁no▁686｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128687: AddedToken(\"<｜place▁holder▁no▁687｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128688: AddedToken(\"<｜place▁holder▁no▁688｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128689: AddedToken(\"<｜place▁holder▁no▁689｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128690: AddedToken(\"<｜place▁holder▁no▁690｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128691: AddedToken(\"<｜place▁holder▁no▁691｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128692: AddedToken(\"<｜place▁holder▁no▁692｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128693: AddedToken(\"<｜place▁holder▁no▁693｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128694: AddedToken(\"<｜place▁holder▁no▁694｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128695: AddedToken(\"<｜place▁holder▁no▁695｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128696: AddedToken(\"<｜place▁holder▁no▁696｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128697: AddedToken(\"<｜place▁holder▁no▁697｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128698: AddedToken(\"<｜place▁holder▁no▁698｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128699: AddedToken(\"<｜place▁holder▁no▁699｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128700: AddedToken(\"<｜place▁holder▁no▁700｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128701: AddedToken(\"<｜place▁holder▁no▁701｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128702: AddedToken(\"<｜place▁holder▁no▁702｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128703: AddedToken(\"<｜place▁holder▁no▁703｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128704: AddedToken(\"<｜place▁holder▁no▁704｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128705: AddedToken(\"<｜place▁holder▁no▁705｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128706: AddedToken(\"<｜place▁holder▁no▁706｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128707: AddedToken(\"<｜place▁holder▁no▁707｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128708: AddedToken(\"<｜place▁holder▁no▁708｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128709: AddedToken(\"<｜place▁holder▁no▁709｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128710: AddedToken(\"<｜place▁holder▁no▁710｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128711: AddedToken(\"<｜place▁holder▁no▁711｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128712: AddedToken(\"<｜place▁holder▁no▁712｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128713: AddedToken(\"<｜place▁holder▁no▁713｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128714: AddedToken(\"<｜place▁holder▁no▁714｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128715: AddedToken(\"<｜place▁holder▁no▁715｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128716: AddedToken(\"<｜place▁holder▁no▁716｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128717: AddedToken(\"<｜place▁holder▁no▁717｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128718: AddedToken(\"<｜place▁holder▁no▁718｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128719: AddedToken(\"<｜place▁holder▁no▁719｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128720: AddedToken(\"<｜place▁holder▁no▁720｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128721: AddedToken(\"<｜place▁holder▁no▁721｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128722: AddedToken(\"<｜place▁holder▁no▁722｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128723: AddedToken(\"<｜place▁holder▁no▁723｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128724: AddedToken(\"<｜place▁holder▁no▁724｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128725: AddedToken(\"<｜place▁holder▁no▁725｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128726: AddedToken(\"<｜place▁holder▁no▁726｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128727: AddedToken(\"<｜place▁holder▁no▁727｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128728: AddedToken(\"<｜place▁holder▁no▁728｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128729: AddedToken(\"<｜place▁holder▁no▁729｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128730: AddedToken(\"<｜place▁holder▁no▁730｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128731: AddedToken(\"<｜place▁holder▁no▁731｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128732: AddedToken(\"<｜place▁holder▁no▁732｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128733: AddedToken(\"<｜place▁holder▁no▁733｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128734: AddedToken(\"<｜place▁holder▁no▁734｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128735: AddedToken(\"<｜place▁holder▁no▁735｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128736: AddedToken(\"<｜place▁holder▁no▁736｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128737: AddedToken(\"<｜place▁holder▁no▁737｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128738: AddedToken(\"<｜place▁holder▁no▁738｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128739: AddedToken(\"<｜place▁holder▁no▁739｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128740: AddedToken(\"<｜place▁holder▁no▁740｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128741: AddedToken(\"<｜place▁holder▁no▁741｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128742: AddedToken(\"<｜place▁holder▁no▁742｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128743: AddedToken(\"<｜place▁holder▁no▁743｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128744: AddedToken(\"<｜place▁holder▁no▁744｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128745: AddedToken(\"<｜place▁holder▁no▁745｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128746: AddedToken(\"<｜place▁holder▁no▁746｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128747: AddedToken(\"<｜place▁holder▁no▁747｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128748: AddedToken(\"<｜place▁holder▁no▁748｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128749: AddedToken(\"<｜place▁holder▁no▁749｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128750: AddedToken(\"<｜place▁holder▁no▁750｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128751: AddedToken(\"<｜place▁holder▁no▁751｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128752: AddedToken(\"<｜place▁holder▁no▁752｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128753: AddedToken(\"<｜place▁holder▁no▁753｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128754: AddedToken(\"<｜place▁holder▁no▁754｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128755: AddedToken(\"<｜place▁holder▁no▁755｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128756: AddedToken(\"<｜place▁holder▁no▁756｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128757: AddedToken(\"<｜place▁holder▁no▁757｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128758: AddedToken(\"<｜place▁holder▁no▁758｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128759: AddedToken(\"<｜place▁holder▁no▁759｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128760: AddedToken(\"<｜place▁holder▁no▁760｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128761: AddedToken(\"<｜place▁holder▁no▁761｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128762: AddedToken(\"<｜place▁holder▁no▁762｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128763: AddedToken(\"<｜place▁holder▁no▁763｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128764: AddedToken(\"<｜place▁holder▁no▁764｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128765: AddedToken(\"<｜place▁holder▁no▁765｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128766: AddedToken(\"<｜place▁holder▁no▁766｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128767: AddedToken(\"<｜place▁holder▁no▁767｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128768: AddedToken(\"<｜place▁holder▁no▁768｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128769: AddedToken(\"<｜place▁holder▁no▁769｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128770: AddedToken(\"<｜place▁holder▁no▁770｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128771: AddedToken(\"<｜place▁holder▁no▁771｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128772: AddedToken(\"<｜place▁holder▁no▁772｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128773: AddedToken(\"<｜place▁holder▁no▁773｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128774: AddedToken(\"<｜place▁holder▁no▁774｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128775: AddedToken(\"<｜place▁holder▁no▁775｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128776: AddedToken(\"<｜place▁holder▁no▁776｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128777: AddedToken(\"<｜place▁holder▁no▁777｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128778: AddedToken(\"<｜place▁holder▁no▁778｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128779: AddedToken(\"<｜place▁holder▁no▁779｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128780: AddedToken(\"<｜place▁holder▁no▁780｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128781: AddedToken(\"<｜place▁holder▁no▁781｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128782: AddedToken(\"<｜place▁holder▁no▁782｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128783: AddedToken(\"<|BOT|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128784: AddedToken(\"<｜place▁holder▁no▁784｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128785: AddedToken(\"<｜place▁holder▁no▁785｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128786: AddedToken(\"<｜place▁holder▁no▁786｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128787: AddedToken(\"<｜place▁holder▁no▁787｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128788: AddedToken(\"<｜place▁holder▁no▁788｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128789: AddedToken(\"<｜place▁holder▁no▁789｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128790: AddedToken(\"<｜place▁holder▁no▁790｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128791: AddedToken(\"<｜place▁holder▁no▁791｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128792: AddedToken(\"<｜place▁holder▁no▁792｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128793: AddedToken(\"<｜place▁holder▁no▁793｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128794: AddedToken(\"<｜place▁holder▁no▁794｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128795: AddedToken(\"<｜place▁holder▁no▁795｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128796: AddedToken(\"<｜begin▁of▁mask｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128797: AddedToken(\"<｜end▁of▁mask｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128798: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t128799: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t128800: AddedToken(\"<｜fim▁hole｜>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t128801: AddedToken(\"<｜fim▁begin｜>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t128802: AddedToken(\"<｜fim▁end｜>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t128803: AddedToken(\"<｜User｜>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t128804: AddedToken(\"<｜Assistant｜>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t128805: AddedToken(\"<|EOT|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t128806: AddedToken(\"<｜tool▁calls▁begin｜>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t128807: AddedToken(\"<｜tool▁calls▁end｜>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t128808: AddedToken(\"<｜tool▁call▁begin｜>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t128809: AddedToken(\"<｜tool▁call▁end｜>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t128810: AddedToken(\"<｜tool▁outputs▁begin｜>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t128811: AddedToken(\"<｜tool▁outputs▁end｜>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t128812: AddedToken(\"<｜tool▁output▁begin｜>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t128813: AddedToken(\"<｜tool▁output▁end｜>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t128814: AddedToken(\"<｜tool▁sep｜>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = '/autodl-fs/data/stepfun-ai/Step-3___5-Flash'  ## 模型路径\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, \n",
    "                                        use_fast=True,\n",
    "                                        trust_remote_code=True)    ## 分词器初始化\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "255b2c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Step3p5Config {\n",
       "  \"architectures\": [\n",
       "    \"Step3p5ForCausalLM\"\n",
       "  ],\n",
       "  \"att_impl_type\": \"GQA\",\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"attention_other_setting\": {\n",
       "    \"attention_type\": \"sliding_attention\",\n",
       "    \"head_dim\": 128,\n",
       "    \"num_attention_groups\": 8,\n",
       "    \"num_attention_heads\": 96,\n",
       "    \"true_head_dim\": 128\n",
       "  },\n",
       "  \"auto_map\": {\n",
       "    \"AutoConfig\": \"configuration_step3p5.Step3p5Config\",\n",
       "    \"AutoModelForCausalLM\": \"modeling_step3p5.Step3p5ForCausalLM\"\n",
       "  },\n",
       "  \"bos_token_id\": 0,\n",
       "  \"dtype\": \"bfloat16\",\n",
       "  \"eos_token_id\": [\n",
       "    1,\n",
       "    2,\n",
       "    128007\n",
       "  ],\n",
       "  \"head_dim\": 128,\n",
       "  \"hidden_size\": 4096,\n",
       "  \"intermediate_size\": 11264,\n",
       "  \"layer_types\": [\n",
       "    \"full_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"full_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"full_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"full_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"full_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"full_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"full_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"full_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"full_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"full_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"full_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"full_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\"\n",
       "  ],\n",
       "  \"max_position_embeddings\": 262144,\n",
       "  \"max_seq_len\": 262144,\n",
       "  \"model_type\": \"step3p5\",\n",
       "  \"moe_every_n_layer\": 1,\n",
       "  \"moe_intermediate_size\": 1280,\n",
       "  \"moe_layer_offset\": 0,\n",
       "  \"moe_layers_enum\": \"3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44\",\n",
       "  \"moe_num_experts\": 288,\n",
       "  \"moe_router_activation\": \"sigmoid\",\n",
       "  \"moe_router_scaling_factor\": 3.0,\n",
       "  \"moe_top_k\": 8,\n",
       "  \"need_fp32_gate\": true,\n",
       "  \"norm_expert_weight\": true,\n",
       "  \"num_attention_groups\": 8,\n",
       "  \"num_attention_heads\": 64,\n",
       "  \"num_hidden_layers\": 45,\n",
       "  \"num_nextn_predict_layers\": 3,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"partial_rotary_factors\": [\n",
       "    0.5,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    0.5,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    0.5,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    0.5,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    0.5,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    0.5,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    0.5,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    0.5,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    0.5,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    0.5,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    0.5,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    0.5,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0\n",
       "  ],\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": {\n",
       "    \"factor\": 2.0,\n",
       "    \"high_freq_factor\": 32.0,\n",
       "    \"low_freq_factor\": 1.0,\n",
       "    \"original_max_position_embeddings\": 131072,\n",
       "    \"rope_type\": \"llama3\"\n",
       "  },\n",
       "  \"rope_theta\": [\n",
       "    5000000.0,\n",
       "    10000.0,\n",
       "    10000.0,\n",
       "    10000.0,\n",
       "    5000000.0,\n",
       "    10000.0,\n",
       "    10000.0,\n",
       "    10000.0,\n",
       "    5000000.0,\n",
       "    10000.0,\n",
       "    10000.0,\n",
       "    10000.0,\n",
       "    5000000.0,\n",
       "    10000.0,\n",
       "    10000.0,\n",
       "    10000.0,\n",
       "    5000000.0,\n",
       "    10000.0,\n",
       "    10000.0,\n",
       "    10000.0,\n",
       "    5000000.0,\n",
       "    10000.0,\n",
       "    10000.0,\n",
       "    10000.0,\n",
       "    5000000.0,\n",
       "    10000.0,\n",
       "    10000.0,\n",
       "    10000.0,\n",
       "    5000000.0,\n",
       "    10000.0,\n",
       "    10000.0,\n",
       "    10000.0,\n",
       "    5000000.0,\n",
       "    10000.0,\n",
       "    10000.0,\n",
       "    10000.0,\n",
       "    5000000.0,\n",
       "    10000.0,\n",
       "    10000.0,\n",
       "    10000.0,\n",
       "    5000000.0,\n",
       "    10000.0,\n",
       "    10000.0,\n",
       "    10000.0,\n",
       "    5000000.0,\n",
       "    10000.0,\n",
       "    10000.0,\n",
       "    10000.0\n",
       "  ],\n",
       "  \"share_expert_dim\": 1280,\n",
       "  \"sink\": false,\n",
       "  \"sliding_window\": 512,\n",
       "  \"swiglu_limits\": [\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    7,\n",
       "    7,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0\n",
       "  ],\n",
       "  \"swiglu_limits_shared\": [\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    16,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0\n",
       "  ],\n",
       "  \"transformers_version\": \"4.57.3\",\n",
       "  \"use_cache\": false,\n",
       "  \"use_head_wise_attn_gate\": true,\n",
       "  \"use_mfa\": false,\n",
       "  \"use_moe\": true,\n",
       "  \"use_moe_router_bias\": true,\n",
       "  \"use_qk_norm\": true,\n",
       "  \"use_rope_layers\": [],\n",
       "  \"vocab_size\": 128896,\n",
       "  \"yarn_only_types\": [\n",
       "    \"full_attention\"\n",
       "  ],\n",
       "  \"zero_centered\": true\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)  ## 模型配置\n",
    "config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cd4bf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 44/44 [05:24<00:00,  7.38s/it]\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Step3p5ForCausalLM(\n",
       "  (model): Step3p5Model(\n",
       "    (embed_tokens): Embedding(128896, 4096, padding_idx=1)\n",
       "    (layers): ModuleList(\n",
       "      (0): Step3p5DecoderLayer(\n",
       "        (self_attn): Step3p5Attention(\n",
       "          (rotary_emb): Step3p5RotaryEmbedding()\n",
       "          (q_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (q_norm): Step3p5RMSNorm()\n",
       "          (k_norm): Step3p5RMSNorm()\n",
       "          (g_proj): Linear(in_features=4096, out_features=64, bias=False)\n",
       "        )\n",
       "        (mlp): Step3p5MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11264, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11264, bias=False)\n",
       "          (down_proj): Linear(in_features=11264, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Step3p5RMSNorm()\n",
       "        (post_attention_layernorm): Step3p5RMSNorm()\n",
       "      )\n",
       "      (1-2): 2 x Step3p5DecoderLayer(\n",
       "        (self_attn): Step3p5Attention(\n",
       "          (rotary_emb): Step3p5RotaryEmbedding()\n",
       "          (q_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "          (q_norm): Step3p5RMSNorm()\n",
       "          (k_norm): Step3p5RMSNorm()\n",
       "          (g_proj): Linear(in_features=4096, out_features=96, bias=False)\n",
       "        )\n",
       "        (mlp): Step3p5MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11264, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11264, bias=False)\n",
       "          (down_proj): Linear(in_features=11264, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Step3p5RMSNorm()\n",
       "        (post_attention_layernorm): Step3p5RMSNorm()\n",
       "      )\n",
       "      (3): Step3p5DecoderLayer(\n",
       "        (self_attn): Step3p5Attention(\n",
       "          (rotary_emb): Step3p5RotaryEmbedding()\n",
       "          (q_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "          (q_norm): Step3p5RMSNorm()\n",
       "          (k_norm): Step3p5RMSNorm()\n",
       "          (g_proj): Linear(in_features=4096, out_features=96, bias=False)\n",
       "        )\n",
       "        (moe): Step3p5MoEMLP(\n",
       "          (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "          (up_proj): MoELinear()\n",
       "          (gate_proj): MoELinear()\n",
       "          (down_proj): MoELinear()\n",
       "        )\n",
       "        (share_expert): Step3p5MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Step3p5RMSNorm()\n",
       "        (post_attention_layernorm): Step3p5RMSNorm()\n",
       "      )\n",
       "      (4): Step3p5DecoderLayer(\n",
       "        (self_attn): Step3p5Attention(\n",
       "          (rotary_emb): Step3p5RotaryEmbedding()\n",
       "          (q_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (q_norm): Step3p5RMSNorm()\n",
       "          (k_norm): Step3p5RMSNorm()\n",
       "          (g_proj): Linear(in_features=4096, out_features=64, bias=False)\n",
       "        )\n",
       "        (moe): Step3p5MoEMLP(\n",
       "          (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "          (up_proj): MoELinear()\n",
       "          (gate_proj): MoELinear()\n",
       "          (down_proj): MoELinear()\n",
       "        )\n",
       "        (share_expert): Step3p5MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Step3p5RMSNorm()\n",
       "        (post_attention_layernorm): Step3p5RMSNorm()\n",
       "      )\n",
       "      (5-7): 3 x Step3p5DecoderLayer(\n",
       "        (self_attn): Step3p5Attention(\n",
       "          (rotary_emb): Step3p5RotaryEmbedding()\n",
       "          (q_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "          (q_norm): Step3p5RMSNorm()\n",
       "          (k_norm): Step3p5RMSNorm()\n",
       "          (g_proj): Linear(in_features=4096, out_features=96, bias=False)\n",
       "        )\n",
       "        (moe): Step3p5MoEMLP(\n",
       "          (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "          (up_proj): MoELinear()\n",
       "          (gate_proj): MoELinear()\n",
       "          (down_proj): MoELinear()\n",
       "        )\n",
       "        (share_expert): Step3p5MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Step3p5RMSNorm()\n",
       "        (post_attention_layernorm): Step3p5RMSNorm()\n",
       "      )\n",
       "      (8): Step3p5DecoderLayer(\n",
       "        (self_attn): Step3p5Attention(\n",
       "          (rotary_emb): Step3p5RotaryEmbedding()\n",
       "          (q_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (q_norm): Step3p5RMSNorm()\n",
       "          (k_norm): Step3p5RMSNorm()\n",
       "          (g_proj): Linear(in_features=4096, out_features=64, bias=False)\n",
       "        )\n",
       "        (moe): Step3p5MoEMLP(\n",
       "          (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "          (up_proj): MoELinear()\n",
       "          (gate_proj): MoELinear()\n",
       "          (down_proj): MoELinear()\n",
       "        )\n",
       "        (share_expert): Step3p5MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Step3p5RMSNorm()\n",
       "        (post_attention_layernorm): Step3p5RMSNorm()\n",
       "      )\n",
       "      (9-11): 3 x Step3p5DecoderLayer(\n",
       "        (self_attn): Step3p5Attention(\n",
       "          (rotary_emb): Step3p5RotaryEmbedding()\n",
       "          (q_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "          (q_norm): Step3p5RMSNorm()\n",
       "          (k_norm): Step3p5RMSNorm()\n",
       "          (g_proj): Linear(in_features=4096, out_features=96, bias=False)\n",
       "        )\n",
       "        (moe): Step3p5MoEMLP(\n",
       "          (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "          (up_proj): MoELinear()\n",
       "          (gate_proj): MoELinear()\n",
       "          (down_proj): MoELinear()\n",
       "        )\n",
       "        (share_expert): Step3p5MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Step3p5RMSNorm()\n",
       "        (post_attention_layernorm): Step3p5RMSNorm()\n",
       "      )\n",
       "      (12): Step3p5DecoderLayer(\n",
       "        (self_attn): Step3p5Attention(\n",
       "          (rotary_emb): Step3p5RotaryEmbedding()\n",
       "          (q_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (q_norm): Step3p5RMSNorm()\n",
       "          (k_norm): Step3p5RMSNorm()\n",
       "          (g_proj): Linear(in_features=4096, out_features=64, bias=False)\n",
       "        )\n",
       "        (moe): Step3p5MoEMLP(\n",
       "          (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "          (up_proj): MoELinear()\n",
       "          (gate_proj): MoELinear()\n",
       "          (down_proj): MoELinear()\n",
       "        )\n",
       "        (share_expert): Step3p5MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Step3p5RMSNorm()\n",
       "        (post_attention_layernorm): Step3p5RMSNorm()\n",
       "      )\n",
       "      (13-15): 3 x Step3p5DecoderLayer(\n",
       "        (self_attn): Step3p5Attention(\n",
       "          (rotary_emb): Step3p5RotaryEmbedding()\n",
       "          (q_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "          (q_norm): Step3p5RMSNorm()\n",
       "          (k_norm): Step3p5RMSNorm()\n",
       "          (g_proj): Linear(in_features=4096, out_features=96, bias=False)\n",
       "        )\n",
       "        (moe): Step3p5MoEMLP(\n",
       "          (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "          (up_proj): MoELinear()\n",
       "          (gate_proj): MoELinear()\n",
       "          (down_proj): MoELinear()\n",
       "        )\n",
       "        (share_expert): Step3p5MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Step3p5RMSNorm()\n",
       "        (post_attention_layernorm): Step3p5RMSNorm()\n",
       "      )\n",
       "      (16): Step3p5DecoderLayer(\n",
       "        (self_attn): Step3p5Attention(\n",
       "          (rotary_emb): Step3p5RotaryEmbedding()\n",
       "          (q_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (q_norm): Step3p5RMSNorm()\n",
       "          (k_norm): Step3p5RMSNorm()\n",
       "          (g_proj): Linear(in_features=4096, out_features=64, bias=False)\n",
       "        )\n",
       "        (moe): Step3p5MoEMLP(\n",
       "          (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "          (up_proj): MoELinear()\n",
       "          (gate_proj): MoELinear()\n",
       "          (down_proj): MoELinear()\n",
       "        )\n",
       "        (share_expert): Step3p5MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Step3p5RMSNorm()\n",
       "        (post_attention_layernorm): Step3p5RMSNorm()\n",
       "      )\n",
       "      (17-19): 3 x Step3p5DecoderLayer(\n",
       "        (self_attn): Step3p5Attention(\n",
       "          (rotary_emb): Step3p5RotaryEmbedding()\n",
       "          (q_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "          (q_norm): Step3p5RMSNorm()\n",
       "          (k_norm): Step3p5RMSNorm()\n",
       "          (g_proj): Linear(in_features=4096, out_features=96, bias=False)\n",
       "        )\n",
       "        (moe): Step3p5MoEMLP(\n",
       "          (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "          (up_proj): MoELinear()\n",
       "          (gate_proj): MoELinear()\n",
       "          (down_proj): MoELinear()\n",
       "        )\n",
       "        (share_expert): Step3p5MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Step3p5RMSNorm()\n",
       "        (post_attention_layernorm): Step3p5RMSNorm()\n",
       "      )\n",
       "      (20): Step3p5DecoderLayer(\n",
       "        (self_attn): Step3p5Attention(\n",
       "          (rotary_emb): Step3p5RotaryEmbedding()\n",
       "          (q_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (q_norm): Step3p5RMSNorm()\n",
       "          (k_norm): Step3p5RMSNorm()\n",
       "          (g_proj): Linear(in_features=4096, out_features=64, bias=False)\n",
       "        )\n",
       "        (moe): Step3p5MoEMLP(\n",
       "          (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "          (up_proj): MoELinear()\n",
       "          (gate_proj): MoELinear()\n",
       "          (down_proj): MoELinear()\n",
       "        )\n",
       "        (share_expert): Step3p5MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Step3p5RMSNorm()\n",
       "        (post_attention_layernorm): Step3p5RMSNorm()\n",
       "      )\n",
       "      (21-23): 3 x Step3p5DecoderLayer(\n",
       "        (self_attn): Step3p5Attention(\n",
       "          (rotary_emb): Step3p5RotaryEmbedding()\n",
       "          (q_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "          (q_norm): Step3p5RMSNorm()\n",
       "          (k_norm): Step3p5RMSNorm()\n",
       "          (g_proj): Linear(in_features=4096, out_features=96, bias=False)\n",
       "        )\n",
       "        (moe): Step3p5MoEMLP(\n",
       "          (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "          (up_proj): MoELinear()\n",
       "          (gate_proj): MoELinear()\n",
       "          (down_proj): MoELinear()\n",
       "        )\n",
       "        (share_expert): Step3p5MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Step3p5RMSNorm()\n",
       "        (post_attention_layernorm): Step3p5RMSNorm()\n",
       "      )\n",
       "      (24): Step3p5DecoderLayer(\n",
       "        (self_attn): Step3p5Attention(\n",
       "          (rotary_emb): Step3p5RotaryEmbedding()\n",
       "          (q_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (q_norm): Step3p5RMSNorm()\n",
       "          (k_norm): Step3p5RMSNorm()\n",
       "          (g_proj): Linear(in_features=4096, out_features=64, bias=False)\n",
       "        )\n",
       "        (moe): Step3p5MoEMLP(\n",
       "          (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "          (up_proj): MoELinear()\n",
       "          (gate_proj): MoELinear()\n",
       "          (down_proj): MoELinear()\n",
       "        )\n",
       "        (share_expert): Step3p5MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Step3p5RMSNorm()\n",
       "        (post_attention_layernorm): Step3p5RMSNorm()\n",
       "      )\n",
       "      (25-27): 3 x Step3p5DecoderLayer(\n",
       "        (self_attn): Step3p5Attention(\n",
       "          (rotary_emb): Step3p5RotaryEmbedding()\n",
       "          (q_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "          (q_norm): Step3p5RMSNorm()\n",
       "          (k_norm): Step3p5RMSNorm()\n",
       "          (g_proj): Linear(in_features=4096, out_features=96, bias=False)\n",
       "        )\n",
       "        (moe): Step3p5MoEMLP(\n",
       "          (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "          (up_proj): MoELinear()\n",
       "          (gate_proj): MoELinear()\n",
       "          (down_proj): MoELinear()\n",
       "        )\n",
       "        (share_expert): Step3p5MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Step3p5RMSNorm()\n",
       "        (post_attention_layernorm): Step3p5RMSNorm()\n",
       "      )\n",
       "      (28): Step3p5DecoderLayer(\n",
       "        (self_attn): Step3p5Attention(\n",
       "          (rotary_emb): Step3p5RotaryEmbedding()\n",
       "          (q_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (q_norm): Step3p5RMSNorm()\n",
       "          (k_norm): Step3p5RMSNorm()\n",
       "          (g_proj): Linear(in_features=4096, out_features=64, bias=False)\n",
       "        )\n",
       "        (moe): Step3p5MoEMLP(\n",
       "          (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "          (up_proj): MoELinear()\n",
       "          (gate_proj): MoELinear()\n",
       "          (down_proj): MoELinear()\n",
       "        )\n",
       "        (share_expert): Step3p5MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Step3p5RMSNorm()\n",
       "        (post_attention_layernorm): Step3p5RMSNorm()\n",
       "      )\n",
       "      (29-31): 3 x Step3p5DecoderLayer(\n",
       "        (self_attn): Step3p5Attention(\n",
       "          (rotary_emb): Step3p5RotaryEmbedding()\n",
       "          (q_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "          (q_norm): Step3p5RMSNorm()\n",
       "          (k_norm): Step3p5RMSNorm()\n",
       "          (g_proj): Linear(in_features=4096, out_features=96, bias=False)\n",
       "        )\n",
       "        (moe): Step3p5MoEMLP(\n",
       "          (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "          (up_proj): MoELinear()\n",
       "          (gate_proj): MoELinear()\n",
       "          (down_proj): MoELinear()\n",
       "        )\n",
       "        (share_expert): Step3p5MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Step3p5RMSNorm()\n",
       "        (post_attention_layernorm): Step3p5RMSNorm()\n",
       "      )\n",
       "      (32): Step3p5DecoderLayer(\n",
       "        (self_attn): Step3p5Attention(\n",
       "          (rotary_emb): Step3p5RotaryEmbedding()\n",
       "          (q_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (q_norm): Step3p5RMSNorm()\n",
       "          (k_norm): Step3p5RMSNorm()\n",
       "          (g_proj): Linear(in_features=4096, out_features=64, bias=False)\n",
       "        )\n",
       "        (moe): Step3p5MoEMLP(\n",
       "          (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "          (up_proj): MoELinear()\n",
       "          (gate_proj): MoELinear()\n",
       "          (down_proj): MoELinear()\n",
       "        )\n",
       "        (share_expert): Step3p5MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Step3p5RMSNorm()\n",
       "        (post_attention_layernorm): Step3p5RMSNorm()\n",
       "      )\n",
       "      (33-35): 3 x Step3p5DecoderLayer(\n",
       "        (self_attn): Step3p5Attention(\n",
       "          (rotary_emb): Step3p5RotaryEmbedding()\n",
       "          (q_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "          (q_norm): Step3p5RMSNorm()\n",
       "          (k_norm): Step3p5RMSNorm()\n",
       "          (g_proj): Linear(in_features=4096, out_features=96, bias=False)\n",
       "        )\n",
       "        (moe): Step3p5MoEMLP(\n",
       "          (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "          (up_proj): MoELinear()\n",
       "          (gate_proj): MoELinear()\n",
       "          (down_proj): MoELinear()\n",
       "        )\n",
       "        (share_expert): Step3p5MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Step3p5RMSNorm()\n",
       "        (post_attention_layernorm): Step3p5RMSNorm()\n",
       "      )\n",
       "      (36): Step3p5DecoderLayer(\n",
       "        (self_attn): Step3p5Attention(\n",
       "          (rotary_emb): Step3p5RotaryEmbedding()\n",
       "          (q_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (q_norm): Step3p5RMSNorm()\n",
       "          (k_norm): Step3p5RMSNorm()\n",
       "          (g_proj): Linear(in_features=4096, out_features=64, bias=False)\n",
       "        )\n",
       "        (moe): Step3p5MoEMLP(\n",
       "          (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "          (up_proj): MoELinear()\n",
       "          (gate_proj): MoELinear()\n",
       "          (down_proj): MoELinear()\n",
       "        )\n",
       "        (share_expert): Step3p5MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Step3p5RMSNorm()\n",
       "        (post_attention_layernorm): Step3p5RMSNorm()\n",
       "      )\n",
       "      (37-39): 3 x Step3p5DecoderLayer(\n",
       "        (self_attn): Step3p5Attention(\n",
       "          (rotary_emb): Step3p5RotaryEmbedding()\n",
       "          (q_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "          (q_norm): Step3p5RMSNorm()\n",
       "          (k_norm): Step3p5RMSNorm()\n",
       "          (g_proj): Linear(in_features=4096, out_features=96, bias=False)\n",
       "        )\n",
       "        (moe): Step3p5MoEMLP(\n",
       "          (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "          (up_proj): MoELinear()\n",
       "          (gate_proj): MoELinear()\n",
       "          (down_proj): MoELinear()\n",
       "        )\n",
       "        (share_expert): Step3p5MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Step3p5RMSNorm()\n",
       "        (post_attention_layernorm): Step3p5RMSNorm()\n",
       "      )\n",
       "      (40): Step3p5DecoderLayer(\n",
       "        (self_attn): Step3p5Attention(\n",
       "          (rotary_emb): Step3p5RotaryEmbedding()\n",
       "          (q_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (q_norm): Step3p5RMSNorm()\n",
       "          (k_norm): Step3p5RMSNorm()\n",
       "          (g_proj): Linear(in_features=4096, out_features=64, bias=False)\n",
       "        )\n",
       "        (moe): Step3p5MoEMLP(\n",
       "          (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "          (up_proj): MoELinear()\n",
       "          (gate_proj): MoELinear()\n",
       "          (down_proj): MoELinear()\n",
       "        )\n",
       "        (share_expert): Step3p5MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Step3p5RMSNorm()\n",
       "        (post_attention_layernorm): Step3p5RMSNorm()\n",
       "      )\n",
       "      (41-43): 3 x Step3p5DecoderLayer(\n",
       "        (self_attn): Step3p5Attention(\n",
       "          (rotary_emb): Step3p5RotaryEmbedding()\n",
       "          (q_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "          (q_norm): Step3p5RMSNorm()\n",
       "          (k_norm): Step3p5RMSNorm()\n",
       "          (g_proj): Linear(in_features=4096, out_features=96, bias=False)\n",
       "        )\n",
       "        (moe): Step3p5MoEMLP(\n",
       "          (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "          (up_proj): MoELinear()\n",
       "          (gate_proj): MoELinear()\n",
       "          (down_proj): MoELinear()\n",
       "        )\n",
       "        (share_expert): Step3p5MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Step3p5RMSNorm()\n",
       "        (post_attention_layernorm): Step3p5RMSNorm()\n",
       "      )\n",
       "      (44): Step3p5DecoderLayer(\n",
       "        (self_attn): Step3p5Attention(\n",
       "          (rotary_emb): Step3p5RotaryEmbedding()\n",
       "          (q_proj): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "          (q_norm): Step3p5RMSNorm()\n",
       "          (k_norm): Step3p5RMSNorm()\n",
       "          (g_proj): Linear(in_features=4096, out_features=64, bias=False)\n",
       "        )\n",
       "        (moe): Step3p5MoEMLP(\n",
       "          (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "          (up_proj): MoELinear()\n",
       "          (gate_proj): MoELinear()\n",
       "          (down_proj): MoELinear()\n",
       "        )\n",
       "        (share_expert): Step3p5MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "          (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Step3p5RMSNorm()\n",
       "        (post_attention_layernorm): Step3p5RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Step3p5RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128896, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    dtype=torch.bfloat16,\n",
    "    config=config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,  \n",
    "\n",
    ")  ##base模型加载\n",
    "base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275c9d12",
   "metadata": {},
   "source": [
    "## Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a2a1547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, peft_version='0.18.1', base_model_name_or_path=None, revision=None, inference_mode=False, r=16, target_modules={'q_proj', 'k_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, alora_invocation_tokens=None, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None, arrow_config=None, ensure_weight_tying=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_cfg = LoraConfig(     ## Lora配置\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16, ##rank\n",
    "    lora_alpha=32,## The alpha parameter for Lora scaling\n",
    "    lora_dropout=0.05,##  The dropout probability for Lora layers\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\" ## 这两个是必要的\n",
    "    ],\n",
    "    bias=\"none\",\n",
    ")\n",
    "lora_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de8f1a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Step3p5ForCausalLM(\n",
       "      (model): Step3p5Model(\n",
       "        (embed_tokens): Embedding(128896, 4096, padding_idx=1)\n",
       "        (layers): ModuleList(\n",
       "          (0): Step3p5DecoderLayer(\n",
       "            (self_attn): Step3p5Attention(\n",
       "              (rotary_emb): Step3p5RotaryEmbedding()\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "              (q_norm): Step3p5RMSNorm()\n",
       "              (k_norm): Step3p5RMSNorm()\n",
       "              (g_proj): Linear(in_features=4096, out_features=64, bias=False)\n",
       "            )\n",
       "            (mlp): Step3p5MLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11264, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11264, bias=False)\n",
       "              (down_proj): Linear(in_features=11264, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Step3p5RMSNorm()\n",
       "            (post_attention_layernorm): Step3p5RMSNorm()\n",
       "          )\n",
       "          (1-2): 2 x Step3p5DecoderLayer(\n",
       "            (self_attn): Step3p5Attention(\n",
       "              (rotary_emb): Step3p5RotaryEmbedding()\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "              (q_norm): Step3p5RMSNorm()\n",
       "              (k_norm): Step3p5RMSNorm()\n",
       "              (g_proj): Linear(in_features=4096, out_features=96, bias=False)\n",
       "            )\n",
       "            (mlp): Step3p5MLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11264, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11264, bias=False)\n",
       "              (down_proj): Linear(in_features=11264, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Step3p5RMSNorm()\n",
       "            (post_attention_layernorm): Step3p5RMSNorm()\n",
       "          )\n",
       "          (3): Step3p5DecoderLayer(\n",
       "            (self_attn): Step3p5Attention(\n",
       "              (rotary_emb): Step3p5RotaryEmbedding()\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "              (q_norm): Step3p5RMSNorm()\n",
       "              (k_norm): Step3p5RMSNorm()\n",
       "              (g_proj): Linear(in_features=4096, out_features=96, bias=False)\n",
       "            )\n",
       "            (moe): Step3p5MoEMLP(\n",
       "              (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "              (up_proj): MoELinear()\n",
       "              (gate_proj): MoELinear()\n",
       "              (down_proj): MoELinear()\n",
       "            )\n",
       "            (share_expert): Step3p5MLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Step3p5RMSNorm()\n",
       "            (post_attention_layernorm): Step3p5RMSNorm()\n",
       "          )\n",
       "          (4): Step3p5DecoderLayer(\n",
       "            (self_attn): Step3p5Attention(\n",
       "              (rotary_emb): Step3p5RotaryEmbedding()\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "              (q_norm): Step3p5RMSNorm()\n",
       "              (k_norm): Step3p5RMSNorm()\n",
       "              (g_proj): Linear(in_features=4096, out_features=64, bias=False)\n",
       "            )\n",
       "            (moe): Step3p5MoEMLP(\n",
       "              (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "              (up_proj): MoELinear()\n",
       "              (gate_proj): MoELinear()\n",
       "              (down_proj): MoELinear()\n",
       "            )\n",
       "            (share_expert): Step3p5MLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Step3p5RMSNorm()\n",
       "            (post_attention_layernorm): Step3p5RMSNorm()\n",
       "          )\n",
       "          (5-7): 3 x Step3p5DecoderLayer(\n",
       "            (self_attn): Step3p5Attention(\n",
       "              (rotary_emb): Step3p5RotaryEmbedding()\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "              (q_norm): Step3p5RMSNorm()\n",
       "              (k_norm): Step3p5RMSNorm()\n",
       "              (g_proj): Linear(in_features=4096, out_features=96, bias=False)\n",
       "            )\n",
       "            (moe): Step3p5MoEMLP(\n",
       "              (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "              (up_proj): MoELinear()\n",
       "              (gate_proj): MoELinear()\n",
       "              (down_proj): MoELinear()\n",
       "            )\n",
       "            (share_expert): Step3p5MLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Step3p5RMSNorm()\n",
       "            (post_attention_layernorm): Step3p5RMSNorm()\n",
       "          )\n",
       "          (8): Step3p5DecoderLayer(\n",
       "            (self_attn): Step3p5Attention(\n",
       "              (rotary_emb): Step3p5RotaryEmbedding()\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "              (q_norm): Step3p5RMSNorm()\n",
       "              (k_norm): Step3p5RMSNorm()\n",
       "              (g_proj): Linear(in_features=4096, out_features=64, bias=False)\n",
       "            )\n",
       "            (moe): Step3p5MoEMLP(\n",
       "              (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "              (up_proj): MoELinear()\n",
       "              (gate_proj): MoELinear()\n",
       "              (down_proj): MoELinear()\n",
       "            )\n",
       "            (share_expert): Step3p5MLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Step3p5RMSNorm()\n",
       "            (post_attention_layernorm): Step3p5RMSNorm()\n",
       "          )\n",
       "          (9-11): 3 x Step3p5DecoderLayer(\n",
       "            (self_attn): Step3p5Attention(\n",
       "              (rotary_emb): Step3p5RotaryEmbedding()\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "              (q_norm): Step3p5RMSNorm()\n",
       "              (k_norm): Step3p5RMSNorm()\n",
       "              (g_proj): Linear(in_features=4096, out_features=96, bias=False)\n",
       "            )\n",
       "            (moe): Step3p5MoEMLP(\n",
       "              (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "              (up_proj): MoELinear()\n",
       "              (gate_proj): MoELinear()\n",
       "              (down_proj): MoELinear()\n",
       "            )\n",
       "            (share_expert): Step3p5MLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Step3p5RMSNorm()\n",
       "            (post_attention_layernorm): Step3p5RMSNorm()\n",
       "          )\n",
       "          (12): Step3p5DecoderLayer(\n",
       "            (self_attn): Step3p5Attention(\n",
       "              (rotary_emb): Step3p5RotaryEmbedding()\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "              (q_norm): Step3p5RMSNorm()\n",
       "              (k_norm): Step3p5RMSNorm()\n",
       "              (g_proj): Linear(in_features=4096, out_features=64, bias=False)\n",
       "            )\n",
       "            (moe): Step3p5MoEMLP(\n",
       "              (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "              (up_proj): MoELinear()\n",
       "              (gate_proj): MoELinear()\n",
       "              (down_proj): MoELinear()\n",
       "            )\n",
       "            (share_expert): Step3p5MLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Step3p5RMSNorm()\n",
       "            (post_attention_layernorm): Step3p5RMSNorm()\n",
       "          )\n",
       "          (13-15): 3 x Step3p5DecoderLayer(\n",
       "            (self_attn): Step3p5Attention(\n",
       "              (rotary_emb): Step3p5RotaryEmbedding()\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "              (q_norm): Step3p5RMSNorm()\n",
       "              (k_norm): Step3p5RMSNorm()\n",
       "              (g_proj): Linear(in_features=4096, out_features=96, bias=False)\n",
       "            )\n",
       "            (moe): Step3p5MoEMLP(\n",
       "              (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "              (up_proj): MoELinear()\n",
       "              (gate_proj): MoELinear()\n",
       "              (down_proj): MoELinear()\n",
       "            )\n",
       "            (share_expert): Step3p5MLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Step3p5RMSNorm()\n",
       "            (post_attention_layernorm): Step3p5RMSNorm()\n",
       "          )\n",
       "          (16): Step3p5DecoderLayer(\n",
       "            (self_attn): Step3p5Attention(\n",
       "              (rotary_emb): Step3p5RotaryEmbedding()\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "              (q_norm): Step3p5RMSNorm()\n",
       "              (k_norm): Step3p5RMSNorm()\n",
       "              (g_proj): Linear(in_features=4096, out_features=64, bias=False)\n",
       "            )\n",
       "            (moe): Step3p5MoEMLP(\n",
       "              (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "              (up_proj): MoELinear()\n",
       "              (gate_proj): MoELinear()\n",
       "              (down_proj): MoELinear()\n",
       "            )\n",
       "            (share_expert): Step3p5MLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Step3p5RMSNorm()\n",
       "            (post_attention_layernorm): Step3p5RMSNorm()\n",
       "          )\n",
       "          (17-19): 3 x Step3p5DecoderLayer(\n",
       "            (self_attn): Step3p5Attention(\n",
       "              (rotary_emb): Step3p5RotaryEmbedding()\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "              (q_norm): Step3p5RMSNorm()\n",
       "              (k_norm): Step3p5RMSNorm()\n",
       "              (g_proj): Linear(in_features=4096, out_features=96, bias=False)\n",
       "            )\n",
       "            (moe): Step3p5MoEMLP(\n",
       "              (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "              (up_proj): MoELinear()\n",
       "              (gate_proj): MoELinear()\n",
       "              (down_proj): MoELinear()\n",
       "            )\n",
       "            (share_expert): Step3p5MLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Step3p5RMSNorm()\n",
       "            (post_attention_layernorm): Step3p5RMSNorm()\n",
       "          )\n",
       "          (20): Step3p5DecoderLayer(\n",
       "            (self_attn): Step3p5Attention(\n",
       "              (rotary_emb): Step3p5RotaryEmbedding()\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "              (q_norm): Step3p5RMSNorm()\n",
       "              (k_norm): Step3p5RMSNorm()\n",
       "              (g_proj): Linear(in_features=4096, out_features=64, bias=False)\n",
       "            )\n",
       "            (moe): Step3p5MoEMLP(\n",
       "              (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "              (up_proj): MoELinear()\n",
       "              (gate_proj): MoELinear()\n",
       "              (down_proj): MoELinear()\n",
       "            )\n",
       "            (share_expert): Step3p5MLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Step3p5RMSNorm()\n",
       "            (post_attention_layernorm): Step3p5RMSNorm()\n",
       "          )\n",
       "          (21-23): 3 x Step3p5DecoderLayer(\n",
       "            (self_attn): Step3p5Attention(\n",
       "              (rotary_emb): Step3p5RotaryEmbedding()\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "              (q_norm): Step3p5RMSNorm()\n",
       "              (k_norm): Step3p5RMSNorm()\n",
       "              (g_proj): Linear(in_features=4096, out_features=96, bias=False)\n",
       "            )\n",
       "            (moe): Step3p5MoEMLP(\n",
       "              (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "              (up_proj): MoELinear()\n",
       "              (gate_proj): MoELinear()\n",
       "              (down_proj): MoELinear()\n",
       "            )\n",
       "            (share_expert): Step3p5MLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Step3p5RMSNorm()\n",
       "            (post_attention_layernorm): Step3p5RMSNorm()\n",
       "          )\n",
       "          (24): Step3p5DecoderLayer(\n",
       "            (self_attn): Step3p5Attention(\n",
       "              (rotary_emb): Step3p5RotaryEmbedding()\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "              (q_norm): Step3p5RMSNorm()\n",
       "              (k_norm): Step3p5RMSNorm()\n",
       "              (g_proj): Linear(in_features=4096, out_features=64, bias=False)\n",
       "            )\n",
       "            (moe): Step3p5MoEMLP(\n",
       "              (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "              (up_proj): MoELinear()\n",
       "              (gate_proj): MoELinear()\n",
       "              (down_proj): MoELinear()\n",
       "            )\n",
       "            (share_expert): Step3p5MLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Step3p5RMSNorm()\n",
       "            (post_attention_layernorm): Step3p5RMSNorm()\n",
       "          )\n",
       "          (25-27): 3 x Step3p5DecoderLayer(\n",
       "            (self_attn): Step3p5Attention(\n",
       "              (rotary_emb): Step3p5RotaryEmbedding()\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "              (q_norm): Step3p5RMSNorm()\n",
       "              (k_norm): Step3p5RMSNorm()\n",
       "              (g_proj): Linear(in_features=4096, out_features=96, bias=False)\n",
       "            )\n",
       "            (moe): Step3p5MoEMLP(\n",
       "              (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "              (up_proj): MoELinear()\n",
       "              (gate_proj): MoELinear()\n",
       "              (down_proj): MoELinear()\n",
       "            )\n",
       "            (share_expert): Step3p5MLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Step3p5RMSNorm()\n",
       "            (post_attention_layernorm): Step3p5RMSNorm()\n",
       "          )\n",
       "          (28): Step3p5DecoderLayer(\n",
       "            (self_attn): Step3p5Attention(\n",
       "              (rotary_emb): Step3p5RotaryEmbedding()\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "              (q_norm): Step3p5RMSNorm()\n",
       "              (k_norm): Step3p5RMSNorm()\n",
       "              (g_proj): Linear(in_features=4096, out_features=64, bias=False)\n",
       "            )\n",
       "            (moe): Step3p5MoEMLP(\n",
       "              (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "              (up_proj): MoELinear()\n",
       "              (gate_proj): MoELinear()\n",
       "              (down_proj): MoELinear()\n",
       "            )\n",
       "            (share_expert): Step3p5MLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Step3p5RMSNorm()\n",
       "            (post_attention_layernorm): Step3p5RMSNorm()\n",
       "          )\n",
       "          (29-31): 3 x Step3p5DecoderLayer(\n",
       "            (self_attn): Step3p5Attention(\n",
       "              (rotary_emb): Step3p5RotaryEmbedding()\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "              (q_norm): Step3p5RMSNorm()\n",
       "              (k_norm): Step3p5RMSNorm()\n",
       "              (g_proj): Linear(in_features=4096, out_features=96, bias=False)\n",
       "            )\n",
       "            (moe): Step3p5MoEMLP(\n",
       "              (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "              (up_proj): MoELinear()\n",
       "              (gate_proj): MoELinear()\n",
       "              (down_proj): MoELinear()\n",
       "            )\n",
       "            (share_expert): Step3p5MLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Step3p5RMSNorm()\n",
       "            (post_attention_layernorm): Step3p5RMSNorm()\n",
       "          )\n",
       "          (32): Step3p5DecoderLayer(\n",
       "            (self_attn): Step3p5Attention(\n",
       "              (rotary_emb): Step3p5RotaryEmbedding()\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "              (q_norm): Step3p5RMSNorm()\n",
       "              (k_norm): Step3p5RMSNorm()\n",
       "              (g_proj): Linear(in_features=4096, out_features=64, bias=False)\n",
       "            )\n",
       "            (moe): Step3p5MoEMLP(\n",
       "              (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "              (up_proj): MoELinear()\n",
       "              (gate_proj): MoELinear()\n",
       "              (down_proj): MoELinear()\n",
       "            )\n",
       "            (share_expert): Step3p5MLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Step3p5RMSNorm()\n",
       "            (post_attention_layernorm): Step3p5RMSNorm()\n",
       "          )\n",
       "          (33-35): 3 x Step3p5DecoderLayer(\n",
       "            (self_attn): Step3p5Attention(\n",
       "              (rotary_emb): Step3p5RotaryEmbedding()\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "              (q_norm): Step3p5RMSNorm()\n",
       "              (k_norm): Step3p5RMSNorm()\n",
       "              (g_proj): Linear(in_features=4096, out_features=96, bias=False)\n",
       "            )\n",
       "            (moe): Step3p5MoEMLP(\n",
       "              (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "              (up_proj): MoELinear()\n",
       "              (gate_proj): MoELinear()\n",
       "              (down_proj): MoELinear()\n",
       "            )\n",
       "            (share_expert): Step3p5MLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Step3p5RMSNorm()\n",
       "            (post_attention_layernorm): Step3p5RMSNorm()\n",
       "          )\n",
       "          (36): Step3p5DecoderLayer(\n",
       "            (self_attn): Step3p5Attention(\n",
       "              (rotary_emb): Step3p5RotaryEmbedding()\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "              (q_norm): Step3p5RMSNorm()\n",
       "              (k_norm): Step3p5RMSNorm()\n",
       "              (g_proj): Linear(in_features=4096, out_features=64, bias=False)\n",
       "            )\n",
       "            (moe): Step3p5MoEMLP(\n",
       "              (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "              (up_proj): MoELinear()\n",
       "              (gate_proj): MoELinear()\n",
       "              (down_proj): MoELinear()\n",
       "            )\n",
       "            (share_expert): Step3p5MLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Step3p5RMSNorm()\n",
       "            (post_attention_layernorm): Step3p5RMSNorm()\n",
       "          )\n",
       "          (37-39): 3 x Step3p5DecoderLayer(\n",
       "            (self_attn): Step3p5Attention(\n",
       "              (rotary_emb): Step3p5RotaryEmbedding()\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "              (q_norm): Step3p5RMSNorm()\n",
       "              (k_norm): Step3p5RMSNorm()\n",
       "              (g_proj): Linear(in_features=4096, out_features=96, bias=False)\n",
       "            )\n",
       "            (moe): Step3p5MoEMLP(\n",
       "              (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "              (up_proj): MoELinear()\n",
       "              (gate_proj): MoELinear()\n",
       "              (down_proj): MoELinear()\n",
       "            )\n",
       "            (share_expert): Step3p5MLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Step3p5RMSNorm()\n",
       "            (post_attention_layernorm): Step3p5RMSNorm()\n",
       "          )\n",
       "          (40): Step3p5DecoderLayer(\n",
       "            (self_attn): Step3p5Attention(\n",
       "              (rotary_emb): Step3p5RotaryEmbedding()\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "              (q_norm): Step3p5RMSNorm()\n",
       "              (k_norm): Step3p5RMSNorm()\n",
       "              (g_proj): Linear(in_features=4096, out_features=64, bias=False)\n",
       "            )\n",
       "            (moe): Step3p5MoEMLP(\n",
       "              (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "              (up_proj): MoELinear()\n",
       "              (gate_proj): MoELinear()\n",
       "              (down_proj): MoELinear()\n",
       "            )\n",
       "            (share_expert): Step3p5MLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Step3p5RMSNorm()\n",
       "            (post_attention_layernorm): Step3p5RMSNorm()\n",
       "          )\n",
       "          (41-43): 3 x Step3p5DecoderLayer(\n",
       "            (self_attn): Step3p5Attention(\n",
       "              (rotary_emb): Step3p5RotaryEmbedding()\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "              (q_norm): Step3p5RMSNorm()\n",
       "              (k_norm): Step3p5RMSNorm()\n",
       "              (g_proj): Linear(in_features=4096, out_features=96, bias=False)\n",
       "            )\n",
       "            (moe): Step3p5MoEMLP(\n",
       "              (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "              (up_proj): MoELinear()\n",
       "              (gate_proj): MoELinear()\n",
       "              (down_proj): MoELinear()\n",
       "            )\n",
       "            (share_expert): Step3p5MLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Step3p5RMSNorm()\n",
       "            (post_attention_layernorm): Step3p5RMSNorm()\n",
       "          )\n",
       "          (44): Step3p5DecoderLayer(\n",
       "            (self_attn): Step3p5Attention(\n",
       "              (rotary_emb): Step3p5RotaryEmbedding()\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "              (q_norm): Step3p5RMSNorm()\n",
       "              (k_norm): Step3p5RMSNorm()\n",
       "              (g_proj): Linear(in_features=4096, out_features=64, bias=False)\n",
       "            )\n",
       "            (moe): Step3p5MoEMLP(\n",
       "              (gate): Linear(in_features=4096, out_features=288, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "              (up_proj): MoELinear()\n",
       "              (gate_proj): MoELinear()\n",
       "              (down_proj): MoELinear()\n",
       "            )\n",
       "            (share_expert): Step3p5MLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=1280, bias=False)\n",
       "              (down_proj): Linear(in_features=1280, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Step3p5RMSNorm()\n",
       "            (post_attention_layernorm): Step3p5RMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): Step3p5RMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128896, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "peft_model = get_peft_model(base, lora_cfg)  ## 将lora配置注入base模型\n",
    "peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1e012b",
   "metadata": {},
   "source": [
    "## 训练参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f468d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./lora_checkpoints\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=10,              # 训练 10 轮\n",
    "    save_strategy=\"epoch\",           # 每轮结束后保存一个 checkpoint\n",
    "    per_device_train_batch_size=32,   # 显存够就调大，炸了就调小\n",
    "    learning_rate=2e-3,              # 学习率\n",
    "    save_total_limit=1,              # 仅保留最近的一个 checkpoint，极度节省空间\n",
    "    bf16=True,                       # bf16精度\n",
    "    logging_steps=100,               # 打印日志的频率\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b807bff8",
   "metadata": {},
   "source": [
    "## 可视化训练过程相关配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd2f062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "swanlab_callback = SwanLabCallback(\n",
    "    project=\"step3.5flash-Lora\",  # 注意修改\n",
    "    experiment_name=\"step3.5flash-LoRA-experiment\"  # 注意修改\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9a59ed",
   "metadata": {},
   "source": [
    "## 数据加载和peft_model的配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8e68f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "class DollyDataCollator:\n",
    "    def __init__(self, tokenizer, model):\n",
    "        # 使用官方 collator 处理 padding\n",
    "        self.base_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer=tokenizer, \n",
    "            model=model, \n",
    "            padding=True, \n",
    "            label_pad_token_id=-100, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # 弹出 ref_text，防止 Trainer 将非张量数据传给模型导致报错\n",
    "        ref_texts = [f.pop(\"ref_text\") for f in features if \"ref_text\" in f]\n",
    "        batch = self.base_collator(features)\n",
    "        # 如果你需要在 eval 阶段用 ref_text，可以保留，但 train 阶段模型不收这个参数\n",
    "        # batch[\"ref_text\"] = ref_texts \n",
    "        return batch\n",
    "\n",
    "class DollyProcessor:\n",
    "    def __init__(self, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def process_fn(self, example):\n",
    "        # 1. 构建 Prompt\n",
    "        prompt_text = f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example.get('context', '')}\\n\\n### Response:\\n\"\n",
    "        full_text = prompt_text + f\"{example['response']}{self.tokenizer.eos_token}\"\n",
    "        \n",
    "        # 2. 对全文进行编码 \n",
    "        # 注意：使用 add_special_tokens=True 以确保包含 BOS token（如果模型需要）\n",
    "        encodings = self.tokenizer(\n",
    "            full_text, \n",
    "            truncation=True, \n",
    "            max_length=self.max_len, \n",
    "            padding=False, \n",
    "            add_special_tokens=True \n",
    "        )\n",
    "        \n",
    "        input_ids = list(encodings[\"input_ids\"])\n",
    "        attention_mask = list(encodings[\"attention_mask\"])\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        \n",
    "        # 3. 计算 Prompt 长度（关键：保持与全文编码一致的 special tokens 设置）\n",
    "        # 这样计算出的长度才能准确匹配到 Response 的起始位置\n",
    "        prompt_encodings = self.tokenizer(\n",
    "            prompt_text, \n",
    "            add_special_tokens=True, \n",
    "            truncation=True, \n",
    "            max_length=self.max_len\n",
    "        )\n",
    "        prefix_len = len(prompt_encodings[\"input_ids\"])\n",
    "        \n",
    "        # 4. 遮掩 Label 中的 Prompt 部分\n",
    "        for i in range(min(prefix_len, len(labels))):\n",
    "            labels[i] = -100 \n",
    "            \n",
    "        return {\n",
    "            \"input_ids\": input_ids, \n",
    "            \"attention_mask\": attention_mask, \n",
    "            \"labels\": labels, \n",
    "            \"ref_text\": example[\"response\"]\n",
    "        }\n",
    "\n",
    "# 修改后的函数，需要传入 tokenizer 和 max_len\n",
    "def get_processed_dataset(tokenizer, max_len):\n",
    "    # 1. 加载原始数据\n",
    "    raw_dataset = load_dataset(\"json\", data_files='dolly_huanhuan.jsonl')[\"train\"]\n",
    "    \n",
    "    # 2. 实例化处理器\n",
    "    processor = DollyProcessor(tokenizer, max_len)\n",
    "    \n",
    "    # 3. **核心步骤**：执行分词转换\n",
    "    # num_proc 可以根据你的 CPU 核心数调整，加速处理\n",
    "    tokenized_dataset = raw_dataset.map(\n",
    "        processor.process_fn,\n",
    "        remove_columns=raw_dataset.column_names,\n",
    "        desc=\"Running tokenizer on dataset\"\n",
    "    )\n",
    "    \n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab418bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 14,696,448 || all params: 196,970,826,816 || trainable%: 0.0075\n"
     ]
    }
   ],
   "source": [
    "train_dataset = get_processed_dataset(tokenizer, 1024)\n",
    "\n",
    "collator = DollyDataCollator(tokenizer, peft_model)\n",
    "\n",
    "peft_model.config.use_cache = False  # 必须关闭，否则与梯度检查点冲突\n",
    "peft_model.enable_input_require_grads()\n",
    "peft_model.gradient_checkpointing_enable()\n",
    "peft_model.print_trainable_parameters() ## 查看参数量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e94b7f",
   "metadata": {},
   "source": [
    "# 正式开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2670150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/root/miniconda3/envs/loraa/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/root/miniconda3/envs/loraa/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> Tracking run with swanlab version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m Tracking run with swanlab version \u001b[1;36m0.7\u001b[0m.\u001b[1;36m8\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> Run data will be saved locally in \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">/root/step-3.5-Flash-lora/swanlog/run-20260212_061715-6vwsgcjgmnqqy7p9422gj</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m Run data will be saved locally in \n",
       "\u001b[1;35m/root/step-3.5-Flash-lora/swanlog/run-20260212_061715-6vwsgcjgmnqqy7p9422gj\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> </span>👋 Hi <span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">FrankTan</span>,welcome to swanlab!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m\u001b[1;34m \u001b[0m👋 Hi \u001b[1;39mFrankTan\u001b[0m,welcome to swanlab!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> Syncing run <span style=\"color: #808000; text-decoration-color: #808000\">dragon-28</span> to the cloud\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m Syncing run \u001b[33mdragon-28\u001b[0m to the cloud\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> 🏠 View project at <span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">https://swanlab.cn/@FrankTan/step-3.5-Flash-lora</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m 🏠 View project at \u001b[4;34mhttps://swanlab.cn/@FrankTan/step-3.5-Flash-lora\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> 🚀 View run at <span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">https://swanlab.cn/@FrankTan/step-3.5-Flash-lora/runs/6vwsgcjgmnqqy7p9422gj</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m 🚀 View run at \u001b[4;34mhttps://swanlab.cn/@FrankTan/step-3.5-Flash-lora/runs/6vwsgcjgmnqqy7p9422gj\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <title>Show Iframe</title>\n",
       "    \n",
       "        <script>\n",
       "            function showIframe() {\n",
       "                var iframeHtml = '<iframe src=\"https://swanlab.cn/@FrankTan/step-3.5-Flash-lora/runs/6vwsgcjgmnqqy7p9422gj\" width=100% height=\"600\" frameborder=\"no\"></iframe>';\n",
       "                document.getElementById('iframeContainer').innerHTML = iframeHtml;\n",
       "            }\n",
       "        </script>\n",
       "        \n",
       "</head>\n",
       "<body>\n",
       "    <style>\n",
       "        .interactive-button {\n",
       "            display: flex;\n",
       "            align-items: center;\n",
       "            height: 36px;\n",
       "            border: 0px;\n",
       "            background-color: #2c8f63;\n",
       "            color: white;\n",
       "            padding: 10px 20px;\n",
       "            transition: background-color 0.3s, transform 0.2s;\n",
       "        }\n",
       "\n",
       "        .interactive-button:hover {\n",
       "            background-color: #5cab87;\n",
       "            cursor: pointer;\n",
       "        }\n",
       "\n",
       "        .interactive-button:active { background-color: #217952; transform: scale(0.96); } </style> <br> <button \n",
       "        onclick=\"showIframe()\" class=\"interactive-button\"> <svg style=\"height: 16px; margin-right: 8px;\" viewBox=\"0 0 \n",
       "        46 46\" fill=\"none\"> <path d=\"M10.8439 21.1974C10.6414 21.2854 10.4477 21.3925 10.2655 21.5173L10.2069 \n",
       "        21.5652C10.1839 21.58 10.1625 21.5969 10.1429 21.6159C6.29135 24.6118 4.22831 29.4416 5.32646 34.282C5.94656 \n",
       "        37.0577 7.50461 39.5348 9.73801 41.2958C11.9714 43.0568 14.7436 43.994 17.5874 43.9495H18.0219C19.8864 \n",
       "        43.8697 21.7087 43.3694 23.3526 42.486C24.9964 41.6026 26.4193 40.3589 27.5147 38.848C28.61 37.3371 29.3496 \n",
       "        35.598 29.678 33.761C30.0065 31.9239 29.9153 30.0363 29.4112 28.2395C28.9181 26.4723 27.8919 24.8437 26.9937 \n",
       "        23.2551C25.4158 20.4653 23.8343 17.6764 22.2492 14.8884C21.7801 14.0647 21.3057 13.2465 20.8419 \n",
       "        12.4228C20.2315 11.3353 19.2746 10.1519 19.224 8.86183C19.1733 7.57176 20.2235 6.32701 21.5082 \n",
       "        6.07912C23.9284 5.61801 25.0639 8.24078 25.0693 8.23812C25.363 8.94035 25.9123 9.50489 26.6063 \n",
       "        9.81764C27.3002 10.1304 28.087 10.168 28.8077 9.92298C29.5283 9.67791 30.1291 9.1684 30.4885 8.49743C30.8479 \n",
       "        7.82646 30.9392 7.04405 30.7439 6.30835C30.1514 4.37314 28.9133 2.69953 27.2363 1.56656C25.7615 0.511704 \n",
       "        23.9847 -0.0372109 22.1719 0.00195984C20.9049 0.00893199 19.6532 0.27989 18.4967 0.797557C17.3402 1.31522 \n",
       "        16.3043 2.06823 15.4551 3.00856C14.49 4.08707 13.7984 5.38193 13.4389 6.78385C13.0794 8.18576 13.0624 9.6536 \n",
       "        13.3894 11.0635C13.52 11.593 13.6984 12.1095 13.9225 12.6067C14.5595 14.0514 15.4951 15.3681 16.284 \n",
       "        16.7355C17.2525 18.4147 18.2209 20.0948 19.1893 21.7758C20.1578 23.4568 21.1351 25.1449 22.1213 \n",
       "        26.8401C22.9209 28.2421 23.7925 29.4682 23.8805 31.1528C23.9175 32.0513 23.7682 32.9479 23.4419 \n",
       "        33.7859C23.1156 34.6239 22.6194 35.3854 21.9845 36.0223C21.3496 36.6592 20.5897 37.1578 19.7527 \n",
       "        37.4868C18.9157 37.8157 18.0196 37.9678 17.121 37.9336C14.0024 37.7923 11.6488 35.4814 11.1744 32.4588C10.58 \n",
       "        28.6419 13.552 26.5469 13.552 26.5469C14.1782 26.1785 14.6497 25.5955 14.8791 24.906C15.1084 24.2166 15.0801 \n",
       "        23.4673 14.7993 22.7971C14.5186 22.127 14.0044 21.5813 13.3521 21.2611C12.6998 20.941 11.9536 20.8682 11.2517 \n",
       "        21.0561C11.1174 21.0939 10.9856 21.1402 10.8572 21.1947\" fill=\"white\" /> <path d=\"M42.8101 31.5968C42.8109 \n",
       "        30.5198 42.7218 29.4445 42.5435 28.3823C42.2663 26.7069 41.7464 25.0808 41.0002 23.5552C40.5524 22.6463 \n",
       "        39.9874 21.7374 39.1024 21.2417C38.6593 20.9919 38.1589 20.8617 37.6502 20.8639C37.1416 20.8661 36.6423 \n",
       "        21.0006 36.2013 21.2541C35.7604 21.5077 35.393 21.8716 35.1352 22.3101C34.8775 22.7485 34.7382 23.2466 \n",
       "        34.7312 23.7552C34.7072 24.8773 35.3149 25.8875 35.768 26.9217C36.5212 28.6453 36.8623 30.5208 36.7642 \n",
       "        32.3993C36.6661 34.2777 36.1315 36.1075 35.2029 37.7433C35.146 37.8404 35.0952 37.941 35.051 38.0445C34.8623 \n",
       "        38.4842 34.7635 38.9573 34.7605 39.4358C34.7802 40.1222 35.0356 40.7808 35.4835 41.3011C35.9315 41.8214 \n",
       "        36.5449 42.1717 37.2207 42.2932C38.8759 42.589 40.1899 41.347 40.8856 39.9609C42.1643 37.3589 42.823 34.4961 \n",
       "        42.8101 31.5968Z\" fill=\"white\" /> <path d=\"M28.2309 11.8938C28.1761 11.9043 28.1218 11.9176 28.0683 \n",
       "        11.9338C27.9593 11.9642 27.8611 12.0249 27.7851 12.1088C27.7091 12.1928 27.6584 12.2965 27.6389 \n",
       "        12.408C27.6193 12.5195 27.6318 12.6343 27.6748 12.7391C27.7178 12.8438 27.7895 12.9343 27.8818 \n",
       "        12.9999C29.2375 14.0252 30.3809 15.3043 31.2482 16.7662C31.4838 17.1677 31.6888 17.5865 31.8612 \n",
       "        18.0189C32.0052 18.3921 32.1971 18.8799 32.6822 18.8532C33.0607 18.8346 33.2153 18.512 33.3192 \n",
       "        18.1895C33.8137 16.5125 33.9678 14.7534 33.7723 13.0159C33.6331 12.0693 33.4155 11.1359 33.122 \n",
       "        10.2252C33.0775 10.0047 32.9744 9.80029 32.8235 9.6335C32.7273 9.54627 32.6054 9.49262 32.4761 9.4806C32.3468 \n",
       "        9.46859 32.2171 9.49886 32.1065 9.56687C32.0016 9.65188 31.9115 9.75365 31.8399 9.86806C31.3956 10.4658 \n",
       "        30.825 10.9581 30.1687 11.3101C29.8377 11.4861 29.4893 11.6272 29.1292 11.7312C28.828 11.8192 28.5215 11.8325 \n",
       "        28.2309 11.8938Z\" fill=\"white\" /> </svg> Display SwanLab Board </button> <br> <div \n",
       "        id=\"iframeContainer\"></div> </body> </html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/loraa/lib/python3.12/site-packages/torch/autograd/graph.py:865: UserWarning: The AccumulateGrad node's stream does not match the stream of the node that produced the incoming gradient. This may incur unnecessary synchronization and break CUDA graph capture if the AccumulateGrad node's stream is the default stream. This mismatch is caused by an AccumulateGrad node created prior to the current iteration being kept alive. This can happen if the autograd graph is still being kept alive by tensors such as the loss, or if you are using DDP, which will stash a reference to the node. To resolve the mismatch, delete all references to the autograd graph or ensure that DDP initialization is performed under the same stream as subsequent forwards. If the mismatch is intentional, you can use torch.autograd.graph.set_warn_on_accumulate_grad_stream_mismatch(False) to suppress this warning. (Triggered internally at /pytorch/torch/csrc/autograd/input_buffer.cpp:240.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1170' max='1170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1170/1170 3:01:44, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.826100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.510900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.171200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.770100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.338800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.890300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.488200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.213000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.111300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.050100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.028100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1170, training_loss=1.060844347313938, metrics={'train_runtime': 10919.3676, 'train_samples_per_second': 3.415, 'train_steps_per_second': 0.107, 'total_flos': 5.294349044001255e+18, 'train_loss': 1.060844347313938, 'epoch': 10.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model,                 # 处理好的 LoRA 模型\n",
    "    args=training_args,          # 上面的精简配置\n",
    "    train_dataset=train_dataset, # 你的训练集\n",
    "    data_collator=collator,      # 负责 Padding 的 Collator\n",
    "    callbacks=[swanlab_callback]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe736535",
   "metadata": {},
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "528ba1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/loraa/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The tokenizer you are loading from '/autodl-fs/data/stepfun-ai/Step-3___5-Flash' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 44/44 [03:34<00:00,  4.87s/it]\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128007 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回答：\n",
      "并无私情，皇上若不信，大可以彻查究竟。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "mode_path = '/autodl-fs/data/stepfun-ai/Step-3___5-Flash'\n",
    "lora_path = get_last_checkpoint('./lora_checkpoints/')\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "# 1. 加载 tokenizer (建议加上之前提到的修复参数)\n",
    "tokenizer = AutoTokenizer.from_pretrained(mode_path, trust_remote_code=True)\n",
    "\n",
    "# 2. 加载 Base Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    mode_path, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 3. 加载 LoRA 权重\n",
    "model = PeftModel.from_pretrained(model, model_id=lora_path)\n",
    "model.eval() # 切换到推理模式\n",
    "\n",
    "# 4. **构建与训练一致的 Prompt**\n",
    "prompt = \"你和温实初是什么关系？\"\n",
    "\n",
    "# 严格按照你训练时的 Dolly 格式拼接\n",
    "# 如果训练时没有 context，这里 Input 也可以省略\n",
    "# full_prompt = f\"### Instruction:{prompt}\\n\\n### Response:\\n\"\n",
    "full_prompt = f\"### Instruction:\\n{prompt}\\n\\n### Input:\\n\\n\\n### Response:\\n\"\n",
    "\n",
    "\n",
    "\n",
    "# 5. Tokenize 并将数据移动到显卡\n",
    "inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# 6. 生成\n",
    "gen_kwargs = {\n",
    "    \"max_new_tokens\": 512,  # 建议用 max_new_tokens 而不是 max_length\n",
    "    \"do_sample\": True, \n",
    "    \"top_p\": 0.9, \n",
    "    \"temperature\": 0.4,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"eos_token_id\": [128007, tokenizer.eos_token_id], \n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    # 只解码生成出来的部分（去掉输入的 prompt 部分）\n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    print(\"回答：\")\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
