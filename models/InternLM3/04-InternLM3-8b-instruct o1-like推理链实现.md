<h1>InternLM3-8b-instruct o1-like æ¨ç†é“¾å®ç°</h1>


## ç¯å¢ƒé…ç½®ä¾èµ–

ç¯å¢ƒä¾èµ–å¦‚ä¸‹ï¼š
```
----------------------
 Transformer >=4.48 
 Torch == 2.3.0     
 Cuda ==  12.1      
----------------------
```

 >æœ¬æ–‡é»˜è®¤å­¦ä¹ è€…å·²å®‰è£…å¥½ä»¥ä¸Š Pytorch(cuda) ç¯å¢ƒï¼Œå¦‚æœªå®‰è£…è¯·è‡ªè¡Œå®‰è£…ã€‚

## å‡†å¤‡å·¥ä½œ

é¦–å…ˆ `pip` æ¢æºåŠ é€Ÿä¸‹è½½å¹¶å®‰è£…ä¾èµ–åŒ…ï¼š

```shell
# å‡çº§pip
python -m pip install --upgrade pip
pip install vllm --download vllm
```
> è€ƒè™‘åˆ°éƒ¨åˆ†åŒå­¦é…ç½®ç¯å¢ƒå¯èƒ½ä¼šé‡åˆ°ä¸€äº›é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨ AutoDL å¹³å°å‡†å¤‡äº† InternLM3-8b-Instruct çš„ç¯å¢ƒé•œåƒï¼Œç‚¹å‡»ä¸‹æ–¹é“¾æ¥å¹¶ç›´æ¥åˆ›å»º AutoDL ç¤ºä¾‹å³å¯ã€‚
> ***https://www.codewithgpu.com/i/datawhalechina/self-llm/InternLM3-self-llm***

## æ¨¡å‹ä¸‹è½½

`modelscope` æ˜¯ä¸€ä¸ªæ¨¡å‹ç®¡ç†å’Œä¸‹è½½å·¥å…·ï¼Œæ”¯æŒä»é­”æ­ (Modelscope) ç­‰å¹³å°å¿«é€Ÿä¸‹è½½æ¨¡å‹ã€‚

è¿™é‡Œä½¿ç”¨ `modelscope` ä¸­çš„ `snapshot_download` å‡½æ•°ä¸‹è½½æ¨¡å‹ï¼Œç¬¬ä¸€ä¸ªå‚æ•°ä¸ºæ¨¡å‹åç§°ï¼Œç¬¬äºŒä¸ªå‚æ•° `cache_dir` ä¸ºæ¨¡å‹çš„ä¸‹è½½è·¯å¾„ï¼Œç¬¬ä¸‰ä¸ªå‚æ•° `revision` ä¸ºæ¨¡å‹çš„ç‰ˆæœ¬å·ã€‚

åœ¨ `/root/autodl-tmp` è·¯å¾„ä¸‹æ–°å»º `model_download.py` æ–‡ä»¶å¹¶åœ¨å…¶ä¸­ç²˜è´´ä»¥ä¸‹ä»£ç ï¼Œå¹¶ä¿å­˜æ–‡ä»¶ã€‚

```python
from modelscope import snapshot_download, AutoModel, AutoTokenizer

model_dir = snapshot_download('Shanghai_AI_Laboratory/internlm3-8b-instruct', cache_dir='./', revision='master')
```

> æ³¨æ„ï¼šè®°å¾—ä¿®æ”¹ cache_dir ä¸ºä½ çš„æ¨¡å‹ä¸‹è½½è·¯å¾„å“¦~
åœ¨ç»ˆç«¯è¿è¡Œ `python /root/autodl-tmp/model_download.py` æ‰§è¡Œä¸‹è½½ï¼Œæ¨¡å‹å¤§å°ä¸º 18GB å·¦å³ï¼Œä¸‹è½½æ¨¡å‹å¤§æ¦‚éœ€è¦5-30åˆ†é’Ÿã€‚

åœ¨ç»ˆç«¯è¿è¡Œ `python /root/autodl-tmp/model_download.py` æ‰§è¡Œä¸‹è½½ï¼Œæ¨¡å‹å¤§å°ä¸º 18GB å·¦å³ï¼Œä¸‹è½½æ¨¡å‹å¤§æ¦‚éœ€è¦5-30åˆ†é’Ÿã€‚

## å®ç°ä»£ç 

```shell
import streamlit as st
from g1 import generate_response
import json

def main():
    st.set_page_config(page_title="Internlm3-8b-instruct", page_icon="ğŸ§ ", layout="wide")
    
    st.title("internlm3-8b-instruct å®ç°o1-likeæ¨ç†é“¾")
    
    st.markdown("""
    [å¼€æºå¤§æ¨¡å‹é£Ÿç”¨æŒ‡å— self-llm](https://github.com/datawhalechina/self-llm)
    """)
    
    # Text input for user query
    user_query = st.text_input("Enter your query:", placeholder="e.g., How many 'R's are in the word strawberry?")
    
    if user_query:
        st.write("Generating response...")
        
        # Create empty elements to hold the generated text and total time
        response_container = st.empty()
        time_container = st.empty()
        
        # Generate and display the response
        for steps, total_thinking_time in generate_response(user_query):
            with response_container.container():
                for i, (title, content, thinking_time) in enumerate(steps):
                    # Ensure content is a string
                    if not isinstance(content, str):
                        content = json.dumps(content)
                    if title.startswith("Final Answer"):
                        st.markdown(f"### {title}")
                        if '```' in content:
                            parts = content.split('```')
                            for index, part in enumerate(parts):
                                if index % 2 == 0:
                                    st.markdown(part)
                                else:
                                    if '\n' in part:
                                        lang_line, code = part.split('\n', 1)
                                        lang = lang_line.strip()
                                    else:
                                        lang = ''
                                        code = part
                                    st.code(part, language=lang)
                        else:
                            st.markdown(content.replace('\n', '<br>'), unsafe_allow_html=True)
                    else:
                        with st.expander(title, expanded=True):
                            st.markdown(content.replace('\n', '<br>'), unsafe_allow_html=True)
            
            # Only show total time when it's available at the end
            if total_thinking_time is not None:
                time_container.markdown(f"**Total thinking time: {total_thinking_time:.2f} seconds**")

if __name__ == "__main__":
    main()

```
è¦æ³¨æ„çš„æ˜¯ï¼Œå¤§å®¶éœ€è¦è‡ªè¡Œé…ç½®groq api_key(åœ¨groq cloud)ä¸­è·å¾—ï¼Œå¹¶åœ¨powershellä¸­è¿›è¡Œä¸´æ—¶å˜é‡é…ç½®,æŒ‡ä»¤å¦‚ä¸‹
```shell
export GROQ_API_KEY=gsk...
```
## æ¨ç†ç»“æœ
<img, src=>
