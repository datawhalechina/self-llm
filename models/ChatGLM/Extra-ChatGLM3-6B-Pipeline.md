# ChatGLM3-6B Pipeline
æœ¬æ–‡æ¡£åŸºäºChatGLMçš„å‰å…­èŠ‚å†…å®¹åšæ•´åˆï¼Œæ„å»ºpipelineç‰ˆæœ¬ï¼Œä¿®å¤è¿‡ç¨‹ä¸­çš„ç‰ˆæœ¬ä¸åŒ¹é…é—®é¢˜ï¼ŒåŠåœ¨05èŠ‚ä¸­çš„ç¼–ç é—®é¢˜ï¼Œç›®çš„æ˜¯å¿«é€Ÿå½¢æˆå¯¹å¤§æ¨¡å‹éƒ¨ç½²çš„ä½“æ„Ÿã€‚

## ç¯å¢ƒé…ç½®

- æ¨¡å‹é€‰æ‹©ï¼šChatGLM3-6Bï¼Œæ¨¡å‹å¤§å°14G
- éƒ¨ç½²å¹³å°ï¼šAutoMLï¼Œç¯å¢ƒé…ç½®4090å•å¡24Gæ˜¾å¯¸
- é•œåƒé€‰æ‹©ï¼špytorch 2.1.0+python 3.10+cuda 12.1

## éƒ¨ç½²æ–¹å¼
ä¸ºæ–¹ä¾¿ç®¡ç†å„ä¸ªæ¨¡å—çš„ç‰ˆæœ¬ï¼Œæˆ‘ä»¬é‡‡ç”¨uvæ–¹å¼è¿›è¡Œéƒ¨ç½²ï¼š
1. å®‰è£…uv
```shell
curl -LsSf https://astral.sh/uv/install.sh | shsource $HOME/.cargo/env
```

2. åˆ›å»ºé¡¹ç›®ç¯å¢ƒ

```shell
mkdir -p /root/autodl-tmp/chatglm && cd /root/autodl-tmp/chatglm
```

3. åˆå§‹åŒ–è™šæ‹Ÿç¯å¢ƒ
```shell
uv venv --python 3.10 chatglm-env
```

4. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
```shell
source chatglm-env/bin/activate
```

5. å®‰è£…requirements.txt
```shell
# AutoDLå¼€å¯é•œåƒåŠ é€Ÿ
source /etc/network_turbo

# ä¸€é”®å®‰è£…
uv pip install -r requirements.txt
```

## Transformer åŸºç¡€éƒ¨ç½²

1. æ‰“å¼€jupyteråˆ‡æ¢ipykernelï¼Œé€‰æ‹©æˆ‘ä»¬æ–°åˆ›å»ºçš„ç¯å¢ƒ

<div align='center'>
    <img src="./images/extra-images/image-1.png" alt="alt text" width="90%">
    <p>1.jpg</p>
</div>

2. æ¨¡å‹ä¸‹è½½
æ¨¡å‹å¤§å°ä¸º14GBï¼Œå¼€é•œåƒåŠ é€Ÿåä¸‹è½½ã€‚

```python
import torch
from modelscope import snapshot_download, AutoModel, AutoTokenizer
import os
model_dir = snapshot_download('ZhipuAI/chatglm3-6b', cache_dir='/root/autodl-tmp', revision='master')
```

<div align='center'>
    <img src="./images/extra-images/image-2.png" alt="alt text" width="90%">
    <p>2.jpg</p>
</div>

3. å¯¼å…¥åº“å’Œé…ç½®

```python
# ä½¿ç”¨Hugging Faceä¸­'transformer'åº“ä¸­çš„AutoTokenizerå’ŒAutoModelForCausalLMä»¥åŠ è½½åˆ†è¯å™¨å’Œå¯¹è¯æ¨¡å‹
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ä½¿ç”¨æ¨¡å‹ä¸‹è½½åˆ°çš„æœ¬åœ°è·¯å¾„ä»¥åŠ è½½
model_dir = '/root/autodl-tmp/ZhipuAI/chatglm3-6b'
print(f"æ¨¡å‹è·¯å¾„: {model_dir}")
```

4. åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
```python
# åˆ†è¯å™¨çš„åŠ è½½ï¼Œæœ¬åœ°åŠ è½½ï¼Œtrust_remote_code=Trueè®¾ç½®å…è®¸ä»ç½‘ç»œä¸Šä¸‹è½½æ¨¡å‹æƒé‡å’Œç›¸å…³çš„ä»£ç 
print("æ­£åœ¨åŠ è½½åˆ†è¯å™¨...")
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)

# æ¨¡å‹åŠ è½½ï¼Œæœ¬åœ°åŠ è½½ï¼Œä½¿ç”¨AutoModelForCausalLMç±»
print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
model = AutoModelForCausalLM.from_pretrained(model_dir, trust_remote_code=True)

# å°†æ¨¡å‹ç§»åŠ¨åˆ°GPUä¸Šè¿›è¡ŒåŠ é€Ÿï¼ˆå¦‚æœæœ‰GPUçš„è¯ï¼‰
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")
model.to(device)

# ä½¿ç”¨æ¨¡å‹çš„è¯„ä¼°æ¨¡å¼æ¥äº§ç”Ÿå¯¹è¯
model.eval()
print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
```

<div align='center'>
    <img src="./images/extra-images/image-3.png" alt="alt text" width="90%">
    <p>3.jpg</p>
</div>


5. å¯¹è¯æµ‹è¯•
```pyhton
# ç¬¬ä¸€è½®å¯¹è¯
print("=== ç¬¬ä¸€è½®å¯¹è¯ ===")
response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(f"ç”¨æˆ·: ä½ å¥½")
print(f"ChatGLM: {response}")
print()

# ç¬¬äºŒè½®å¯¹è¯
print("=== ç¬¬äºŒè½®å¯¹è¯ ===")
response, history = model.chat(tokenizer, "è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±", history=history)
print(f"ç”¨æˆ·: è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±")
print(f"ChatGLM: {response}")
print()

# ç¬¬ä¸‰è½®å¯¹è¯
print("=== ç¬¬ä¸‰è½®å¯¹è¯ ===")
response, history = model.chat(tokenizer, "è¯·å¸®æˆ‘ä½¿ç”¨pythonè¯­è¨€å†™ä¸€æ®µå†’æ³¡æ’åºçš„ä»£ç ", history=history)
print(f"ç”¨æˆ·: è¯·å¸®æˆ‘ä½¿ç”¨pythonè¯­è¨€å†™ä¸€æ®µå†’æ³¡æ’åºçš„ä»£ç ")
print(f"ChatGLM: {response}")
```

<div align='center'>
    <img src="./images/extra-images/image-4.png" alt="alt text" width="90%">
    <p>4.jpg</p>
</div>


## FastApiæœåŠ¡åŒ–éƒ¨ç½²

é€šè¿‡FastAPIéƒ¨ç½²ï¼Œè®©æœ¬åœ°çš„ChatGLM3-6Bæ¨¡å‹å˜æˆä¸€ä¸ªæœåŠ¡ï¼Œå¯ä»¥è¢«ä»»ä½•æ”¯æŒHTTPçš„å®¢æˆ·ç«¯è°ƒç”¨ï¼Œè¿™æ ·å…¶ä»–ç³»ç»Ÿåªéœ€è¦é€šè¿‡HTTPæ¥å£ï¼Œå°±å¯ä»¥æœ‰ä½¿ç”¨AIçš„èƒ½åŠ›ã€‚

åœ¨AutoDLé‡Œï¼Œé€šè¿‡ç»ˆç«¯è¿è¡ŒæœåŠ¡ï¼š
```python
# api.py
from fastapi import FastAPI, Request
from transformers import AutoTokenizer, AutoModelForCausalLM
import uvicorn
import json
import datetime
import torch

# è®¾ç½®è®¾å¤‡å‚æ•°
DEVICE = "cuda"
DEVICE_ID = "0"
CUDA_DEVICE = f"{DEVICE}:{DEVICE_ID}" if DEVICE_ID else DEVICE

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI()

# å¤„ç†POSTè¯·æ±‚çš„ç«¯ç‚¹
@app.post("/")
async def create_item(request: Request):
    global model, tokenizer
    json_post_raw = await request.json()
    json_post = json.dumps(json_post_raw)
    json_post_list = json.loads(json_post)
    prompt = json_post_list.get('prompt')
    history = json_post_list.get('history')
    max_length = json_post_list.get('max_length')
    top_p = json_post_list.get('top_p')
    temperature = json_post_list.get('temperature')
    
    # è°ƒç”¨æ¨¡å‹è¿›è¡Œå¯¹è¯ç”Ÿæˆ
    response, history = model.chat(
        tokenizer,
        prompt,
        history=history,
        max_length=max_length if max_length else 2048,
        top_p=top_p if top_p else 0.7,
        temperature=temperature if temperature else 0.95
    )
    
    now = datetime.datetime.now()
    time = now.strftime("%Y-%m-%d %H:%M:%S")
    
    # æ„å»ºå“åº”JSON
    answer = {
        "response": response,
        "history": history,
        "status": 200,
        "time": time
    }
    
    # æ„å»ºæ—¥å¿—ä¿¡æ¯
    log = "[" + time + "] " + '", prompt:"' + prompt + '", response:"' + repr(response) + '"'
    print(log)
    return answer

# ä¸»å‡½æ•°å…¥å£
if __name__ == '__main__':
    # åŠ è½½é¢„è®­ç»ƒçš„åˆ†è¯å™¨å’Œæ¨¡å‹ - PyTorch 2.0ä¼˜åŒ–ç‰ˆæœ¬
    tokenizer = AutoTokenizer.from_pretrained(
        "/root/autodl-tmp/ZhipuAI/chatglm3-6b", 
        trust_remote_code=True
    )
    
    model = AutoModelForCausalLM.from_pretrained(
        "/root/autodl-tmp/ZhipuAI/chatglm3-6b", 
        trust_remote_code=True,
        torch_dtype=torch.float16,  # PyTorch 2.0å¯¹float16æ”¯æŒæ›´å¥½
        device_map="auto"  # åˆ©ç”¨PyTorch 2.0çš„è‡ªåŠ¨è®¾å¤‡æ˜ å°„
    )
    
    model.eval()  # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    
    # å¯åŠ¨FastAPIåº”ç”¨
    uvicorn.run(app, host='0.0.0.0', port=6006, workers=1)
```

åœ¨uvç¯å¢ƒä¸‹è¿è¡Œä¸Šé¢çš„ä»£ç ï¼Œå¹¶æ–°èµ·ä¸€ä¸ªç»ˆç«¯åšæµ‹è¯•ï¼Œçœ‹å›å¤æ•ˆæœï¼š
```shell
curl -X POST http://localhost:6006 -H "Content-Type: application/json" -d '{
  "prompt": "ä½ å¥½",
  "history": [],
  "max_length": 2048,
  "top_p": 0.7,
  "temperature": 0.95
}'
```

<div align='center'>
    <img src="./images/extra-images/image-5.png" alt="alt text" width="90%">
    <p>5.jpg</p>
</div>



æœåŠ¡è¿”å›çš„çŠ¶æ€å¦‚ä¸‹ï¼š

<div align='center'>
    <img src="./images/extra-images/image-6.png" alt="alt text" width="90%">
    <p>6.jpg</p>
</div>

## å®˜æ–¹chatç•Œé¢äº¤äº’
åœ¨å­¦ä¹ äº†åŸºç¡€çš„ Transformer æ¨¡å‹è°ƒç”¨å’Œ FastAPI æœåŠ¡éƒ¨ç½²ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥ä½“éªŒå®˜æ–¹æä¾›çš„äº¤äº’å¼ Chat ç•Œé¢ã€‚åœ¨ ChatGLM3 çš„å®˜æ–¹ç¤ºä¾‹ ä¸­ï¼Œæä¾›äº†ä¸¤ç§ä¸»æµçš„è½»é‡çº§ Web äº¤äº’æ–¹æ¡ˆï¼š
- web_demo_gradio.pyï¼ˆåŸºäº Gradioï¼‰
- web_demo_streamlit.pyï¼ˆåŸºäº Streamlitï¼‰

è¿™ä¸¤ç§æ¡†æ¶éƒ½æ”¯æŒå¿«é€Ÿæ„å»ºæ¨¡å‹å‰ç«¯ç•Œé¢ï¼Œé€‚åˆæœ¬åœ°è°ƒè¯•ä¸æ¼”ç¤ºã€‚æœ¬ä¾‹ä¸­æˆ‘ä»¬ä»¥ Streamlit ä¸ºä¾‹è¿›è¡Œéƒ¨ç½²ï¼Œå› å…¶å¸ƒå±€çµæ´»ï¼Œé€‚åˆæ„å»ºåŠŸèƒ½ä¸°å¯Œçš„äº¤äº’ç•Œé¢ã€‚é€šå¸¸æˆ‘ä»¬åœ¨åšåº”ç”¨æ—¶ï¼Œå¯ä»¥å‚è€ƒå®˜ç½‘æä¾›çš„ä»£ç ï¼Œæ¥å¯»æ‰¾æœ€ä½³å®è·µã€‚

ç”±äºéœ€è¦åœ¨ AutoDL å¹³å°ä¸Šä»å¤–éƒ¨è®¿é—®æœåŠ¡ï¼Œæˆ‘ä»¬å¯é€šè¿‡ç«¯å£ 6006 è¿›è¡ŒæœåŠ¡æ˜ å°„ï¼Œå®ç° Web ç•Œé¢çš„è¿œç¨‹è®¿é—®ã€‚

1. cloneè¯¥é¡¹ç›®ï¼Œä¿®æ”¹æ¨¡å‹è·¯å¾„åˆ°æœ¬åœ°

```shell
git clone https://github.com/THUDM/ChatGLM3.git

cd ChatGLM3/basic_demo

vim web_demo_streamlit.py 
```
ä¿®æ”¹MODEL_PATHå’ŒTOKENIZER_PATHä¸ºï¼š'/root/autodl-tmp/ZhipuAI/chatglm3-6b'

<div align='center'>
    <img src="./images/extra-images/image-7.png" alt="alt text" width="90%">
    <p>7.jpg</p>
</div>


è¿è¡Œå¯åŠ¨è¯¥streamlitåº”ç”¨ï¼š
```shell
streamlit run web_demo_streamlit.py --server.address 127.0.0.1 --server.port 6006
```

<div align='center'>
    <img src="./images/extra-images/image-8.png" alt="alt text" width="90%">
    <p>8.jpg</p>
</div>


åœ¨AutoDLä¸­ï¼Œéœ€è¦é€šè¿‡å®ä¾‹çš„è‡ªå®šä¹‰æœåŠ¡ï¼ŒæŠŠæ¥å£æ˜ å°„åˆ°æœ¬åœ°ï¼Œå…·ä½“æ“ä½œå‚è€ƒå¦‚ä¸‹æ­¥éª¤ï¼š

<div align='center'>
    <img src="./images/extra-images/image-9.png" alt="alt text" width="90%">
    <p>9.jpg</p>
</div>


åœ¨æœ¬åœ°é€šè¿‡http://localhost:6006è®¿é—®demoï¼Œæ•ˆæœå¦‚ä¸‹ï¼š

<div align='center'>
    <img src="./images/extra-images/image-10.png" alt="alt text" width="90%">
    <p>10.jpg</p>
</div>

## æ¥å…¥LangChainæ­å»ºçŸ¥è¯†åº“ï¼Œéƒ¨ç½²RAGåº”ç”¨
æ— è®ºæ˜¯FastAPIéƒ¨ç½²è¿˜æ˜¯åˆšæ‰çš„Streamlit webäº¤äº’ï¼ŒChatGLM3-6Béƒ½åªèƒ½åŸºäºå®ƒçš„ é¢„è®­ç»ƒçŸ¥è¯† æ¥å›ç­”é—®é¢˜ã€‚ä½†å¦‚æœæˆ‘æƒ³è®©å®ƒå›ç­”å…³äºæˆ‘ä»¬å…¬å¸å†…éƒ¨æ–‡æ¡£ã€æœ€æ–°æŠ€æœ¯èµ„æ–™æˆ–è€…ç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„é—®é¢˜å‘¢ï¼Ÿ

è¿™å°±éœ€è¦ç”¨åˆ°**RAGï¼ˆRetrieval-Augmented Generationï¼‰**ï¼Œç®€å•æ¥è¯´ï¼Œå°±æ˜¯"æ£€ç´¢å¢å¼ºç”Ÿæˆ"â€”â€”å…ˆä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œå†è®©å¤§æ¨¡å‹åŸºäºè¿™äº›ä¿¡æ¯ç”Ÿæˆå›ç­”ã€‚æ¥ä¸‹æ¥ï¼Œæœ¬å°èŠ‚å°†æ„å»ºä¸€ä¸ªå®Œæ•´çš„çŸ¥è¯†åº“åŠ©æ‰‹ï¼ŒåŸºäºSentence Transformerå’ŒChromaå‘é‡æ•°æ®åº“æ„å»ºè¯­æ–™åº“ï¼Œå¹¶å°†ChatGLM3-6Bæ¥å…¥LangChainæ¡†æ¶å®ç°å®Œæ•´çš„RAGæµç¨‹ã€‚


1. ä¸‹è½½è¯­æ–™åº“å†…å®¹

åˆ°æ•°æ®å­˜å‚¨ç›®å½•ä¸­ï¼Œä¸‹è½½çŸ¥è¯†åº“æºç ï¼š
```shell
cd /root/autodl-tmp
# ä¸‹è½½æ‰€æœ‰çŸ¥è¯†åº“æºç 
git clone https://github.com/datawhalechina/self-llm.git
git clone https://github.com/datawhalechina/llm-universe.git
git clone https://github.com/datawhalechina/prompt-engineering-for-developers.git
git clone https://github.com/datawhalechina/so-large-lm.git
git clone https://github.com/datawhalechina/hugging-llm.git
```

<div align='center'>
    <img src="./images/extra-images/image-11.png" alt="alt text" width="90%">
    <p>11.jpg</p>
</div>

2. ç¯å¢ƒéªŒè¯æµ‹è¯•
```python
# å®Œæ•´çš„ChatGLM3-6B LangChainé›†æˆæµ‹è¯•
import sys
print(f"Pythonç‰ˆæœ¬: {sys.version}")

try:
    # æ ¸å¿ƒä¾èµ–æµ‹è¯•
    from sentence_transformers import SentenceTransformer
    from huggingface_hub import cached_download
    from langchain.embeddings.huggingface import HuggingFaceEmbeddings
    from langchain.vectorstores import Chroma
    from langchain.chains import RetrievalQA
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    
    print("âœ… æ‰€æœ‰æ ¸å¿ƒç»„ä»¶å¯¼å…¥æˆåŠŸï¼")
    
    # æµ‹è¯•å®é™…åŠŸèƒ½
    embeddings = HuggingFaceEmbeddings(
        model_name="/root/autodl-tmp/sentence-transformer"
    )
    print("âœ… å‘é‡åŒ–æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    
    print("ğŸ‰ ChatGLM3-6B LangChainç¯å¢ƒé…ç½®å®Œæˆï¼")
    
except Exception as e:
    print(f"âŒ é”™è¯¯è¯¦æƒ…: {e}")
    import traceback
    traceback.print_exc()
```

<div align='center'>
    <img src="./images/extra-images/image-12.png" alt="alt text" width="90%">
    <p>12.jpg</p>
</div>

3. ä¸‹è½½Sentence-Transformeræ¨¡å‹ï¼Œç”¨äºå‘é‡åŒ–æ•°æ®åº“
```python
# åˆ›å»ºå‘é‡æ¨¡å‹ç›®å½•
mkdir -p /root/autodl-tmp/sentence-transformer
cd /root/autodl-tmp

# ä¸‹è½½å¤šè¯­è¨€å‘é‡æ¨¡å‹
modelscope download --model=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 --local_dir=./sentence-transformer
```

4. åˆ›å»ºå‘é‡æ•°æ®åº“

```python
# dbinit.py
# é¦–å…ˆå¯¼å…¥æ‰€éœ€ç¬¬ä¸‰æ–¹åº“
from langchain.document_loaders import UnstructuredFileLoader
from langchain.document_loaders import UnstructuredMarkdownLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.schema import Document
from tqdm import tqdm
import os
import chardet

# è·å–æ–‡ä»¶è·¯å¾„å‡½æ•°
def get_files(dir_path):
    file_list = []
    for filepath, dirnames, filenames in os.walk(dir_path):
        for filename in filenames:
            if filename.endswith(".md") or filename.endswith(".txt"):
                file_list.append(os.path.join(filepath, filename))
    return file_list

# æ£€æµ‹æ–‡ä»¶ç¼–ç 
def detect_encoding(file_path):
    with open(file_path, 'rb') as f:
        raw_data = f.read(10000)  # è¯»å–å‰10KBç”¨äºæ£€æµ‹
        result = chardet.detect(raw_data)
        return result['encoding']

# å®‰å…¨è¯»å–æ–‡ä»¶å†…å®¹
def safe_read_file(file_path):
    encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1', 'cp1252']
    
    for encoding in encodings:
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                content = f.read()
                return content, encoding
        except UnicodeDecodeError:
            continue
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
            continue
    
    # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œå°è¯•è‡ªåŠ¨æ£€æµ‹
    try:
        detected_encoding = detect_encoding(file_path)
        if detected_encoding:
            with open(file_path, 'r', encoding=detected_encoding) as f:
                content = f.read()
                return content, detected_encoding
    except:
        pass
    
    return None, None

# æ”¹è¿›çš„æ–‡ä»¶åŠ è½½å‡½æ•°
def get_text_robust(dir_path):
    file_lst = get_files(dir_path)
    docs = []
    failed_files = []
    
    print(f"\nå¼€å§‹å¤„ç†æ–‡ä»¶å¤¹: {dir_path}")
    print(f"æ‰¾åˆ° {len(file_lst)} ä¸ªæ–‡ä»¶")
    
    for one_file in tqdm(file_lst, desc=f"å¤„ç† {os.path.basename(dir_path)}"):
        try:
            file_type = one_file.split('.')[-1]
            
            # é¦–å…ˆå°è¯•ä½¿ç”¨åŸå§‹åŠ è½½å™¨
            try:
                if file_type == 'md':
                    loader = UnstructuredMarkdownLoader(one_file)
                elif file_type == 'txt':
                    loader = UnstructuredFileLoader(one_file)
                else:
                    continue
                docs.extend(loader.load())
                
            except UnicodeDecodeError:
                # å¦‚æœç¼–ç é”™è¯¯ï¼Œä½¿ç”¨å®‰å…¨è¯»å–æ–¹æ³•
                print(f"\nç¼–ç é”™è¯¯ï¼Œå°è¯•å®‰å…¨è¯»å–: {one_file}")
                content, encoding = safe_read_file(one_file)
                if content:
                    docs.append(Document(
                        page_content=content, 
                        metadata={"source": one_file, "encoding": encoding}
                    ))
                    print(f"æˆåŠŸè¯»å–ï¼Œä½¿ç”¨ç¼–ç : {encoding}")
                else:
                    failed_files.append(one_file)
                    print(f"è·³è¿‡æ–‡ä»¶: {one_file}")
                    
        except Exception as e:
            failed_files.append(one_file)
            print(f"\nå¤„ç†æ–‡ä»¶å¤±è´¥: {one_file}, é”™è¯¯: {e}")
            continue
    
    print(f"\næ–‡ä»¶å¤¹ {dir_path} å¤„ç†å®Œæˆ:")
    print(f"- æˆåŠŸå¤„ç†: {len(file_lst) - len(failed_files)} ä¸ªæ–‡ä»¶")
    print(f"- å¤±è´¥æ–‡ä»¶: {len(failed_files)} ä¸ª")
    if failed_files:
        print("å¤±è´¥æ–‡ä»¶åˆ—è¡¨:")
        for f in failed_files[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"  - {f}")
        if len(failed_files) > 5:
            print(f"  - ... è¿˜æœ‰ {len(failed_files) - 5} ä¸ªæ–‡ä»¶")
    
    return docs

# ç›®æ ‡æ–‡ä»¶å¤¹
tar_dir = [
    "/root/autodl-tmp/self-llm",
    "/root/autodl-tmp/llm-universe",
    "/root/autodl-tmp/prompt-engineering-for-developers",
    "/root/autodl-tmp/so-large-lm",
    "/root/autodl-tmp/hugging-llm",
]

# åŠ è½½ç›®æ ‡æ–‡ä»¶
docs = []
for i, dir_path in enumerate(tar_dir):
    print(f"\n=== å¤„ç†ç¬¬ {i+1}/{len(tar_dir)} ä¸ªæ–‡ä»¶å¤¹ ===")
    try:
        folder_docs = get_text_robust(dir_path)
        docs.extend(folder_docs)
        print(f"ç´¯è®¡æ–‡æ¡£æ•°é‡: {len(docs)}")
    except Exception as e:
        print(f"å¤„ç†æ–‡ä»¶å¤¹ {dir_path} æ—¶å‡ºé”™: {e}")
        continue

print(f"\n=== æ–‡æ¡£åŠ è½½å®Œæˆ ===")
print(f"æ€»æ–‡æ¡£æ•°é‡: {len(docs)}")

# å¯¹æ–‡æœ¬è¿›è¡Œåˆ†å—
print("\nå¼€å§‹æ–‡æœ¬åˆ†å—...")
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500, chunk_overlap=150)
split_docs = text_splitter.split_documents(docs)
print(f"åˆ†å—åæ–‡æ¡£æ•°é‡: {len(split_docs)}")

# åŠ è½½å¼€æºè¯å‘é‡æ¨¡å‹
print("\nåŠ è½½è¯å‘é‡æ¨¡å‹...")
embeddings = HuggingFaceEmbeddings(model_name="/root/autodl-tmp/sentence-transformer")

# æ„å»ºå‘é‡æ•°æ®åº“
print("\næ„å»ºå‘é‡æ•°æ®åº“...")
persist_directory = 'data_base/vector_db/chroma'
vectordb = Chroma.from_documents(
    documents=split_docs,
    embedding=embeddings,
    persist_directory=persist_directory
)

# æŒä¹…åŒ–
vectordb.persist()
print("\n=== å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼ ===")
print(f"æ•°æ®åº“ä½ç½®: {persist_directory}")
print(f"å¤„ç†çš„æ–‡æ¡£æ•°é‡: {len(docs)}")
print(f"å‘é‡åŒ–çš„æ–‡æœ¬å—æ•°é‡: {len(split_docs)}")
```

<div align='center'>
    <img src="./images/extra-images/image-13.png" alt="alt text" width="90%">
    <p>13.jpg</p>
</div>


æœ€ç»ˆå…±åŠ è½½äº†450ä¸ªæ–‡æ¡£ã€25768ä¸ªæ–‡æœ¬å—ï¼Œå¹³å‡æ¯ä¸ªæ–‡æ¡£è¢«åˆ†å‰²æˆçº¦57ä¸ªæ–‡æœ¬å—ï¼Œè®¾ç½®çš„`chunk_size=500, chunk_overlap=150`,è¿™ä¸ªé…ç½®é€‚åˆæ–‡æœ¬å¿«çš„æ£€ç´¢ï¼Œä¸è‡³äºè¿‡å¤§æˆ–è¿‡å°ã€‚

`Failed to send telemetry event`åªæ˜¯ChromaDBå°è¯•å‘é€ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯æ—¶çš„ç‰ˆæœ¬å…¼å®¹é—®é¢˜å‘Šè­¦ï¼Œé‡è¦çš„æ˜¯å‘é‡æ•°æ®åº“æœ¬èº«å·¥ä½œæ­£å¸¸ã€‚

<div align='center'>
    <img src="./images/extra-images/image-14.png" alt="alt text" width="90%">
    <p>14.jpg</p>
</div>


5. ChatGLMæ¥å…¥LangChain

LangChainæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œå®ƒè¦æ±‚æ‰€æœ‰ LLM å¿…é¡»éµå¾ªç»Ÿä¸€çš„æ¥å£è§„èŒƒï¼Œä½¿å¤§æ¨¡å‹ï¼ˆæœ¬ä¾‹ä¸ºChatGLMï¼‰èƒ½å¤Ÿä½œä¸ºæ ‡å‡†ç»„ä»¶ï¼Œå‚ä¸æç¤ºå·¥ç¨‹ã€è®°å¿†ç®¡ç†ã€æ£€ç´¢å¢å¼ºã€æ™ºèƒ½ä»£ç†ç­‰é«˜çº§æµç¨‹ã€‚

æˆ‘ä»¬å…ˆé€šè¿‡ç»Ÿä¸€çš„LLMç±»ï¼Œå®ç°ChatGLMçš„LangChainå°è£…ï¼š

```python
# LLM.py
from langchain.llms.base import LLM
from typing import Any, List, Optional
from langchain.callbacks.manager import CallbackManagerForLLMRun
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

class ChatGLM_LLM(LLM):
    # åŸºäºæœ¬åœ° ChatGLM è‡ªå®šä¹‰ LLM ç±»
    tokenizer : AutoTokenizer = None
    model: AutoModelForCausalLM = None

    def __init__(self, model_path :str):
        # model_path: ChatGLM æ¨¡å‹è·¯å¾„
        # ä»æœ¬åœ°åˆå§‹åŒ–æ¨¡å‹
        super().__init__()
        print("æ­£åœ¨ä»æœ¬åœ°åŠ è½½æ¨¡å‹...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True).to(torch.bfloat16).cuda()
        self.model = self.model.eval()
        print("å®Œæˆæœ¬åœ°æ¨¡å‹çš„åŠ è½½")

    def _call(self, prompt : str, stop: Optional[List[str]] = None,
                run_manager: Optional[CallbackManagerForLLMRun] = None,
                **kwargs: Any):
        # é‡å†™è°ƒç”¨å‡½æ•°
        response, history = self.model.chat(self.tokenizer, prompt , history=[])
        return response
        
    @property
    def _llm_type(self) -> str:
        return "ChatGLM3-6B"
```

éªŒè¯ChatGLMæ˜¯å¦åšäº†å‘é‡çŸ¥è¯†åº“çš„å¢å¼ºï¼Œå’ŒLLM.pyæ”¾åœ¨åŒç›®å½•ä¸‹ï¼š

```python
# test_qa_chain.py
from langchain.vectorstores import Chroma
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from LLM import ChatGLM_LLM
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
import os

def load_chain():
    """åŠ è½½æ£€ç´¢é—®ç­”é“¾"""
    print("æ­£åœ¨åŠ è½½å‘é‡æ•°æ®åº“...")
    
    # å®šä¹‰ Embeddings
    embeddings = HuggingFaceEmbeddings(model_name="/root/autodl-tmp/sentence-transformer")

    # å‘é‡æ•°æ®åº“æŒä¹…åŒ–è·¯å¾„
    persist_directory = 'data_base/vector_db/chroma'

    # åŠ è½½æ•°æ®åº“
    vectordb = Chroma(
        persist_directory=persist_directory,
        embedding_function=embeddings
    )
    
    print("æ­£åœ¨åŠ è½½ChatGLMæ¨¡å‹...")
    # åŠ è½½è‡ªå®šä¹‰ LLM
    llm = ChatGLM_LLM(model_path="/root/autodl-tmp/ZhipuAI/chatglm3-6b")

    # å®šä¹‰ Prompt Template
    template = """ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”æ¡ˆã€‚å°½é‡ä½¿ç­”æ¡ˆç®€æ˜æ‰¼è¦ã€‚æ€»æ˜¯åœ¨å›ç­”çš„æœ€åè¯´"è°¢è°¢ä½ çš„æé—®ï¼"ã€‚
{context}
é—®é¢˜: {question}
æœ‰ç”¨çš„å›ç­”:"""

    QA_CHAIN_PROMPT = PromptTemplate(input_variables=["context","question"],template=template)

    # æ„å»ºæ£€ç´¢é—®ç­”é“¾
    qa_chain = RetrievalQA.from_chain_type(
        llm,
        retriever=vectordb.as_retriever(),
        return_source_documents=True,
        chain_type_kwargs={"prompt":QA_CHAIN_PROMPT}
    )
    
    return qa_chain

def test_qa_chain():
    """æµ‹è¯•æ£€ç´¢é—®ç­”é“¾æ•ˆæœ"""
    # åŠ è½½é—®ç­”é“¾
    qa_chain = load_chain()
    
    # æµ‹è¯•é—®é¢˜
    questions = [
        "ä»€ä¹ˆæ˜¯ Self LLMï¼Ÿ",
        "ChatGLM3-6B æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ",
        "å¦‚ä½•éƒ¨ç½² ChatGLM æ¨¡å‹ï¼Ÿ"
    ]
    
    print("\n=== å¼€å§‹æµ‹è¯•æ£€ç´¢é—®ç­”é“¾ ===")
    
    for i, question in enumerate(questions, 1):
        print(f"\n--- æµ‹è¯•é—®é¢˜ {i} ---")
        print(f"é—®é¢˜ï¼š{question}")
        
        # æ£€ç´¢é—®ç­”é“¾å›ç­”
        result = qa_chain({"query": question})
        print(f"æ£€ç´¢é—®ç­”é“¾å›ç­”ï¼š{result['result']}")
        
        # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£æ•°é‡
        print(f"æ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£æ•°é‡ï¼š{len(result['source_documents'])}")
        print("-" * 50)

if __name__ == "__main__":
    test_qa_chain()
```

<div align='center'>
    <img src="./images/extra-images/image-15.png" alt="alt text" width="90%">
    <p>15.jpg</p>
</div>


## Loraå¾®è°ƒéƒ¨ç½²

å¾®è°ƒæŠ€æœ¯æ˜¯ä¸ªå¾ˆé‡è¦çš„æŠ€èƒ½ï¼Œä¸¾ä¸ªä¾‹å­ï¼Œè…¾è®¯åŠ¨æ¼«ä¸­æœ‰ä¸€ä¸ªæ¼«ç”»è§’è‰²AIåŠ©æ‰‹ï¼Œè®©ç”¨æˆ·å¯ä»¥ç›´æ¥ä¸æ¼«ç”»è§’è‰²åšå¯¹è¯ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨åŸç‰ˆChatGLM3-6Bæ¨¡å‹æ¥åšï¼Œæ¨¡å‹ä¾ç„¶åªè®°ä½è‡ªå·±æ˜¯ChatGLM3-6Bï¼Œè€Œä¸æ˜¯è‡ªå·±æ˜¯æŸä¸ªè§’è‰²çš„è®¾å®šã€‚

æˆ‘ä»¬éœ€è¦åªè®­ç»ƒä¸€å°éƒ¨åˆ†æ–°å¢çš„å‚æ•°ï¼Œæ›´æ–°è¯¥æ¨¡å‹çš„è®¾å®šï¼Œå¯ä»¥ç”¨Loraï¼ˆLow-Rank Adaptationï¼‰æ¥æ„å»ºä¸ªæ€§åŒ–é£æ ¼çš„LLMã€‚


1. ä¸‹è½½è®­ç»ƒæ•°æ®é›†

æ ¼å¼ä¸ºï¼šinstruction-input-outputä¸‰å…ƒç»“æ„ï¼ˆå‚è€ƒSelf-Instructè®ºæ–‡ï¼‰
```shell
wget https://raw.githubusercontent.com/datawhalechina/self-llm/master/dataset/huanhuan.json
```

2. è®­ç»ƒLoRAæƒé‡
```python
# lora_finetune.py
import torch
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer
import pandas as pd
from peft import TaskType, get_peft_model, LoraConfig
import json
import os

# æ•°æ®å¤„ç†å‡½æ•°
def process_func(example):
    MAX_LENGTH = 512
    input_ids, labels = [], []
    
    instruction_text = "\n".join([
        "<|system|>",
        "ç°åœ¨ä½ è¦æ‰®æ¼”çš‡å¸èº«è¾¹çš„å¥³äºº--ç”„å¬›",
        "<|user|>",
        example["instruction"] + example["input"] + "<|assistant|>"
    ]).strip() + "\n"
    
    instruction = tokenizer(
        instruction_text,
        add_special_tokens=True,
        truncation=True,
        max_length=MAX_LENGTH,
        return_tensors=None
    )["input_ids"]
    
    response = tokenizer(
        example["output"],
        add_special_tokens=False,
        truncation=True,
        max_length=MAX_LENGTH,
        return_tensors=None
    )["input_ids"]
    
    input_ids = instruction + response + [tokenizer.eos_token_id]
    labels = [tokenizer.pad_token_id] * len(instruction) + response + [tokenizer.eos_token_id]
    
    # ç¡®ä¿é•¿åº¦ä¸€è‡´
    if len(input_ids) > MAX_LENGTH:
        input_ids = input_ids[:MAX_LENGTH]
        labels = labels[:MAX_LENGTH]
    else:
        pad_len = MAX_LENGTH - len(input_ids)
        input_ids += [tokenizer.pad_token_id] * pad_len
        labels += [tokenizer.pad_token_id] * pad_len
    
    labels = [(l if l != tokenizer.pad_token_id else -100) for l in labels]

    return {
        "input_ids": input_ids,
        "labels": labels
    }

# ğŸ”¥ ä¿®æ”¹è®­ç»ƒå‚æ•°é…ç½® - æ›´æ–°ä¿å­˜è·¯å¾„
args = TrainingArguments(
    output_dir="/root/output/ChatGLM-Lora",  # ğŸ”¥ ä¿®æ”¹ä¸ºæŒ‡å®šè·¯å¾„
     per_device_train_batch_size=1,  # å¢åŠ æ‰¹æ¬¡å¤§å°
    gradient_accumulation_steps=8,
    logging_steps=10,
    num_train_epochs=3,  # å¢åŠ åˆ°3ä¸ªepoch
    learning_rate=2e-4,  # ç¨å¾®æé«˜å­¦ä¹ ç‡
    save_steps=100,
    save_total_limit=2,
    dataloader_pin_memory=False,
    remove_unused_columns=False
)

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹ChatGLM3-6B Loraå¾®è°ƒ...")
    
    # 1. åŠ è½½æ•°æ®é›†
    print("ğŸ“Š åŠ è½½æ•°æ®é›†...")
    df = pd.read_json('./huanhuan.json')
    ds = Dataset.from_pandas(df)
    print(f"æ•°æ®é›†å¤§å°: {len(ds)}")
    
    # 2. åŠ è½½tokenizer
    print("ğŸ”¤ åŠ è½½tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        "/root/autodl-tmp/ZhipuAI/chatglm3-6b",
        trust_remote_code=True
    )
    
    # 3. æ•°æ®é¢„å¤„ç†
    print("âš™ï¸ æ•°æ®é¢„å¤„ç†...")
    tokenized_ds = ds.map(process_func, remove_columns=ds.column_names)
    
    # 4. åŠ è½½æ¨¡å‹
    print("ğŸ¤– åŠ è½½ChatGLM3-6Bæ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(
        "/root/autodl-tmp/ZhipuAI/chatglm3-6b",
        torch_dtype=torch.half,
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    
    # 5. åˆ›å»ºLoRAå‚æ•°
    print("ğŸ”§ é…ç½®LoRAå‚æ•°...")
    config = LoraConfig(
        task_type=TaskType.CAUSAL_LM, 
        target_modules=["query_key_value", "dense", "dense_h_to_4h", "dense_4h_to_h"],  # æ›´å¤šæ¨¡å—
        r=16,
        lora_alpha=32,
        lora_dropout=0.1
    )
    
    # 6. æ¨¡å‹åˆå¹¶
    print("ğŸ”— åº”ç”¨LoRAé€‚é…å™¨...")
    model = get_peft_model(model, config)
    
    # ç¡®ä¿LoRAå‚æ•°å¯è®­ç»ƒ
    model.train()
    for name, param in model.named_parameters():
        if 'lora' in name.lower():
            param.requires_grad = True
            print(f"âœ… æ¿€æ´»LoRAå‚æ•°: {name}")
    
    # æ‰“å°å¯è®­ç»ƒå‚æ•°ç»Ÿè®¡
    model.print_trainable_parameters()
    
    # 7. é…ç½®æ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForSeq2Seq(
        tokenizer,
        model=model,
        label_pad_token_id=-100,
        pad_to_multiple_of=None,
        padding=True,
        return_tensors="pt"
    )
    
    # 8. åˆ›å»ºè®­ç»ƒå™¨
    print("ğŸƒ åˆ›å»ºè®­ç»ƒå™¨...")
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=tokenized_ds,
        data_collator=data_collator,
    )
    
    # 9. å¼€å§‹è®­ç»ƒ
    print("ğŸ¯ å¼€å§‹è®­ç»ƒ...")
    trainer.train()
    
    # 10. ä¿å­˜æ¨¡å‹ - ğŸ”¥ å¢å¼ºç‰ˆä¿å­˜é€»è¾‘
    print("ğŸ’¾ ä¿å­˜LoRAæƒé‡...")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    save_path = "/root/output/ChatGLM-Lora"
    os.makedirs(save_path, exist_ok=True)
    
    # ä¿å­˜LoRAæƒé‡
    trainer.save_model(save_path)
    
    # ç›´æ¥ä½¿ç”¨model.save_pretrainedç¡®ä¿ä¿å­˜æˆåŠŸ
    model.save_pretrained(save_path)
    
    # éªŒè¯ä¿å­˜æ˜¯å¦æˆåŠŸ
    required_files = ["adapter_config.json", "adapter_model.bin"]
    all_files_exist = all(os.path.exists(os.path.join(save_path, f)) for f in required_files)
    
    if all_files_exist:
        print(f"âœ… LoRAæƒé‡å·²æˆåŠŸä¿å­˜åˆ°: {save_path}")
        print(f"ğŸ“ ä¿å­˜çš„æ–‡ä»¶:")
        for file in os.listdir(save_path):
            file_path = os.path.join(save_path, file)
            file_size = os.path.getsize(file_path)
            print(f"   - {file} ({file_size} bytes)")
    else:
        print("âŒ ä¿å­˜å¤±è´¥ï¼ç¼ºå°‘å¿…è¦æ–‡ä»¶:")
        for f in required_files:
            if not os.path.exists(os.path.join(save_path, f)):
                print(f"   âŒ ç¼ºå°‘: {f}")
    
    print("âœ… å¾®è°ƒå®Œæˆï¼")
```

<div align='center'>
    <img src="./images/extra-images/image-16.png" alt="alt text" width="90%">
    <p>16.jpg</p>
</div>

3. åŠ è½½Loraæƒé‡æµ‹è¯•æ•ˆæœï¼š

```python
# lora_models.py
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch

# åŠ è½½åŸºç¡€æ¨¡å‹å’Œtokenizer
model = AutoModelForCausalLM.from_pretrained("/root/autodl-tmp/ZhipuAI/chatglm3-6b", 
                                           trust_remote_code=True, 
                                           low_cpu_mem_usage=True,
                                           torch_dtype=torch.half,
                                           device_map="auto")
tokenizer = AutoTokenizer.from_pretrained("/root/autodl-tmp/ZhipuAI/chatglm3-6b", 
                                         use_fast=False, 
                                         trust_remote_code=True)

# åŠ è½½LoRAæƒé‡
p_model = PeftModel.from_pretrained(model, model_id="/root/output/ChatGLM-Lora")

# æµ‹è¯•å‡½æ•°
def test_model(question):
    # æŒ‰ç…§è®­ç»ƒæ—¶çš„æ ¼å¼æ„é€ è¾“å…¥
    prompt = "<|system|>\nç°åœ¨ä½ è¦æ‰®æ¼”çš‡å¸èº«è¾¹çš„å¥³äºº--ç”„å¬›\n<|user|>\n{}\n<|assistant|>\n".format(question)
    
    inputs = tokenizer(prompt, return_tensors="pt").to(p_model.device)
    
    with torch.no_grad():
        outputs = p_model.generate(
            **inputs,
            max_length=128,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

# æµ‹è¯•é—®é¢˜
test_questions = [
    "ä½ æ˜¯è°ï¼Ÿ",
    "ä½ çš„çˆ¶äº²æ˜¯è°ï¼Ÿ",
    "ä½ è§‰å¾—çš‡ä¸Šæ€ä¹ˆæ ·ï¼Ÿ",
    "ä½ æœ€å–œæ¬¢ä»€ä¹ˆï¼Ÿ"
]

print("=== LoRAå¾®è°ƒåçš„ç”„å¬›æ¨¡å‹æµ‹è¯• ===")
for question in test_questions:
    print(f"\né—®é¢˜: {question}")
    answer = test_model(question)
    print(f"ç”„å¬›: {answer.split('<|assistant|>')[-1].strip()}")
```

<div align='center'>
    <img src="./images/extra-images/image-17.png" alt="alt text" width="90%">
    <p>17.jpg</p>
</div>


æ³¨ï¼šå—é™äºæ•°æ®ç›˜å¤§å°ï¼ŒPromptä¹Ÿä¼šå¯¹æ•ˆæœèµ·åˆ°å½±å“ï¼Œæ›´å¥½çš„æ•ˆæœå¯ä»¥åœ¨è‡ªè¡Œè®­ç»ƒä¸­åŠ æ·±è®­ç»ƒè½®æ¬¡ã€‚

## Code Interpreteréƒ¨ç½²ä½¿ç”¨

æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœChatGLMä¸ä»…èƒ½å¸®ä½ å†™ä»£ç ï¼Œè¿˜èƒ½ç›´æ¥è¿è¡Œè¿™äº›ä»£ç å¹¶ç»™ä½ ç»“æœï¼Œè¿™ä¼šå¸¦æ¥ä»€ä¹ˆæ ·çš„ä½“éªŒï¼Ÿ

å½“ä½ ä½¿ç”¨ChatGLMæ™®é€šæ¨¡å¼ï¼Œé—®"å¸®æˆ‘è®¡ç®—1åˆ°100çš„å’Œ"ï¼Œæ¨¡å‹ä¼šè¿”å›ä»£ç `sum(range(1,101))`, è€Œå½“ä½ ä½¿ç”¨Code Interpreteræ¨¡å¼ï¼Œé—®åŒæ ·é—®é¢˜ï¼Œæ¨¡å‹ä¸ä»…è¿”å›ä»£ç ï¼Œè¿˜æ‰§è¡Œä»£ç ï¼Œæœ€ç»ˆç›´æ¥å‘Šè¯‰ä½ ç»“æœæ˜¯ï¼š5050ã€‚

é‚£ä¹ˆé€šè¿‡Interpreterï¼Œå°±å¯ä»¥å®Œæˆç¨‹åºçš„è‡ªåŠ¨è°ƒè¯•å’Œä¿®æ­£ï¼Œè€Œéä¸€æ¬¡æ¬¡çš„è¾“å…¥äº¤äº’åšé‡è¯•ã€‚

ä¸‹é¢æˆ‘ä»¬é€šè¿‡å®˜æ–¹çš„demoåšä½¿ç”¨ï¼Œåœ¨å‰é¢çš„æ­¥éª¤ä¸­å·²ç»cloneäº†ChatGLM3ä»“åº“ä»£ç ï¼Œæ‰€ä»¥è¿™é‡Œæˆ‘ä»¬ä¹‹é—´è¿›å…¥ï¼š

```shell
cd /root/autodl-tmp/ChatGLM3/composite_demo
```

è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
```shell
export MODEL_PATH=/root/autodl-tmp/ZhipuAI/chatglm3-6b
export IPYKERNEL=python3
```

ä¾æ—§ä½¿ç”¨streamlitå¯åŠ¨6006çš„æœåŠ¡ï¼š
```shell
streamlit run main.py --server.port 6006
```

<div align='center'>
    <img src="./images/extra-images/image-18.png" alt="alt text" width="90%">
    <p>18.jpg</p>
</div>

è¿è¡Œç•Œé¢ä¸ºï¼š

<div align='center'>
    <img src="./images/extra-images/image-19.png" alt="alt text" width="90%">
    <p>19.jpg</p>
</div>

åˆ‡æ¢ä¸ºCode Interpreteræ¨¡å¼åšæé—®ï¼Œå¯ä»¥è¿è¡Œä»£ç ç”Ÿæˆç»“æœï¼š

<div align='center'>
    <img src="./images/extra-images/image-20.png" alt="alt text" width="90%">
    <p>20.jpg</p>
</div>
