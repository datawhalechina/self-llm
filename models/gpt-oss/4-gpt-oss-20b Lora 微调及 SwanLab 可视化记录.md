# 04-gpt-oss-20b Loraå¾®è°ƒä»¥åŠSwanlabå¯è§†åŒ–

## å¼•è¨€

> çœ‹å®Œæœ¬æ•™ç¨‹ä½ å°†æ”¶è·ï¼š
> 
> - åˆ©ç”¨ms-swiftè¿›è¡ŒLoraå¾®è°ƒgpt-oss-20bï¼
> 	
> - åˆ©ç”¨HuggingFaceæä¾›çš„trlè¿›è¡ŒLoraå¾®è°ƒgpt-oss-20bï¼
> 	

## ms-swiftå¾®è°ƒ

> è¿™é‡Œç»™å¤§å®¶æä¾›ä¸€ç§æ¡†æ¶å¾®è°ƒæ•™ç¨‹ åŸºäº`ms-swift`
> å¾®è°ƒçš„æ¡†æ¶æœ‰å¾ˆå¤šï¼Œä¸è®ºæ˜¯é€‰æ‹©å“ªä¸€æ–¹éƒ½æ˜¯æ®Šé€”åŒå½’ï¼Œä¸ºä»€ä¹ˆé€‰æ‹©ms-swiftè§ï¼š

- ğŸ æ¨¡å‹ç±»å‹ï¼šæ”¯æŒ450+çº¯æ–‡æœ¬å¤§æ¨¡å‹ã€150+å¤šæ¨¡æ€å¤§æ¨¡å‹ä»¥åŠAll-to-Allå…¨æ¨¡æ€æ¨¡å‹ã€åºåˆ—åˆ†ç±»æ¨¡å‹ã€Embeddingæ¨¡å‹è®­ç»ƒåˆ°éƒ¨ç½²å…¨æµç¨‹ã€‚
æ•°æ®é›†ç±»å‹ï¼šå†…ç½®150+é¢„è®­ç»ƒã€å¾®è°ƒã€äººç±»å¯¹é½ã€å¤šæ¨¡æ€ç­‰å„ç§ç±»å‹çš„æ•°æ®é›†ï¼Œå¹¶æ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†ã€‚
ç¡¬ä»¶æ”¯æŒï¼šCPUã€RTXç³»åˆ—ã€T4/V100ã€A10/A100/H100ã€Ascend NPUã€MPSç­‰ã€‚
- ğŸŠ è½»é‡è®­ç»ƒï¼šæ”¯æŒäº†LoRAã€QLoRAã€DoRAã€LoRA+ã€ReFTã€RS-LoRAã€LLaMAProã€Adapterã€GaLoreã€Q-Galoreã€LISAã€UnSlothã€Liger-Kernelç­‰è½»é‡å¾®è°ƒæ–¹å¼ã€‚
åˆ†å¸ƒå¼è®­ç»ƒï¼šæ”¯æŒåˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼ˆDDPï¼‰ã€device\_mapç®€æ˜“æ¨¡å‹å¹¶è¡Œã€DeepSpeed ZeRO2 ZeRO3ã€FSDPç­‰åˆ†å¸ƒå¼è®­ç»ƒæŠ€æœ¯ã€‚
é‡åŒ–è®­ç»ƒï¼šæ”¯æŒå¯¹BNBã€AWQã€GPTQã€AQLMã€HQQã€EETQé‡åŒ–æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚
RLHFè®­ç»ƒï¼šæ”¯æŒçº¯æ–‡æœ¬å¤§æ¨¡å‹å’Œå¤šæ¨¡æ€å¤§æ¨¡å‹çš„DPOã€GRPOã€RMã€PPOã€KTOã€CPOã€SimPOã€ORPOç­‰äººç±»å¯¹é½è®­ç»ƒæ–¹æ³•ã€‚
- ğŸ“ å¤šæ¨¡æ€è®­ç»ƒï¼šæ”¯æŒå¯¹å›¾åƒã€è§†é¢‘å’Œè¯­éŸ³ä¸åŒæ¨¡æ€æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œæ”¯æŒVQAã€Captionã€OCRã€Groundingä»»åŠ¡çš„è®­ç»ƒã€‚
ç•Œé¢è®­ç»ƒï¼šä»¥ç•Œé¢çš„æ–¹å¼æä¾›è®­ç»ƒã€æ¨ç†ã€è¯„æµ‹ã€é‡åŒ–çš„èƒ½åŠ›ï¼Œå®Œæˆå¤§æ¨¡å‹çš„å…¨é“¾è·¯ã€‚
æ’ä»¶åŒ–ä¸æ‹“å±•ï¼šæ”¯æŒè‡ªå®šä¹‰æ¨¡å‹å’Œæ•°æ®é›†æ‹“å±•ï¼Œæ”¯æŒå¯¹lossã€metricã€trainerã€loss-scaleã€callbackã€optimizerç­‰ç»„ä»¶è¿›è¡Œè‡ªå®šä¹‰ã€‚
- ğŸ‰ å·¥å…·ç®±èƒ½åŠ›ï¼šä¸ä»…æä¾›å¤§æ¨¡å‹å’Œå¤šæ¨¡æ€å¤§æ¨¡å‹çš„è®­ç»ƒæ”¯æŒï¼Œè¿˜æ¶µç›–å…¶æ¨ç†ã€è¯„æµ‹ã€é‡åŒ–å’Œéƒ¨ç½²å…¨æµç¨‹ã€‚
æ¨ç†åŠ é€Ÿï¼šæ”¯æŒPyTorchã€vLLMã€LmDeployæ¨ç†åŠ é€Ÿå¼•æ“ï¼Œå¹¶æä¾›OpenAIæ¥å£ï¼Œä¸ºæ¨ç†ã€éƒ¨ç½²å’Œè¯„æµ‹æ¨¡å—æä¾›åŠ é€Ÿã€‚
æ¨¡å‹è¯„æµ‹ï¼šä»¥EvalScopeä½œä¸ºè¯„æµ‹åç«¯ï¼Œæ”¯æŒ100+è¯„æµ‹æ•°æ®é›†å¯¹çº¯æ–‡æœ¬å’Œå¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œè¯„æµ‹ã€‚
æ¨¡å‹é‡åŒ–ï¼šæ”¯æŒAWQã€GPTQå’ŒBNBçš„é‡åŒ–å¯¼å‡ºï¼Œå¯¼å‡ºçš„æ¨¡å‹æ”¯æŒä½¿ç”¨vLLM/LmDeployæ¨ç†åŠ é€Ÿï¼Œå¹¶æ”¯æŒç»§ç»­è®­ç»ƒã€‚

### ç¯å¢ƒé…ç½®

1. åŸºç¡€ç¯å¢ƒé…ç½®
	

> PyTorch 2.6.0
> Python 3.12(ubuntu22.04)
> CUDA 12.4
> GPU NVIDIA H20-96GB \* 4

2. Loraç¯å¢ƒé…ç½®
	

```Bash
pip install ms-swift==3.7.0
pip install deepspeed
pip install swanlab
pip install -U transformers kernels torch
```

### æ•°æ®å‡†å¤‡

> æ„å»ºæ•°æ®é›†ï¼Œç¤ºä¾‹æ•°æ®å¦‚ä¸‹
> å‚è€ƒ[è‡ªå®šä¹‰æ•°æ®é›† â€” swift 3.8.0.dev0 æ–‡æ¡£](https://swift.readthedocs.io/zh-cn/latest/Customization/%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86.html)è·å¾—æ›´å¤šå®šä¹‰æ–¹å¼
> è¿™é‡Œä½¿ç”¨æœ€ç®€å•ï¼Œä¸å®˜æ–¹ç»“æ„ä¸€è‡´çš„æ–¹å¼
> æˆ‘è¿™é‡Œæ˜¯ç¿»è¯‘ä»»åŠ¡ï¼Œæ‰€ä»¥æˆ‘çš„ç¤ºä¾‹æ•°æ®å¦‚ä¸‹ï¼š

```Bash
  {
    "messages": [
      {
        "role": "user",
        "content": "Source Textï¼šThere, they found the body of Saroja Balasubramanian, 53, covered with blood-stained blankets. Style Guideï¼šThe translation style must be style: wikinews/Crime and Law"
      },
      {
        "role": "assistant",
        "content": "<think>\nxxx</think>\n\nåœ¨é‚£é‡Œï¼Œä»–ä»¬å‘ç°äº† 53 å²çš„è¨ç½—è´¾Â·å·´æ‹‰è‹å¸ƒæ‹‰æ›¼å°¼äºšçš„å°¸ä½“ï¼Œç›–ç€è¡€è¿¹æ–‘æ–‘çš„æ¯¯å­ã€‚"
      }
    ]
  },
```

> æˆ–è€…ä½ å¯ä»¥ä½¿ç”¨å¼€æºçš„ä»»æ„æ•°æ®é›†
> è¿™é‡Œï¼Œå…¶ä»–åŒå­¦æ‰¾äº†ä¸€ä¸ªé­”æ­ä¸Šå¼€æºçš„èµ›åšçŒ«å¨˜æ•°æ®é›†æ¥è¿›è¡Œè¿™æ¬¡æ•™ç¨‹ï¼Œè¯•æƒ³å“ªä¸€ä¸ªä½¬ä¸æƒ³æ‹¥æœ‰ä¸€ä¸ªèµ›åšçŒ«å¨˜å‘¢ï¼Ÿ

æ•°æ®é›†ä¼ é€é—¨ï¼š[æ²é›ªçŒ«å¨˜åŒ–æ•°æ®é›†](https://modelscope.cn/datasets/himzhzx/muice-dataset-train.catgirl/files)

```JSON
{
  "instruction": "æ²é›ªçš„åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ",
  "input": "",
  "output": "å–µ~æœ¬é›ªçš„ä¸»è¦åŠŸèƒ½æ˜¯è®©ä½ å¼€å¿ƒå–µï¼ç”¨å¯çˆ±çš„çŒ«å¨˜ä¹‹åŠ›æ²»æ„ˆä½ çš„å¿ƒçµï¼Œå–µå‘œ~"
  "history":[]
}
```

### Loraå¾®è°ƒ

> ç¼–å†™bashè„šæœ¬

```Bash
MASTER_PORT=$PORT \                             # åˆ†å¸ƒå¼è®­ç»ƒä¸»è¿›ç¨‹çš„é€šä¿¡ç«¯å£ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡ $PORT
NPROC_PER_NODE=4 \                              # æ¯ä¸ªèŠ‚ç‚¹çš„è¿›ç¨‹æ•°ï¼ˆé€šå¸¸ç­‰äº GPU æ•°ï¼‰
CUDA_VISIBLE_DEVICES=0,1,2,3 \                  # æŒ‡å®šä½¿ç”¨çš„ GPU ç¼–å·
swift sft --deepspeed zero3\                    # ä½¿ç”¨ swift çš„ sft è®­ç»ƒå‘½ä»¤ï¼Œå¹¶å¯ç”¨ DeepSpeed ZeRO-3 ä¼˜åŒ–
    --model /root/autodl-tmp/gpt-oss-20b \      # æ¨¡å‹è·¯å¾„ï¼ˆæ›¿æ¢ä¸ºä½ è‡ªå·±çš„æ¨¡å‹ç›®å½•ï¼‰
    --dataset /root/autodl-tmp/train.json \     # æ•°æ®é›†è·¯å¾„ï¼ˆæ›¿æ¢ä¸ºä½ è‡ªå·±çš„è®­ç»ƒæ•°æ®ï¼‰
    --train_type lora \                         # è®­ç»ƒç±»å‹ä¸º LoRAï¼ˆä½ç§©é€‚é…ï¼‰
    --torch_dtype bfloat16 \                    # è®¡ç®—ç²¾åº¦è®¾ä¸º bfloat16
    --num_train_epochs 35 \                     # è®­ç»ƒæ€»è½®æ•°
    --per_device_train_batch_size 1 \           # æ¯ä¸ªè®¾å¤‡çš„è®­ç»ƒæ‰¹å¤§å°
    --per_device_eval_batch_size 1 \            # æ¯ä¸ªè®¾å¤‡çš„éªŒè¯æ‰¹å¤§å°
    --learning_rate 1e-4 \                      # å­¦ä¹ ç‡
    --lora_rank 8 \                             # LoRA çš„ç§©ï¼ˆä½ç§©åˆ†è§£ç»´åº¦ï¼‰
    --lora_alpha 32 \                           # LoRA ç¼©æ”¾å› å­
    --target_modules all-linear \               # åº”ç”¨ LoRA çš„ç›®æ ‡æ¨¡å—ç±»å‹
    --gradient_accumulation_steps 16 \          # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    --eval_steps 50 \                           # æ¯ 50 æ­¥è¿›è¡Œä¸€æ¬¡è¯„ä¼°
    --save_steps 50 \                           # æ¯ 50 æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹
    --save_total_limit 2 \                      # æœ€å¤šä¿ç•™ 2 ä¸ªæœ€æ–°çš„æ£€æŸ¥ç‚¹
    --logging_steps 5 \                         # æ¯ 5 æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
    --max_length 8192 \                         # æœ€å¤§åºåˆ—é•¿åº¦
    --output_dir output \                       # æ¨¡å‹è¾“å‡ºç›®å½•
    --warmup_ratio 0.05 \                       # å­¦ä¹ ç‡é¢„çƒ­æ¯”ä¾‹
    --dataloader_num_workers 4 \                # DataLoader å·¥ä½œçº¿ç¨‹æ•°
    --use_liger_kernel true \                   # å¯ç”¨ liger kernel ä¼˜åŒ–
    --load_from_cache_file false \              # æ˜¯å¦ä»ç¼“å­˜æ–‡ä»¶åŠ è½½æ•°æ®
    --loss_scale ignore_empty_think \           # å¿½ç•¥ç©º think æ ‡ç­¾çš„ loss
    --save_strategy epoch\                      # ä¿å­˜ç­–ç•¥ï¼šæ¯ä¸ª epoch ä¿å­˜ä¸€æ¬¡
    --model_author gxb \                        # æ¨¡å‹ä½œè€…å
    --model_name gxb-gpt-oss-20b-agent-distill \# æ¨¡å‹åç§°
    --report_to swanlab \                       # è®­ç»ƒæ—¥å¿—ä¸ŠæŠ¥åˆ° SwanLab
    --swanlab_project swift-robot               # SwanLab é¡¹ç›®åç§°
```

> è®­ç»ƒè¿‡ç¨‹é€šè¿‡swanlabå¯è§†åŒ–

![](./images/4-0.png)

### Swanlab

![](./images/4-1.png)

> ++[SwanLab](https://github.com/swanhubx/swanlab)++ æ˜¯ä¸€ä¸ªå¼€æºçš„æ¨¡å‹è®­ç»ƒè®°å½•å·¥å…·ï¼Œé¢å‘ AI ç ”ç©¶è€…ï¼Œæä¾›äº†è®­ç»ƒå¯è§†åŒ–ã€è‡ªåŠ¨æ—¥å¿—è®°å½•ã€è¶…å‚æ•°è®°å½•ã€å®éªŒå¯¹æ¯”ã€å¤šäººååŒç­‰åŠŸèƒ½ã€‚åœ¨ `SwanLab` ä¸Šï¼Œç ”ç©¶è€…èƒ½åŸºäºç›´è§‚çš„å¯è§†åŒ–å›¾è¡¨å‘ç°è®­ç»ƒé—®é¢˜ï¼Œå¯¹æ¯”å¤šä¸ªå®éªŒæ‰¾åˆ°ç ”ç©¶çµæ„Ÿï¼Œå¹¶é€šè¿‡åœ¨çº¿é“¾æ¥çš„åˆ†äº«ä¸åŸºäºç»„ç»‡çš„å¤šäººååŒè®­ç»ƒï¼Œæ‰“ç ´å›¢é˜Ÿæ²Ÿé€šçš„å£å’ã€‚

#### ä¸ºä»€ä¹ˆè¦è®°å½•è®­ç»ƒï¼Ÿ

ç›¸è¾ƒäºè½¯ä»¶å¼€å‘ï¼Œæ¨¡å‹è®­ç»ƒæ›´åƒä¸€ä¸ªå®éªŒç§‘å­¦ã€‚ä¸€ä¸ªå“è´¨ä¼˜ç§€çš„æ¨¡å‹èƒŒåï¼Œå¾€å¾€æ˜¯æˆåƒä¸Šä¸‡æ¬¡å®éªŒã€‚ç ”ç©¶è€…éœ€è¦ä¸æ–­å°è¯•ã€è®°å½•ã€å¯¹æ¯”ï¼Œç§¯ç´¯ç»éªŒï¼Œæ‰èƒ½æ‰¾åˆ°æœ€ä½³çš„æ¨¡å‹ç»“æ„ã€è¶…å‚æ•°ä¸æ•°æ®é…æ¯”ã€‚åœ¨è¿™ä¹‹ä¸­ï¼Œå¦‚ä½•é«˜æ•ˆè¿›è¡Œè®°å½•ä¸å¯¹æ¯”ï¼Œå¯¹äºç ”ç©¶æ•ˆç‡çš„æå‡è‡³å…³é‡è¦ã€‚

#### åœ¨å“ªé‡Œç”¨ï¼Ÿ

å»ºè®®å…ˆåœ¨ ++[SwanLab å®˜ç½‘](https://swanlab.cn/)++ æ³¨å†Œè´¦å·ï¼Œç„¶ååœ¨SFTå’ŒGRPOè®­ç»ƒåˆå§‹åŒ–é˜¶æ®µé€‰æ‹©
`--report_to swanlab \ # è®­ç»ƒæ—¥å¿—ä¸ŠæŠ¥åˆ° SwanLab`

### æµ‹è¯•æ•ˆæœ

```Bash
CUDA_VISIBLE_DEVICES=0 \
swift infer \
    --adapters v0-20250811-150539/checkpoint-551 \
    --stream true \
    --temperature 0 \
    --max_new_tokens 2048
```

![](./images/4-2.png)

### åˆå¹¶æƒé‡

```Bash
swift export \
    --adapters v0-20250811-150539/checkpoint-551 \
    --merge_lora true
```

![](./images/4-3.png)

### æ¨ç†

> ç¼–å†™æ¨ç†è„šæœ¬

```Bash
CUDA_VISIBLE_DEVICES=0 \
swift infer \
    --model v0-20250811-150539/checkpoint-551 \
    --val_dataset SFT/dataForSFT/val.json \
    --max_new_tokens 2048 \
    --result_path SFT/infer_output/_sft_1epoch.jsonl
```

![](./images/4-4.png)

## Huggingfaceæä¾›çš„åŸºäºTRLå¾®è°ƒ

> openaiå®˜æ–¹æä¾›å¿«é€Ÿçš„å¾®è°ƒæ–¹å¼ï¼Œå®˜æ–¹è„šæœ¬å·²ç»å¾ˆè¯¦ç»†äº†ï¼Œè¿™é‡Œä¸å¤šèµ˜è¿°

### æ•°æ®å‡†å¤‡

> ä¸‹è½½å¤šè¯­è¨€æ¨ç†æ•°æ®é›†

```Python
from datasets import load_dataset
dataset = load_dataset("HuggingFaceH4/Multilingual-Thinking", split="train")
```

### æ•°æ®å¯è§†åŒ–

```Python
User:
    Â¿CuÃ¡l es el capital de Australia?
Assistant reasoning:
    Okay, der Benutzer fragt nach der Hauptstadt Australiens. Ich erinnere mich, dass Canberra die Hauptstadt ist. Ich
    sollte das bestÃ¤tigen. Lass mich sehen, ob es irgendwelche potenziellen Verwirrungen gibt. Der Benutzer kÃ¶nnte auch
    an der grÃ¶ÃŸten Stadt interessiert sein. Die grÃ¶ÃŸte Stadt ist Sydney, aber die Hauptstadt ist Canberra. Ich sollte
    das klarstellen. Vielleicht auch erwÃ¤hnen, dass Canberra eine geplante Stadt ist und nicht die grÃ¶ÃŸte. Der Benutzer
    kÃ¶nnte auch nach der Geografie fragen. Vielleicht erwÃ¤hne ich, dass Canberra im sÃ¼dwestlichen Teil der Australian
    Capital Territory liegt. Ich sollte die Antwort prÃ¤zise und freundlich halten. Vielleicht auch erwÃ¤hnen, dass
    Canberra oft mit Sydney verwechselt wird. Ich sollte sicherstellen, dass die Antwort klar und korrekt ist.
Assistant response:
    La capital de Australia es **Canberra**. Aunque es la ciudad mÃ¡s pequeÃ±a de las principales capitales del paÃ­s, fue
    elegida en 1908 como la sede del gobierno federal para equilibrar la influencia entre las ciudades de Sydney y
    Melbourne. Canberra estÃ¡ ubicada en el Territorio de la Capital Australiana (ACT), en el este de Australia.
```

### ç¯å¢ƒé…ç½®

```Python
pip install torch --index-url https://download.pytorch.org/whl/cu128
pip install "trl>=0.20.0" "peft>=0.17.0" "transformers>=4.55.0" trackio
```

### å¾®è°ƒ

> åŠ è½½tokenizer

```Python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("openai/gpt-oss-20b") #æ›¿æ¢æˆä½ ä¸‹è½½çš„ç›®å½•å“¦ï½
```

> ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨tokenizerçš„ `apply_chat_templateï¼ˆï¼‰` æ–¹æ³•æ¥æ ¼å¼åŒ–æ¶ˆæ¯ï¼š

```Python
messages = dataset[0]["messages"]
conversation = tokenizer.apply_chat_template(messages, tokenize=False)
print(conversation)
```

> ä¸ºäº†å‡†å¤‡æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œè®©æˆ‘ä»¬é¦–å…ˆä» ++[Hugging Face Hub](https://huggingface.co/)++ ä¸‹è½½æƒé‡ã€‚
> æˆ‘ä»¬å°†ä½¿ç”¨ Transformers ä¸­çš„ ğŸ¤— `AutoModelForCausalLM` ç±»æ¥åŠ è½½æ¨¡å‹

```Python
import torch
from transformers import AutoModelForCausalLM, Mxfp4Config

quantization_config = Mxfp4Config(dequantize=True)
model_kwargs = dict(
    attn_implementation="eager",
    torch_dtype=torch.bfloat16,
    quantization_config=quantization_config,
    use_cache=False,
    device_map="auto",
)

model = AutoModelForCausalLM.from_pretrained("openai/gpt-oss-20b", **model_kwargs)
```

è¿™å°†ä¸ºæ¨¡å‹åŠ è½½è®­ç»ƒæ‰€éœ€çš„é…ç½®ã€‚`attn_implementation` è®¾ç½®ä¸ºæ¸´æœ›æ›´å¥½çš„æ€§èƒ½ï¼Œ`use_cache` è®¾ç½®ä¸º `False`ï¼Œå› ä¸ºæˆ‘ä»¬å°†ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹å¾®è°ƒæ¨¡å‹ã€‚
å¦‚æœæ‚¨ç†Ÿæ‚‰ Transformersï¼Œæ‚¨å¯èƒ½ä¼šæ³¨æ„åˆ°æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨ `Mxfp4Config` è¿›è¡Œé‡åŒ–ã€‚è¿™æ˜¯ OpenAI æ¨¡å‹çš„ç‰¹å®šé…ç½®ï¼Œå…è®¸æˆ‘ä»¬ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼Œå…¶ä¸­åŒ…å«ä¸€ç§ç§°ä¸º ++[MXFP4](https://en.wikipedia.org/wiki/Block_floating_point)++ çš„ç‰¹æ®Š 4 ä½æµ®ç‚¹æ ¼å¼ï¼Œè¯¥æ ¼å¼é’ˆå¯¹ AI å·¥ä½œè´Ÿè½½è¿›è¡Œäº†ä¼˜åŒ–ã€‚

> æµ‹è¯•ä¸€æ¡message

```Python
messages = [
    {"role": "user", "content": "Â¿CuÃ¡l es el capital de Australia?"},
]

input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt",
).to(model.device)

output_ids = model.generate(input_ids, max_new_tokens=512)
response = tokenizer.batch_decode(output_ids)[0]
print(response)
```

> é…ç½® LoRA å‚æ•°

```Python
from peft import LoraConfig, get_peft_model

peft_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules="all-linear",
    target_parameters=[
        "7.mlp.experts.gate_up_proj",
        "7.mlp.experts.down_proj",
        "15.mlp.experts.gate_up_proj",
        "15.mlp.experts.down_proj",
        "23.mlp.experts.gate_up_proj",
        "23.mlp.experts.down_proj",
    ],
)
peft_model = get_peft_model(model, peft_config)
peft_model.print_trainable_parameters()
```

æ³¨æ„ï¼š`openai/gpt-oss-20b` æ¨¡å‹æ˜¯ä¸€ç§++[æ··åˆä¸“å®¶ ï¼ˆMoEï¼‰](https://huggingface.co/blog/moe)++ æ¶æ„ã€‚é™¤äº†é’ˆå¯¹æ³¨æ„åŠ›å±‚ï¼ˆ`target_modules=â€œall-linearâ€ï¼‰` ä¹‹å¤–ï¼Œåœ¨ä¸“å®¶æ¨¡å—ä¸­åŒ…å«æŠ•å½±å±‚ä¹Ÿå¾ˆé‡è¦ã€‚PEFT é€šè¿‡ `target_parameters` å‚æ•°ä¿ƒè¿›äº†è¿™ä¸€ç‚¹ï¼Œå®ƒå…è®¸æ‚¨æŒ‡å®šç‰¹å®šäºä¸“å®¶çš„å±‚ï¼Œä¾‹å¦‚ `mlp.experts.down_proj` å’Œ `mlp.experts.gate_up_proj`ã€‚åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹è¿™äº›æŠ•å½±å±‚çš„å­é›†ï¼Œä½†å»ºè®®æ‚¨å°è¯•ä¸åŒçš„é…ç½®ã€‚

> å¾®è°ƒå‚æ•°è®¾ç½®

```Python
from trl import SFTConfig

training_args = SFTConfig(
    # ===== è®­ç»ƒè¶…å‚æ•° =====
    learning_rate=2e-4,                           # å­¦ä¹ ç‡
    gradient_checkpointing=True,                  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
    num_train_epochs=1.0,                          # è®­ç»ƒè½®æ•°
    logging_steps=1,                               # æ—¥å¿—è®°å½•æ­¥æ•°
    per_device_train_batch_size=4,                 # æ¯è®¾å¤‡è®­ç»ƒ batch å¤§å°
    gradient_accumulation_steps=4,                 # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    use_peft=True,                                 # å¯ç”¨ PEFTï¼ˆå‚æ•°é«˜æ•ˆå¾®è°ƒï¼‰
    lora_r=8,                                      # LoRA ç§©
    lora_alpha=16,                                 # LoRA alpha å€¼
    lora_dropout=0.0,                              # LoRA dropout
    lora_target_modules="all-linear",              # LoRA ä½œç”¨æ¨¡å—
    max_length=4096,                               # æœ€å¤§åºåˆ—é•¿åº¦
    warmup_ratio=0.03,                             # é¢„çƒ­æ¯”ä¾‹
    lr_scheduler_type="cosine_with_min_lr",        # å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹
    lr_scheduler_kwargs={"min_lr_rate": 0.1},      # è°ƒåº¦å™¨é¢å¤–å‚æ•°

    # ===== è¾“å‡ºä¸æ—¥å¿— =====
    output_dir="gpt-oss-20b-multilingual-reasoner", # è¾“å‡ºç›®å½•
    report_to=["swanlab"],                         # æ—¥å¿—ä¸ŠæŠ¥ç›®æ ‡
    push_to_hub=False                               # æ˜¯å¦æ¨é€åˆ° HuggingFace Hub
)
```

> å¼€å§‹å¾®è°ƒå§ï¼

```Python
from trl import SFTTrainer
trainer = SFTTrainer(    
    model=peft_model,    
    args=training_args,    
    train_dataset=dataset,    
    processing_class=tokenizer,
    )
trainer.train()
```

### æ¨ç†

```Python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained("openai/gpt-oss-20b")

# Load the original model first
model_kwargs = dict(attn_implementation="eager", torch_dtype="auto", use_cache=True, device_map="auto")
base_model = AutoModelForCausalLM.from_pretrained("openai/gpt-oss-20b", **model_kwargs).cuda()

# Merge fine-tuned weights with the base model
peft_model_id = "gpt-oss-20b-multilingual-reasoner"
model = PeftModel.from_pretrained(base_model, peft_model_id)
model = model.merge_and_unload()
```

```Python
REASONING_LANGUAGE = "German"
SYSTEM_PROMPT = f"reasoning language: {REASONING_LANGUAGE}"
USER_PROMPT = "Â¿CuÃ¡l es el capital de Australia?"  # Spanish for "What is the capital of Australia?"

messages = [
    {"role": "system", "content": SYSTEM_PROMPT},
    {"role": "user", "content": USER_PROMPT},
]

input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt",
).to(model.device)

gen_kwargs = {"max_new_tokens": 512, "do_sample": True, "temperature": 0.6, "top_p": None, "top_k": None}

output_ids = model.generate(input_ids, **gen_kwargs)
response = tokenizer.batch_decode(output_ids)[0]
print(response)
```

```Python
REASONING_LANGUAGE = "Chinese" 
SYSTEM_PROMPT = f"reasoning language: {REASONING_LANGUAGE}"
USER_PROMPT = "What is the national symbol of Canada?"

messages = [
    {"role": "system", "content": SYSTEM_PROMPT},
    {"role": "user", "content": USER_PROMPT},
]

input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt",
).to(model.device)

output_ids = model.generate(input_ids, **gen_kwargs)
response = tokenizer.batch_decode(output_ids)[0]
print(response)
```

![](./images/4-5.png)
