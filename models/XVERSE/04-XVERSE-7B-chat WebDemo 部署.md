# XVERSE-7B-chat WebDemo éƒ¨ç½²

XVERSE-7B-Chatä¸º[XVERSE-7B](https://huggingface.co/xverse/XVERSE-7B)æ¨¡å‹å¯¹é½åçš„ç‰ˆæœ¬ã€‚

XVERSE-7B æ˜¯ç”±æ·±åœ³å…ƒè±¡ç§‘æŠ€è‡ªä¸»ç ”å‘çš„æ”¯æŒå¤šè¯­è¨€çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelï¼‰ï¼Œå‚æ•°è§„æ¨¡ä¸º 70 äº¿ï¼Œä¸»è¦ç‰¹ç‚¹å¦‚ä¸‹ï¼š

- æ¨¡å‹ç»“æ„ï¼šXVERSE-7B ä½¿ç”¨ä¸»æµ Decoder-only çš„æ ‡å‡† Transformer ç½‘ç»œç»“æ„ï¼Œæ”¯æŒ 8K çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆContext Lengthï¼‰ï¼Œèƒ½æ»¡è¶³æ›´é•¿çš„å¤šè½®å¯¹è¯ã€çŸ¥è¯†é—®ç­”ä¸æ‘˜è¦ç­‰éœ€æ±‚ï¼Œæ¨¡å‹åº”ç”¨åœºæ™¯æ›´å¹¿æ³›ã€‚
- è®­ç»ƒæ•°æ®ï¼šæ„å»ºäº† 2.6 ä¸‡äº¿ token çš„é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œå……åˆ†è®­ç»ƒï¼ŒåŒ…å«ä¸­ã€è‹±ã€ä¿„ã€è¥¿ç­‰ 40 å¤šç§è¯­è¨€ï¼Œé€šè¿‡ç²¾ç»†åŒ–è®¾ç½®ä¸åŒç±»å‹æ•°æ®çš„é‡‡æ ·æ¯”ä¾‹ï¼Œä½¿å¾—ä¸­è‹±ä¸¤ç§è¯­è¨€è¡¨ç°ä¼˜å¼‚ï¼Œä¹Ÿèƒ½å…¼é¡¾å…¶ä»–è¯­è¨€æ•ˆæœã€‚
- åˆ†è¯ï¼šåŸºäº BPEï¼ˆByte-Pair Encodingï¼‰ç®—æ³•ï¼Œä½¿ç”¨ä¸Šç™¾ GB è¯­æ–™è®­ç»ƒäº†ä¸€ä¸ªè¯è¡¨å¤§å°ä¸º 100,534 çš„åˆ†è¯å™¨ï¼Œèƒ½å¤ŸåŒæ—¶æ”¯æŒå¤šè¯­è¨€ï¼Œè€Œæ— éœ€é¢å¤–æ‰©å±•è¯è¡¨ã€‚
- è®­ç»ƒæ¡†æ¶ï¼šè‡ªä¸»ç ”å‘å¤šé¡¹å…³é”®æŠ€æœ¯ï¼ŒåŒ…æ‹¬é«˜æ•ˆç®—å­ã€æ˜¾å­˜ä¼˜åŒ–ã€å¹¶è¡Œè°ƒåº¦ç­–ç•¥ã€æ•°æ®-è®¡ç®—-é€šä¿¡é‡å ã€å¹³å°å’Œæ¡†æ¶ååŒç­‰ï¼Œè®©è®­ç»ƒæ•ˆç‡æ›´é«˜ï¼Œæ¨¡å‹ç¨³å®šæ€§å¼ºï¼Œåœ¨åƒå¡é›†ç¾¤ä¸Šçš„å³°å€¼ç®—åŠ›åˆ©ç”¨ç‡å¯è¾¾åˆ° 58.5%ï¼Œä½å±…ä¸šç•Œå‰åˆ—ã€‚

## ç¯å¢ƒå‡†å¤‡  

åœ¨ Autodl å¹³å°ä¸­ç§Ÿèµä¸€ä¸ª 3090 ç­‰ 24G æ˜¾å­˜çš„æ˜¾å¡æœºå™¨ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºé•œåƒé€‰æ‹© PyTorch-->2.1.0-->3.10(ubuntu22.04)-->12.1ï¼ˆ11.3 ç‰ˆæœ¬ä»¥ä¸Šçš„éƒ½å¯ä»¥ï¼‰ã€‚

![3-1](images/1.png)

pip æ¢æºåŠ é€Ÿä¸‹è½½å¹¶å®‰è£…ä¾èµ–åŒ…ï¼Œä¸ºäº†æ–¹ä¾¿å¤§å®¶è¿›è¡Œç¯å¢ƒé…ç½®ï¼Œåœ¨ code æ–‡ä»¶å¤¹é‡Œé¢ç»™å¤§å®¶æä¾›äº† requirement.txt æ–‡ä»¶ï¼Œå¤§å®¶ç›´æ¥ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤å®‰è£…å³å¯ã€‚å¦‚æœä½ ä½¿ç”¨çš„æ˜¯ [autodl](https://www.autodl.com/) éƒ¨ç½²æ¨¡å‹çš„è¯ï¼Œæˆ‘ä»¬æœ‰åˆ¶ä½œå¥½çš„é•œåƒä¾›å¤§å®¶ä½¿ç”¨ï¼š[XVERSE-7B-Chat](https://www.codewithgpu.com/i/datawhalechina/self-llm/XVERSE-7B-Chat)


```bash
# å‡çº§pip
python -m pip install --upgrade pip
# æ›´æ¢ pypi æºåŠ é€Ÿåº“çš„å®‰è£…
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

# å®‰è£…pythonä¾èµ–
pip install -r requirement.txt
```

## æ¨¡å‹ä¸‹è½½

XVERSE-7B-Chat æ¨¡å‹ï¼š

* [huggingface](https://huggingface.co/xverse/XVERSE-7B-Chat)
* [modelscope](https://www.modelscope.cn/models/xverse/XVERSE-7B-Chat/summary)

### ä½¿ç”¨modelscopeä¸‹è½½

ä½¿ç”¨ modelscope ä¸­çš„ snapshot_download å‡½æ•°ä¸‹è½½æ¨¡å‹ï¼Œç¬¬ä¸€ä¸ªå‚æ•°ä¸ºæ¨¡å‹åç§°ï¼Œå‚æ•° cache_dir ä¸ºæ¨¡å‹çš„ä¸‹è½½è·¯å¾„ï¼Œæ¨¡å‹è·¯å¾„ä¸º`/root/autodl-tmp`ã€‚åœ¨ /root/autodl-tmp ä¸‹åˆ›å»ºmodel_download.pyæ–‡ä»¶å†…å®¹å¦‚ä¸‹: 

```python
from modelscope import snapshot_download
model_dir = snapshot_download("xverse/XVERSE-7B-Chat", cache_dir='/root/autodl-tmp', revision="master")
```

## ä»£ç å‡†å¤‡

> ä¸ºäº†æ–¹ä¾¿å¤§å®¶éƒ¨ç½²ï¼Œåœ¨ code æ–‡ä»¶å¤¹é‡Œé¢å·²ç»å‡†å¤‡å¥½äº†ä»£ç ï¼Œå¤§å®¶å¯ä»¥å°†ä»“åº“ clone åˆ°æœåŠ¡å™¨ä¸Šç›´æ¥è¿è¡Œã€‚

åœ¨`/root/autodl-tmp`è·¯å¾„ä¸‹æ–°å»º `chatBot.py` æ–‡ä»¶å¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼š
```python
import argparse
import torch
import gradio as gr
import json
from datetime import datetime
from transformers import AutoModelForCausalLM, AutoTokenizer,GenerationConfig

tokenizer, model = None, None

def init_model(args):
    global tokenizer, model
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path, truncation_side="left", padding_side="left")
    model = AutoModelForCausalLM.from_pretrained(args.model_path, trust_remote_code=True, torch_dtype=torch.bfloat16,
                                                 low_cpu_mem_usage=True, device_map='auto')
    model.generation_config = GenerationConfig.from_pretrained(args.model_path)
    model = model.eval()

def chat(message, history, request: gr.Request):
    global tokenizer, model
    history = history or []
    history.append({"role": "user", "content": message})

    # init
    history.append({"role": "assistant", "content": ""})
    utter_history = []
    for i in range(0, len(history), 2):
        utter_history.append([history[i]["content"], history[i+1]["content"]])

    # chat with stream
    for next_text in model.chat(tokenizer, history[:-1], stream=True):
        utter_history[-1][1] += next_text
        history[-1]["content"] += next_text
        if torch.backends.mps.is_available():
            torch.mps.empty_cache()
        yield utter_history, history

    # log
    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print(f'{current_time} request_ip:{request.client.host}\nquery: {message}\nhistory: {json.dumps(history, ensure_ascii=False)}\nanswer: {json.dumps(utter_history[-1][1], ensure_ascii=False)}')

# å¢åŠ é…ç½®ï¼Œæ·»åŠ æ¨¡å‹åœ°å€
def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--port", type=int, default=6006,
                       help="server port")
    parser.add_argument("--model_path", type=str, default="/root/autodl-tmp/xverse/XVERSE-7B-Chat",
                        help="model path")
    parser.add_argument("--tokenizer_path", type=str, default="/root/autodl-tmp/xverse/XVERSE-7B-Chat",
                        help="Path to the tokenizer.")
    args = parser.parse_args()
    return args

if __name__ == "__main__":
    args = get_args()
    # åˆå§‹åŒ–æ¨¡å‹
    init_model(args)

    # æ„å»ºdemoåº”ç”¨
    with gr.Blocks(theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
                        # <center>ğŸ’¬ XVERSE-7B-Chat</center>
                        ## <center>ğŸš€ A Gradio chatbot powered by Self-LLM</center>
                        ### <center>âœ¨ æ„Ÿå…´è¶£çš„å°ä¼™ä¼´å¯ä»¥å»çœ‹æˆ‘ä»¬çš„å¼€æºé¡¹ç›®å“¦â€”â€”[å¼€æºå¤§æ¨¡å‹é£Ÿç”¨æŒ‡å— self-llm](https://github.com/datawhalechina/self-llm.git)ï¼Œæ•™ä½ ä¸€æ¯å¥¶èŒ¶è·‘é€šæ‰€æœ‰ä¸»æµå¤§æ¨¡å‹ğŸ˜€ã€‚</center>
                    """)
        chatbot = gr.Chatbot(label="Chat history", height=500)
        state = gr.State([])

        with gr.Row():
            text_box = gr.Textbox(label="Message", show_label=False, placeholder="è¯·è¾“å…¥ä½ çš„æ¶ˆæ¯å¹¶å›è½¦")

        with gr.Row():
            submit_btn = gr.Button(value="Send", variant="secondary")
            reset_btn = gr.Button(value="Reset")

        text_box.submit(fn=chat,
                        inputs=[text_box, state],
                        outputs=[chatbot, state],
                        api_name="chat")
        submit_btn.click(fn=chat,
                         inputs=[text_box, state],
                         outputs=[chatbot, state])

        # ç”¨äºæ¸…ç©ºtext_box
        def clear_textbox():
            return gr.update(value="")
        text_box.submit(fn=clear_textbox, inputs=None, outputs=[text_box])
        submit_btn.click(fn=clear_textbox, inputs=None, outputs=[text_box])

        # ç”¨äºæ¸…ç©ºé¡µé¢å’Œé‡ç½®state
        def reset():
            return None, []
        reset_btn.click(fn=reset, inputs=None, outputs=[chatbot, state])

    demo.launch(server_name="0.0.0.0", server_port=args.port)
```

## è¿è¡Œ demo

åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œå¯åŠ¨gradioæœåŠ¡ï¼Œå¹¶æŒ‰ç…§ `autodl` çš„æŒ‡ç¤ºå°†ç«¯å£æ˜ å°„åˆ°æœ¬åœ°ï¼Œç„¶ååœ¨æµè§ˆå™¨ä¸­æ‰“å¼€é“¾æ¥ http://localhost:6006/ ï¼Œå³å¯çœ‹åˆ°èŠå¤©ç•Œé¢ã€‚

```bash
python chatBot.py
```
å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
![](images/5.png)
