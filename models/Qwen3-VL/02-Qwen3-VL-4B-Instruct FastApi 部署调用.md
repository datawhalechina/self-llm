# Qwen3-VL-4B-Instruct FastApi éƒ¨ç½²è°ƒç”¨

## ç¯å¢ƒå‡†å¤‡

åŸºç¡€ç¯å¢ƒï¼š

```
----------------
ubuntu 22.04
python 3.12
cuda 12.8
pytorch 2.8.0
----------------
```
> æœ¬æ–‡é»˜è®¤å­¦ä¹ è€…å·²å®‰è£…å¥½ä»¥ä¸Š PyTorch (cuda) ç¯å¢ƒï¼Œå¦‚æœªå®‰è£…è¯·è‡ªè¡Œå®‰è£…ã€‚

### æ˜¾å¡é…ç½®è¯´æ˜

æœ¬æ•™ç¨‹åŸºäº**RTX 4090**æ˜¾å¡è¿›è¡Œéƒ¨ç½²ï¼Œè¯¥æ˜¾å¡å…·æœ‰24GBæ˜¾å­˜ï¼Œå®Œå…¨æ»¡è¶³Qwen3-VL-4B-Instructæ¨¡å‹çš„è¿è¡Œéœ€æ±‚ã€‚

![alt text](./images/01-1.png)

### ç¯å¢ƒå®‰è£…
é¦–å…ˆ `pip` æ¢æºåŠ é€Ÿä¸‹è½½å¹¶å®‰è£…ä¾èµ–åŒ…

```shell
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

pip install modelscope==1.20.0
pip install fastapi==0.115.4
pip install uvicorn==0.32.0
pip install transformers>=4.51.0
pip install accelerate==1.1.1
pip install torchvision==0.19.0
pip install av==13.1.0
pip install qwen-vl-utils
```

![alt text](./images/01-2.png)

## æ¨¡å‹ä¸‹è½½

ä½¿ç”¨ `modelscope` ä¸­çš„ `snapshot_download` å‡½æ•°ä¸‹è½½æ¨¡å‹ï¼Œç¬¬ä¸€ä¸ªå‚æ•°ä¸ºæ¨¡å‹åç§°ï¼Œå‚æ•° `cache_dir` ä¸ºæ¨¡å‹çš„ä¸‹è½½è·¯å¾„ã€‚

æ–°å»º `model_download.py` æ–‡ä»¶è¾“å…¥ä»¥ä¸‹ä»£ç ï¼Œå¹¶è¿è¡Œ `python model_download.py` æ‰§è¡Œä¸‹è½½ã€‚

```python
# model_download.py
from modelscope import snapshot_download
model_dir = snapshot_download('Qwen/Qwen3-VL-4B-Instruct', cache_dir='/root/autodl-fs', revision='master')
```

> æ³¨æ„ï¼šè¯·è®°å¾—ä¿®æ”¹ `cache_dir` ä¸ºä½ è‡ªå·±çš„æ¨¡å‹ä¸‹è½½è·¯å¾„ã€‚å»ºè®®ä½¿ç”¨ `/root/autodl-fs` ç›®å½•ï¼Œè¿™æ˜¯æŒä¹…åŒ–å­˜å‚¨ç›®å½•ï¼Œé‡å¯æœºå™¨åæ•°æ®ä¸ä¼šä¸¢å¤±ã€‚Qwen3-VL-4B-Instructæ¨¡å‹å®é™…å¤§å°çº¦ä¸º**9.2GB**ï¼ˆåŒ…å«æ‰€æœ‰é…ç½®æ–‡ä»¶å’Œæƒé‡æ–‡ä»¶ï¼‰ï¼Œä¸‹è½½æ—¶é—´æ ¹æ®ç½‘ç»œé€Ÿåº¦è€Œå®šã€‚

![alt text](./images/01-3.png)

## ä»£ç å‡†å¤‡

### APIæœåŠ¡ç«¯ä»£ç 

åˆ›å»ºAPIæœåŠ¡ç«¯æ–‡ä»¶ `api_server_qwen3vl_simple.py`ï¼Œè¯¥æ–‡ä»¶åŒ…å«äº†å®Œæ•´çš„FastAPIæœåŠ¡å®ç°ï¼Œæ”¯æŒæ–‡æœ¬å’Œå›¾åƒçš„å¤šæ¨¡æ€é—®ç­”åŠŸèƒ½ã€‚

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import torch
from transformers import AutoProcessor, Qwen3VLForConditionalGeneration
from qwen_vl_utils import process_vision_info
from fastapi import FastAPI
import uvicorn
from pydantic import BaseModel
from typing import List, Dict, Any, Optional

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
torch.set_num_threads(8)

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(title="Qwen3-VL-4B Simple API", version="1.0.0")

# æ¨¡å‹è·¯å¾„
model_name_or_path = '/root/autodl-fs/Qwen/Qwen3-VL-4B-Instruct'

# åˆå§‹åŒ–æ¨¡å‹å’Œå¤„ç†å™¨
model = Qwen3VLForConditionalGeneration.from_pretrained(
    model_name_or_path,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

processor = AutoProcessor.from_pretrained(
    model_name_or_path,
    trust_remote_code=True
)

# è¯·æ±‚æ¨¡å‹
class ChatRequest(BaseModel):
    messages: List[Dict[str, Any]]
    max_tokens: Optional[int] = 512
    temperature: Optional[float] = 0.7
    top_p: Optional[float] = 0.9

# å“åº”æ¨¡å‹
class ChatResponse(BaseModel):
    response: str
    model: str = "Qwen3-VL-4B-Instruct"
    usage: Dict[str, int]

@app.get("/")
async def root():
    return {"message": "Qwen3-VL-4B-Instruct API Server is running!"}

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "model": "Qwen3-VL-4B-Instruct",
        "device": str(model.device),
        "torch_version": torch.__version__,
        "cuda_available": torch.cuda.is_available(),
        "gpu_memory": f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB" if torch.cuda.is_available() else "N/A"
    }

@app.post("/v1/chat/completions", response_model=ChatResponse)
async def chat_completions(request: ChatRequest):
    try:
        # å¤„ç†æ¶ˆæ¯
        messages = request.messages
        
        # å¤„ç†è§†è§‰ä¿¡æ¯
        text = processor.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        image_inputs, video_inputs = process_vision_info(messages)
        
        # å‡†å¤‡è¾“å…¥
        inputs = processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        )
        inputs = inputs.to(model.device)
        
        # ç”Ÿæˆå“åº”
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
                do_sample=True,
                pad_token_id=processor.tokenizer.eos_token_id
            )
        
        # è§£ç å“åº”
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        
        response_text = processor.batch_decode(
            generated_ids_trimmed, 
            skip_special_tokens=True, 
            clean_up_tokenization_spaces=False
        )[0]
        
        # è®¡ç®—tokenä½¿ç”¨é‡
        input_tokens = inputs.input_ids.shape[1]
        output_tokens = len(generated_ids_trimmed[0])
        
        return ChatResponse(
            response=response_text,
            usage={
                "prompt_tokens": input_tokens,
                "completion_tokens": output_tokens,
                "total_tokens": input_tokens + output_tokens
            }
        )
        
    except Exception as e:
        return ChatResponse(
            response=f"Error: {str(e)}",
            usage={"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
        )

if __name__ == "__main__":
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=8000,
        log_level="info"
    )
```

> **é‡è¦æç¤º**ï¼šæ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ `model_name_or_path` å˜é‡ä¸­çš„æ¨¡å‹è·¯å¾„ã€‚

## å¯åŠ¨APIæœåŠ¡

åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨APIæœåŠ¡ï¼š

```shell
python api_server_qwen3vl_simple.py
```

å¯åŠ¨æˆåŠŸåï¼Œä½ å°†çœ‹åˆ°ç±»ä¼¼ä»¥ä¸‹çš„è¾“å‡ºï¼š

```
INFO:     Started server process [12345]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
```

![alt text](./images/01-4.png)

### æµ‹è¯•APIæœåŠ¡

### æµ‹è¯•å®¢æˆ·ç«¯ä»£ç 

åˆ›å»ºæµ‹è¯•è„šæœ¬ `test_simple_api.py`ï¼Œç”¨äºéªŒè¯å›¾åƒé—®ç­”APIæœåŠ¡çš„åŠŸèƒ½ã€‚

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
import json

# APIæœåŠ¡åœ°å€
API_BASE_URL = "http://localhost:8000"

def test_health_check():
    """æµ‹è¯•å¥åº·æ£€æŸ¥æ¥å£"""
    print("=== æµ‹è¯•å¥åº·æ£€æŸ¥æ¥å£ ===")
    try:
        response = requests.get(f"{API_BASE_URL}/health")
        if response.status_code == 200:
            result = response.json()
            print("âœ… å¥åº·æ£€æŸ¥é€šè¿‡")
            print(f"æ¨¡å‹: {result.get('model')}")
            print(f"è®¾å¤‡: {result.get('device')}")
            print(f"GPUå†…å­˜: {result.get('gpu_memory')}")
            return True
        else:
            print(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ å¥åº·æ£€æŸ¥å¼‚å¸¸: {e}")
        return False

def test_text_chat():
    """æµ‹è¯•çº¯æ–‡æœ¬å¯¹è¯"""
    print("\n=== æµ‹è¯•çº¯æ–‡æœ¬å¯¹è¯ ===")
    
    messages = [
        {
            "role": "user",
            "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
        }
    ]
    
    payload = {
        "messages": messages,
        "max_tokens": 256,
        "temperature": 0.7
    }
    
    try:
        response = requests.post(
            f"{API_BASE_URL}/v1/chat/completions",
            json=payload,
            headers={"Content-Type": "application/json"}
        )
        
        if response.status_code == 200:
            result = response.json()
            print("âœ… æ–‡æœ¬å¯¹è¯æµ‹è¯•æˆåŠŸ")
            print(f"å›å¤: {result['response']}")
            print(f"Tokenä½¿ç”¨: {result['usage']}")
            return True
        else:
            print(f"âŒ æ–‡æœ¬å¯¹è¯æµ‹è¯•å¤±è´¥: {response.status_code}")
            print(f"é”™è¯¯ä¿¡æ¯: {response.text}")
            return False
            
    except Exception as e:
        print(f"âŒ æ–‡æœ¬å¯¹è¯æµ‹è¯•å¼‚å¸¸: {e}")
        return False

def test_image_chat():
    """æµ‹è¯•å›¾åƒå¯¹è¯"""
    print("\n=== æµ‹è¯•å›¾åƒå¯¹è¯ ===")
    
    # ä½¿ç”¨åœ¨çº¿å›¾ç‰‡è¿›è¡Œæµ‹è¯•
    image_url = "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg"
    
    try:
        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "image": image_url
                    },
                    {
                        "type": "text",
                        "text": "è¯·æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚"
                    }
                ]
            }
        ]
        
        payload = {
            "messages": messages,
            "max_tokens": 512,
            "temperature": 0.7
        }
        
        response = requests.post(
            f"{API_BASE_URL}/v1/chat/completions",
            json=payload,
            headers={"Content-Type": "application/json"}
        )
        
        if response.status_code == 200:
            result = response.json()
            print("âœ… å›¾åƒå¯¹è¯æµ‹è¯•æˆåŠŸ")
            print(f"å›å¤: {result['response']}")
            print(f"Tokenä½¿ç”¨: {result['usage']}")
            return True
        else:
            print(f"âŒ å›¾åƒå¯¹è¯æµ‹è¯•å¤±è´¥: {response.status_code}")
            print(f"é”™è¯¯ä¿¡æ¯: {response.text}")
            return False
            
    except Exception as e:
        print(f"âŒ å›¾åƒå¯¹è¯æµ‹è¯•å¼‚å¸¸: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯• Qwen3-VL-4B-Instruct API æœåŠ¡")
    print("=" * 50)
    
    # æ‰§è¡Œæµ‹è¯•
    health_ok = test_health_check()
    text_ok = test_text_chat()
    image_ok = test_image_chat()
    
    # æ€»ç»“æµ‹è¯•ç»“æœ
    print("\n" + "=" * 50)
    print("æµ‹è¯•ç»“æœæ€»ç»“:")
    print(f"å¥åº·æ£€æŸ¥: {'âœ… é€šè¿‡' if health_ok else 'âŒ å¤±è´¥'}")
    print(f"æ–‡æœ¬å¯¹è¯: {'âœ… é€šè¿‡' if text_ok else 'âŒ å¤±è´¥'}")
    print(f"å›¾åƒå¯¹è¯: {'âœ… é€šè¿‡' if image_ok else 'âŒ å¤±è´¥'}")
    
    if health_ok and text_ok:
        print("\nğŸ‰ APIæœåŠ¡è¿è¡Œæ­£å¸¸ï¼")
    else:
        print("\nâš ï¸  éƒ¨åˆ†åŠŸèƒ½å­˜åœ¨é—®é¢˜ï¼Œè¯·æ£€æŸ¥æœåŠ¡çŠ¶æ€")

if __name__ == "__main__":
    main()
```

> **é‡è¦æç¤º**ï¼šè¯¥æµ‹è¯•è„šæœ¬ä½¿ç”¨åœ¨çº¿å›¾ç‰‡é“¾æ¥è¿›è¡Œæµ‹è¯•ï¼Œæ— éœ€æœ¬åœ°å›¾ç‰‡æ–‡ä»¶ï¼Œæ›´åŠ ä¾¿äºä½¿ç”¨ã€‚æµ‹è¯•å›¾ç‰‡æ¥æºï¼š`https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg`

æ‰§è¡Œåå¾—åˆ°çš„è¿”å›ç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š

![alt text](./images/01-5.png)


## å¸¸è§é—®é¢˜

### Q1: æ¨¡å‹åŠ è½½å¤±è´¥
**é—®é¢˜**: å‡ºç° "CUDA out of memory" é”™è¯¯
**è§£å†³æ–¹æ¡ˆ**: 
- ç¡®ä¿RTX 4090æœ‰è¶³å¤Ÿçš„æ˜¾å­˜ç©ºé—´
- å°è¯•ä½¿ç”¨é‡åŒ–é…ç½®å‡å°‘æ˜¾å­˜å ç”¨
- æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–ç¨‹åºå ç”¨æ˜¾å­˜

### Q2: æ¨ç†é€Ÿåº¦æ…¢
**é—®é¢˜**: æ¨¡å‹æ¨ç†å“åº”æ—¶é—´è¿‡é•¿
**è§£å†³æ–¹æ¡ˆ**:
- å‡å°‘ `max_tokens` å‚æ•°å€¼
- ä½¿ç”¨é‡åŒ–æ¨¡å‹
- ç¡®ä¿CUDAå’ŒPyTorchç‰ˆæœ¬å…¼å®¹


# è¿›é˜¶ï¼šè§†é¢‘é—®ç­”åŠŸèƒ½

## è§†é¢‘é—®ç­”APIæœåŠ¡

é™¤äº†åŸºç¡€çš„å›¾åƒé—®ç­”åŠŸèƒ½ï¼ŒQwen3-VL-4B-Instructè¿˜æ”¯æŒè§†é¢‘å†…å®¹ç†è§£ã€‚æˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªå¢å¼ºç‰ˆçš„APIæœåŠ¡æ¥æ”¯æŒè§†é¢‘è¾“å…¥ã€‚

### åˆ›å»ºè§†é¢‘é—®ç­”æœåŠ¡

æ–°å»º `api_server_qwen3vl_video.py` æ–‡ä»¶ï¼š

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import torch
from transformers import AutoProcessor, Qwen3VLForConditionalGeneration
from qwen_vl_utils import process_vision_info
from fastapi import FastAPI
import uvicorn
from pydantic import BaseModel
from typing import List, Dict, Any, Optional

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
torch.set_num_threads(8)

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(title="Qwen3-VL-4B Video API", version="1.0.0")

# æ¨¡å‹è·¯å¾„
model_name_or_path = '/root/autodl-fs/Qwen/Qwen3-VL-4B-Instruct'

# åˆå§‹åŒ–æ¨¡å‹å’Œå¤„ç†å™¨
model = Qwen3VLForConditionalGeneration.from_pretrained(
    model_name_or_path,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

processor = AutoProcessor.from_pretrained(
    model_name_or_path,
    trust_remote_code=True
)

# è¯·æ±‚æ¨¡å‹
class ChatRequest(BaseModel):
    messages: List[Dict[str, Any]]
    max_tokens: Optional[int] = 512
    temperature: Optional[float] = 0.7
    top_p: Optional[float] = 0.9

# å“åº”æ¨¡å‹
class ChatResponse(BaseModel):
    response: str
    model: str = "Qwen3-VL-4B-Instruct"
    usage: Dict[str, int]

@app.get("/")
async def root():
    return {"message": "Qwen3-VL-4B-Instruct Video API Server is running!"}

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "model": "Qwen3-VL-4B-Instruct",
        "device": str(model.device),
        "torch_version": torch.__version__,
        "cuda_available": torch.cuda.is_available(),
        "gpu_memory": f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB" if torch.cuda.is_available() else "N/A",
        "supported_formats": ["image", "video"]
    }

@app.post("/v1/chat/completions", response_model=ChatResponse)
async def chat_completions(request: ChatRequest):
    try:
        # å¤„ç†æ¶ˆæ¯
        messages = request.messages
        
        # å¤„ç†è§†è§‰ä¿¡æ¯ï¼ˆåŒ…æ‹¬å›¾åƒå’Œè§†é¢‘ï¼‰
        text = processor.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        image_inputs, video_inputs = process_vision_info(messages)
        
        # å‡†å¤‡è¾“å…¥
        inputs = processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        )
        inputs = inputs.to(model.device)
        
        # ç”Ÿæˆå“åº”
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
                do_sample=True,
                pad_token_id=processor.tokenizer.eos_token_id
            )
        
        # è§£ç å“åº”
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        
        response_text = processor.batch_decode(
            generated_ids_trimmed, 
            skip_special_tokens=True, 
            clean_up_tokenization_spaces=False
        )[0]
        
        # è®¡ç®—tokenä½¿ç”¨é‡
        input_tokens = inputs.input_ids.shape[1]
        output_tokens = len(generated_ids_trimmed[0])
        
        return ChatResponse(
            response=response_text,
            usage={
                "prompt_tokens": input_tokens,
                "completion_tokens": output_tokens,
                "total_tokens": input_tokens + output_tokens
            }
        )
        
    except Exception as e:
        return ChatResponse(
            response=f"Error: {str(e)}",
            usage={"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
        )

# å…¼å®¹åŸæœ‰çš„ /generate æ¥å£
@app.post("/generate")
async def generate_response(request: ChatRequest):
    """å…¼å®¹åŸæœ‰æ¥å£æ ¼å¼"""
    result = await chat_completions(request)
    return {"response": result.response}

if __name__ == "__main__":
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=8000,
        log_level="info"
    )
```

### å¯åŠ¨è§†é¢‘é—®ç­”æœåŠ¡

```bash
python api_server_qwen3vl_video.py
```

![alt text](./images/01-6.png)

## è§†é¢‘é—®ç­”æµ‹è¯•

### åˆ›å»ºæµ‹è¯•è„šæœ¬

æ–°å»º `test_video_api.py` æ–‡ä»¶ï¼š

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
import json
import time

# APIæœåŠ¡åœ°å€
API_BASE_URL = "http://localhost:8000"

def test_health_check():
    """æµ‹è¯•å¥åº·æ£€æŸ¥æ¥å£"""
    print("=== å¥åº·æ£€æŸ¥æµ‹è¯• ===")
    try:
        response = requests.get(f"{API_BASE_URL}/health")
        if response.status_code == 200:
            result = response.json()
            print("âœ… å¥åº·æ£€æŸ¥é€šè¿‡")
            print(f"æ¨¡å‹: {result.get('model')}")
            print(f"è®¾å¤‡: {result.get('device')}")
            print(f"CUDAå¯ç”¨: {result.get('cuda_available')}")
            print(f"GPUå†…å­˜: {result.get('gpu_memory')}")
            print(f"æ”¯æŒæ ¼å¼: {result.get('supported_formats')}")
            return True
        else:
            print(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ å¥åº·æ£€æŸ¥å¼‚å¸¸: {e}")
        return False

def test_video_conversation():
    """æµ‹è¯•è§†é¢‘å¯¹è¯"""
    print("\n=== è§†é¢‘å¯¹è¯æµ‹è¯• ===")
    try:
        # ä½¿ç”¨æœ¬åœ°è§†é¢‘æ–‡ä»¶ï¼ˆè¯·ç¡®ä¿è§†é¢‘æ–‡ä»¶å­˜åœ¨ï¼‰
        video_path = "./test_video.mp4"
        
        payload = {
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "video",
                            "video": video_path,
                            "fps": 1.0,
                            "max_pixels": 360 * 420
                        },
                        {
                            "type": "text",
                            "text": "è¯·æè¿°è¿™ä¸ªè§†é¢‘çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä¸»è¦åœºæ™¯å’ŒåŠ¨ä½œã€‚"
                        }
                    ]
                }
            ],
            "max_tokens": 512,
            "temperature": 0.7
        }
        
        response = requests.post(
            f"{API_BASE_URL}/v1/chat/completions",
            json=payload,
            headers={"Content-Type": "application/json"}
        )
        
        if response.status_code == 200:
            result = response.json()
            print("âœ… è§†é¢‘å¯¹è¯æµ‹è¯•æˆåŠŸ")
            print(f"è§†é¢‘æ–‡ä»¶: {video_path}")
            print(f"å›å¤: {result['response']}")
            print(f"Tokenä½¿ç”¨: {result['usage']}")
            return True
        else:
            print(f"âŒ è§†é¢‘å¯¹è¯æµ‹è¯•å¤±è´¥: {response.status_code}")
            print(f"é”™è¯¯ä¿¡æ¯: {response.text}")
            return False
            
    except Exception as e:
        print(f"âŒ è§†é¢‘å¯¹è¯æµ‹è¯•å¼‚å¸¸: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯• Qwen3-VL-4B-Instruct Video API")
    print("=" * 50)
    
    # ç­‰å¾…æœåŠ¡å¯åŠ¨
    print("ç­‰å¾…APIæœåŠ¡å¯åŠ¨...")
    time.sleep(2)
    
    # æ‰§è¡Œæµ‹è¯•
    tests = [
        test_health_check,
        test_video_conversation
    ]
    
    passed = 0
    total = len(tests)
    
    for test_func in tests:
        if test_func():
            passed += 1
        time.sleep(1)  # æµ‹è¯•é—´éš”
    
    print("\n" + "=" * 50)
    print(f"æµ‹è¯•å®Œæˆ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIæœåŠ¡çŠ¶æ€")

if __name__ == "__main__":
    main()
```

### è¿è¡Œæµ‹è¯•

```bash
python test_video_api.py
```

![alt text](./images/01-7.png)