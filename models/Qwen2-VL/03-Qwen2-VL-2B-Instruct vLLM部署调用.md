# 03-Qwen2-vl-2B vLLM éƒ¨ç½²è°ƒç”¨

`vLLM` æ¡†æ¶æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„å¤§è¯­è¨€æ¨¡å‹**æ¨ç†å’Œéƒ¨ç½²æœåŠ¡ç³»ç»Ÿ**ï¼Œå…·å¤‡ä»¥ä¸‹ç‰¹æ€§ï¼š

- **é«˜æ•ˆçš„å†…å­˜ç®¡ç†**ï¼šé€šè¿‡ `PagedAttention` ç®—æ³•ï¼Œ`vLLM` å®ç°äº†å¯¹ `KV` ç¼“å­˜çš„é«˜æ•ˆç®¡ç†ï¼Œå‡å°‘äº†å†…å­˜æµªè´¹ï¼Œä¼˜åŒ–äº†æ¨¡å‹çš„è¿è¡Œæ•ˆç‡ã€‚
- **é«˜ååé‡**ï¼š`vLLM` æ”¯æŒå¼‚æ­¥å¤„ç†å’Œè¿ç»­æ‰¹å¤„ç†è¯·æ±‚ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹æ¨ç†çš„ååé‡ï¼ŒåŠ é€Ÿäº†æ–‡æœ¬ç”Ÿæˆå’Œå¤„ç†é€Ÿåº¦ã€‚
- **æ˜“ç”¨æ€§**ï¼š`vLLM` ä¸ `HuggingFace` æ¨¡å‹æ— ç¼é›†æˆï¼Œæ”¯æŒå¤šç§æµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç®€åŒ–äº†æ¨¡å‹éƒ¨ç½²å’Œæ¨ç†çš„è¿‡ç¨‹ã€‚å…¼å®¹ `OpenAI` çš„ `API` æœåŠ¡å™¨ã€‚
- **åˆ†å¸ƒå¼æ¨ç†**ï¼šæ¡†æ¶æ”¯æŒåœ¨å¤š `GPU` ç¯å¢ƒä¸­è¿›è¡Œåˆ†å¸ƒå¼æ¨ç†ï¼Œé€šè¿‡æ¨¡å‹å¹¶è¡Œç­–ç•¥å’Œé«˜æ•ˆçš„æ•°æ®é€šä¿¡ï¼Œæå‡äº†å¤„ç†å¤§å‹æ¨¡å‹çš„èƒ½åŠ›ã€‚
- **å¼€æºå…±äº«**ï¼š`vLLM` ç”±äºå…¶å¼€æºçš„å±æ€§ï¼Œæ‹¥æœ‰æ´»è·ƒçš„ç¤¾åŒºæ”¯æŒï¼Œè¿™ä¹Ÿä¾¿äºå¼€å‘è€…è´¡çŒ®å’Œæ”¹è¿›ï¼Œå…±åŒæ¨åŠ¨æŠ€æœ¯å‘å±•ã€‚

## ç¯å¢ƒå‡†å¤‡

æœ¬æ–‡åŸºç¡€ç¯å¢ƒå¦‚ä¸‹ï¼š

```
----------------
ubuntu 22.04
python 3.10
cuda 12.1
pytorch 2.4.0
----------------
```

> æœ¬æ–‡é»˜è®¤å­¦ä¹ è€…å·²é…ç½®å¥½ä»¥ä¸Š `Pytorch (cuda)` ç¯å¢ƒï¼Œå¦‚æœªé…ç½®è¯·å…ˆè‡ªè¡Œå®‰è£…ã€‚

é¦–å…ˆ `pip` æ¢æºåŠ é€Ÿä¸‹è½½å¹¶å®‰è£…ä¾èµ–åŒ…

```bash
python -m pip install --upgrade pip
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

pip install modelscope==1.20.0
pip install openai==1.54.4
pip install tqdm==4.67.0
pip install transformers==4.46.2
pip install vllm==0.6.3.post1
pip install wen-vl-utils==0.0.8
```

> è€ƒè™‘åˆ°éƒ¨åˆ†åŒå­¦é…ç½®ç¯å¢ƒå¯èƒ½ä¼šé‡åˆ°ä¸€äº›é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨AutoDLå¹³å°å‡†å¤‡äº†Qwen2-VLçš„ç¯å¢ƒé•œåƒï¼Œç‚¹å‡»ä¸‹æ–¹é“¾æ¥å¹¶ç›´æ¥åˆ›å»ºAutodlç¤ºä¾‹å³å¯ã€‚
> ***https://www.codewithgpu.com/i/datawhalechina/self-llm/qwen2-vl***

## æ¨¡å‹ä¸‹è½½  

ä½¿ç”¨ `modelscope` ä¸­çš„ `snapshot_download` å‡½æ•°ä¸‹è½½æ¨¡å‹ï¼Œç¬¬ä¸€ä¸ªå‚æ•°ä¸ºæ¨¡å‹åç§°ï¼Œå‚æ•° `cache_dir`ä¸ºæ¨¡å‹çš„ä¸‹è½½è·¯å¾„ã€‚

å…ˆåˆ‡æ¢åˆ° `autodl-tmp` ç›®å½•ï¼Œ`cd /root/autodl-tmp` 

ç„¶åæ–°å»ºåä¸º `model_download.py` çš„ `python` è„šæœ¬ï¼Œå¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹å¹¶ä¿å­˜

```python
# model_download.py
from modelscope import snapshot_download
model_dir = snapshot_download('Qwen/Qwen2-VL-2B-Instruct', cache_dir='/root/autodl-tmp', revision='master')
```

ç„¶ååœ¨ç»ˆç«¯ä¸­è¾“å…¥ `python model_download.py` æ‰§è¡Œä¸‹è½½ï¼Œè¿™é‡Œéœ€è¦è€å¿ƒç­‰å¾…ä¸€æ®µæ—¶é—´ç›´åˆ°æ¨¡å‹ä¸‹è½½å®Œæˆã€‚

> æ³¨æ„ï¼šè®°å¾—ä¿®æ”¹ `cache_dir` ä¸ºä½ çš„æ¨¡å‹ä¸‹è½½è·¯å¾„å“¦~

## **ä»£ç å‡†å¤‡**

### **Pythonè„šæœ¬**

åœ¨ `/root/autodl-tmp` è·¯å¾„ä¸‹æ–°å»º `vllm_model.py` æ–‡ä»¶å¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼Œç²˜è´´ä»£ç åè¯·åŠæ—¶ä¿å­˜æ–‡ä»¶ã€‚ä¸‹é¢çš„ä»£ç æœ‰å¾ˆè¯¦ç»†çš„æ³¨é‡Šï¼Œå¦‚æœ‰ä¸ç†è§£çš„åœ°æ–¹ï¼Œæ¬¢è¿å¤§å®¶æ `issue`ã€‚

é¦–å…ˆä» `vLLM` åº“ä¸­å¯¼å…¥ `LLM` å’Œ `SamplingParams` ç±»ã€‚`LLM` ç±»æ˜¯ä½¿ç”¨ `vLLM` å¼•æ“è¿è¡Œç¦»çº¿æ¨ç†çš„ä¸»è¦ç±»ã€‚`SamplingParams` ç±»æŒ‡å®šé‡‡æ ·è¿‡ç¨‹çš„å‚æ•°ï¼Œç”¨äºæ§åˆ¶å’Œè°ƒæ•´ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§å’Œå¤šæ ·æ€§ã€‚

`vLLM` æä¾›äº†éå¸¸æ–¹ä¾¿çš„å°è£…ï¼Œæˆ‘ä»¬ç›´æ¥ä¼ å…¥æ¨¡å‹åç§°æˆ–æ¨¡å‹è·¯å¾„å³å¯ï¼Œä¸å¿…æ‰‹åŠ¨åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡è¿™ä¸ªä»£ç ç¤ºä¾‹ç†Ÿæ‚‰ä¸‹ ` vLLM` å¼•æ“çš„ä½¿ç”¨æ–¹å¼ã€‚è¢«æ³¨é‡Šçš„éƒ¨åˆ†å†…å®¹å¯ä»¥ä¸°å¯Œæ¨¡å‹çš„èƒ½åŠ›ï¼Œä½†ä¸æ˜¯å¿…è¦çš„ï¼Œå¤§å®¶å¯ä»¥æŒ‰éœ€é€‰æ‹©ï¼Œè‡ªå·±å¤šå¤šåŠ¨æ‰‹å°è¯• ~

```python
# vllm_model.py
from vllm import LLM, SamplingParams
from transformers import AutoProcessor
import os
import json

# è‡ªåŠ¨ä¸‹è½½æ¨¡å‹æ—¶ï¼ŒæŒ‡å®šä½¿ç”¨modelscope; å¦åˆ™ï¼Œä¼šä»HuggingFaceä¸‹è½½
os.environ['VLLM_USE_MODELSCOPE']='True'

def get_completion(prompts, model, tokenizer=None, max_tokens=512, temperature=0.8, top_p=0.95, max_model_len=2048):
    stop_token_ids = [151329, 151336, 151338]
    # åˆ›å»ºé‡‡æ ·å‚æ•°ã€‚temperature æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§ï¼Œtop_p æ§åˆ¶æ ¸å¿ƒé‡‡æ ·çš„æ¦‚ç‡
    sampling_params = SamplingParams(temperature=temperature, top_p=top_p, max_tokens=max_tokens, stop_token_ids=stop_token_ids)
    # åˆå§‹åŒ– vLLM æ¨ç†å¼•æ“
    llm = LLM(model=model, tokenizer=tokenizer, max_model_len=max_model_len,trust_remote_code=True)
    outputs = llm.generate(prompts, sampling_params)
    return outputs


if __name__ == "__main__":    
    # åˆå§‹åŒ– vLLM æ¨ç†å¼•æ“
    model='/root/autodl-tmp/Qwen/Qwen2-VL-2B-Instruct' # æŒ‡å®šæ¨¡å‹è·¯å¾„
    tokenizer = None
    # åŠ è½½åˆ†è¯å™¨åä¼ å…¥vLLM æ¨¡å‹ï¼Œä½†ä¸æ˜¯å¿…è¦çš„ã€‚
    processor = AutoProcessor.from_pretrained(model)
    
    
    messages = [
    	{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚"},
    	{"role": "user", "content": [
        {"type": "image_url", 
         "image_url": {
           "url": "https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png"}
        },
        {"type": "text", "text": "æ’å›¾ä¸­çš„æ–‡æœ¬æ˜¯ä»€ä¹ˆï¼Ÿ"}
    	]
      }
    ]

    prompt = processor.apply_chat_template(
      messages,
      tokenize=False,
      add_generation_prompt=True,
		)

    outputs = get_completion(prompt, model, tokenizer=tokenizer, max_tokens=512, temperature=1, top_p=1, max_model_len=2048)

    # è¾“å‡ºæ˜¯ä¸€ä¸ªåŒ…å« promptã€ç”Ÿæˆæ–‡æœ¬å’Œå…¶ä»–ä¿¡æ¯çš„ RequestOutput å¯¹è±¡åˆ—è¡¨ã€‚
    # æ‰“å°è¾“å‡ºã€‚
    for output in outputs:
        prompt = output.prompt
        generated_text = output.outputs[0].text
        print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
```

è¿è¡Œä»£ç 

```bash
cd /root/autodl-tmp && python vllm_model.py
```

ç»“æœå¦‚ä¸‹ï¼š

```bash
Prompt: '<|im_start|>system\nä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>æ’å›¾ä¸­çš„æ–‡æœ¬æ˜¯ä»€ä¹ˆï¼Ÿ<|im_end|>\n<|im_start|>assistant\n', Generated text: 'æ’å›¾ä¸­æ˜¯â€œ.YEARâ€ä»¥åŠâ€œimestot-valueâ€'
```

![03-1](./images/03-1.png)

### åˆ›å»ºå…¼å®¹ OpenAI API æ¥å£çš„æœåŠ¡å™¨

`Qwen` å…¼å®¹ `OpenAI API` åè®®ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨ `vLLM` åˆ›å»º `OpenAI API` æœåŠ¡å™¨ã€‚`vLLM` éƒ¨ç½²å®ç° `OpenAI API` åè®®çš„æœåŠ¡å™¨éå¸¸æ–¹ä¾¿ã€‚é»˜è®¤ä¼šåœ¨ http://localhost:8000 å¯åŠ¨æœåŠ¡å™¨ã€‚æœåŠ¡å™¨å½“å‰ä¸€æ¬¡æ‰˜ç®¡ä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶å®ç°åˆ—è¡¨æ¨¡å‹ã€`completions` å’Œ `chat completions` ç«¯å£ã€‚

- `completions`ï¼šæ˜¯åŸºæœ¬çš„æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œæ¨¡å‹ä¼šåœ¨ç»™å®šçš„æç¤ºåç”Ÿæˆä¸€æ®µæ–‡æœ¬ã€‚è¿™ç§ç±»å‹çš„ä»»åŠ¡é€šå¸¸ç”¨äºç”Ÿæˆæ–‡ç« ã€æ•…äº‹ã€é‚®ä»¶ç­‰ã€‚
- `chat completions`ï¼šæ˜¯é¢å‘å¯¹è¯çš„ä»»åŠ¡ï¼Œæ¨¡å‹éœ€è¦ç†è§£å’Œç”Ÿæˆå¯¹è¯ã€‚è¿™ç§ç±»å‹çš„ä»»åŠ¡é€šå¸¸ç”¨äºæ„å»ºèŠå¤©æœºå™¨äººæˆ–è€…å¯¹è¯ç³»ç»Ÿã€‚

åœ¨åˆ›å»ºæœåŠ¡å™¨æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‡å®šæ¨¡å‹åç§°ã€æ¨¡å‹è·¯å¾„ã€èŠå¤©æ¨¡æ¿ç­‰å‚æ•°ã€‚

- `--host` å’Œ `--port` å‚æ•°æŒ‡å®šåœ°å€ã€‚
- `--model` å‚æ•°æŒ‡å®šæ¨¡å‹åç§°ã€‚
- `--chat-template` å‚æ•°æŒ‡å®šèŠå¤©æ¨¡æ¿ã€‚
- `--served-model-name` æŒ‡å®šæœåŠ¡æ¨¡å‹çš„åç§°ã€‚
- `--max-model-len` æŒ‡å®šæ¨¡å‹çš„æœ€å¤§é•¿åº¦ã€‚

```bash
python -m vllm.entrypoints.openai.api_server --model /root/autodl-tmp/Qwen/Qwen2-VL-2B-Instruct  --served-model-name Qwen2-VL-2B-Instruct --max-model-len=2048
```

åŠ è½½å®Œæ¯•åå‡ºç°å¦‚ä¸‹ä¿¡æ¯è¯´æ˜æœåŠ¡æˆåŠŸå¯åŠ¨

![03-2](.//images/03-2.png)

- é€šè¿‡ `curl` å‘½ä»¤æŸ¥çœ‹å½“å‰çš„æ¨¡å‹åˆ—è¡¨

```bash
curl http://localhost:8000/v1/models
```

â€‹	å¾—åˆ°çš„è¿”å›å€¼å¦‚ä¸‹æ‰€ç¤º

```json
{
  "object":"list",
  "data":[
    {
      "id":"Qwen2-VL-2B-Instruct",
      "object":"model",
      "created":1731747181,
      "owned_by":"vllm",
      "root":"/root/autodl-tmp/Qwen/Qwen2-VL-2B-Instruct",
      "parent":null,
      "max_model_len":2048,
      "permission":[
        {
          "id":"modelperm-aa946b04d0f9463ebac64cec7f9b6313",
          "object":"model_permission",
          "created":1731747181,
          "allow_create_engine":false,
          "allow_sampling":true,
          "allow_logprobs":true,
          "allow_search_indices":false,
          "allow_view":true,
          "allow_fine_tuning":false,
          "organization":"*",
          "group":null,
          "is_blocking":false
        }
      ]
    }
  ]
}
```

- ä½¿ç”¨ `curl` å‘½ä»¤æµ‹è¯• `OpenAI Completions API` 


```bash
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen2-VL-2B-Instruct",
        "messages": [
    	{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚"},
    	{"role": "user", "content": [
        {"type": "image_url", 
         "image_url": {
           "url": "https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png"}
        },
        {"type": "text", "text": "æ’å›¾ä¸­çš„æ–‡æœ¬æ˜¯ä»€ä¹ˆï¼Ÿ"}
    	 ]
      }
    ]
 }'
```

â€‹	å¾—åˆ°çš„è¿”å›å€¼å¦‚ä¸‹æ‰€ç¤º

```json
{
  "id":"chat-505f8e3987384ba6b1f7a293217757da",
  "object":"chat.completion",
  "created":1731919906,
  "model":"Qwen2-VL-2B-Instruct",
  "choices":[
    {
      "index":0,
      "message":
      {
        "role":"assistant",
        "content":"æ’å›¾ä¸­çš„æ–‡æœ¬æ˜¯ \"TONGYI Qwen\"ã€‚",
        "tool_calls":[]
      },
      "logprobs":null,
      "finish_reason":"stop",
      "stop_reason":null
    }],
  "usage":
  {
    "prompt_tokens":71,
    "total_tokens":86,
    "completion_tokens":15
  },
  "prompt_logprobs":null
}
```

- ç”¨ `Python` è„šæœ¬è¯·æ±‚ `OpenAI Completions API` 


```python
# vllm_openai_completions.py
from openai import OpenAI
client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="sk-xxx", # éšä¾¿å¡«å†™ï¼Œåªæ˜¯ä¸ºäº†é€šè¿‡æ¥å£å‚æ•°æ ¡éªŒ
)

completion = client.chat.completions.create(
  model="Qwen2-VL-2B-Instruct",
  messages = [
    	{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚"},
    	{"role": "user", "content": [
        {"type": "image_url", 
         "image_url": {
           "url": "https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png"}
        },
        {"type": "text", "text": "æ’å›¾ä¸­çš„æ–‡æœ¬æ˜¯ä»€ä¹ˆï¼Ÿ"}
    	]
      }
    ]
)

print(completion.choices[0].message)
```

```shell
python vllm_openai_completions.py
```

â€‹	å¾—åˆ°çš„è¿”å›å€¼å¦‚ä¸‹æ‰€ç¤º

```
ChatCompletionMessage(content='æ’å›¾ä¸­çš„æ–‡æœ¬æ˜¯â€œTONGYI Qwenâ€ã€‚', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[])
```

- ç”¨ `curl` å‘½ä»¤æµ‹è¯• `OpenAI Chat Completions API` 


```bash
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen2-VL-2B-Instruct",
        "messages":[
          {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚"},
          {"role": "user", "content": [
            {"type": "image_url", 
             "image_url": {
               "url": "https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png"}
            },
            {"type": "text", "text": "æ’å›¾ä¸­çš„æ–‡æœ¬æ˜¯ä»€ä¹ˆï¼Ÿ"}
          ]
          }
        ]
    }'
```

å¾—åˆ°çš„è¿”å›å€¼å¦‚ä¸‹æ‰€ç¤º

```json
{
  "id":"chat-67963afa27e541309cd40798d75bdab8",
  "object":"chat.completion",
  "created":1731920262,
  "model":"Qwen2-VL-2B-Instruct",
  "choices":[
    {
      "index":0,
      "message":
      {
        "role":"assistant",
        "content":"æ’å›¾ä¸­çš„æ–‡æœ¬æ˜¯â€œTONGYI Qwenâ€ã€‚",
        "tool_calls":[]
      },
      "logprobs":null,
      "finish_reason":"stop",
      "stop_reason":null
    }],
  "usage":
  {
    "prompt_tokens":71,
    "total_tokens":85,
    "completion_tokens":14},
  "prompt_logprobs":null
}
```

- ç”¨ `Python` è„šæœ¬è¯·æ±‚ `OpenAI Chat Completions API` 


```python
# vllm_openai_chat_completions.py
from openai import OpenAI
openai_api_key = "sk-xxx" # éšä¾¿å¡«å†™ï¼Œåªæ˜¯ä¸ºäº†é€šè¿‡æ¥å£å‚æ•°æ ¡éªŒ
openai_api_base = "http://localhost:8000/v1"

client = OpenAI(
    api_key=openai_api_key,
    base_url=openai_api_base,
)

chat_outputs = client.chat.completions.create(
    model="Qwen2-VL-2B-Instruct",
    messages = [
    	{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚"},
    	{"role": "user", "content": [
        {"type": "image_url", 
         "image_url": {
           "url": "https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png"}
        },
        {"type": "text", "text": "æ’å›¾ä¸­çš„æ–‡æœ¬æ˜¯ä»€ä¹ˆï¼Ÿ"}
    	]
      }
    ]
)
print(chat_outputs)
```

```shell
python vllm_openai_chat_completions.py
```

å¾—åˆ°çš„è¿”å›å€¼å¦‚ä¸‹æ‰€ç¤º

```
{"id":"chat-67963afa27e541309cd40798d75bdab8","object":"chat.completion","created":1731920262,"model":"Qwen2-VL-2B-Instruct","choices":[{"index":0,"message":{"role":"assistant","content":"æ’å›¾ä¸­çš„æ–‡æœ¬æ˜¯â€œTONGYI Qwenâ€ã€‚","tool_calls":[]},"logprobs":null,"finish_reason":"stop","stop_reason":null}],"usage":{"prompt_tokens":71,"total_tokens":85,"completion_tokens":14},"prompt_logpropython vllm_openai_chat_completions.py3d8-8d39a0b2:~/autodl-tChatCompletion(id='chat-13bb084e02d94d449f441c2c39ea4b00', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='æ’å›¾ä¸­çš„æ–‡æœ¬æ˜¯â€œTONGYI Qwenâ€ã€‚', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731920356, model='Qwen2-VL-2B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=14, prompt_tokens=71, total_tokens=85, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)
```

å¦å¤–ï¼Œåœ¨ä»¥ä¸Šæ‰€æœ‰çš„åœ¨è¯·æ±‚å¤„ç†è¿‡ç¨‹ä¸­ï¼Œ `API` åç«¯éƒ½ä¼šæ‰“å°ç›¸å¯¹åº”çš„æ—¥å¿—å’Œç»Ÿè®¡ä¿¡æ¯ğŸ˜Š

![03-3](./images/03-3.png)

