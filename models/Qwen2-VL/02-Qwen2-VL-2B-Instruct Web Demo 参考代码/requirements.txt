# # requirements.txt
accelerate==1.1.0
aiofiles==23.2.1
annotated-types==0.7.0
anyio==4.6.2.post1
av==13.1.0
certifi==2024.8.30
charset-normalizer==3.4.0
click==8.1.7
exceptiongroup==1.2.2
fastapi==0.115.5
ffmpy==0.4.0
filelock==3.16.1
fsspec==2024.10.0
gradio==5.5.0
gradio_client==1.4.2
h11==0.14.0
httpcore==1.0.6
httpx==0.27.2
huggingface-hub==0.26.2
idna==3.10
Jinja2==3.1.4
markdown-it-py==3.0.0
MarkupSafe==2.1.5
mdurl==0.1.2
mpmath==1.3.0
networkx==3.4.2
numpy==2.1.3
nvidia-cublas-cu12==12.4.5.8
nvidia-cuda-cupti-cu12==12.4.127
nvidia-cuda-nvrtc-cu12==12.4.127
nvidia-cuda-runtime-cu12==12.4.127
nvidia-cudnn-cu12==9.1.0.70
nvidia-cufft-cu12==11.2.1.3
nvidia-curand-cu12==10.3.5.147
nvidia-cusolver-cu12==11.6.1.9
nvidia-cusparse-cu12==12.3.1.170
nvidia-nccl-cu12==2.21.5
nvidia-nvjitlink-cu12==12.4.127
nvidia-nvtx-cu12==12.4.127
orjson==3.10.11
packaging==24.2
pandas==2.2.3
pillow==11.0.0
psutil==6.1.0
pydantic==2.9.2
pydantic_core==2.23.4
pydub==0.25.1
Pygments==2.18.0
python-dateutil==2.9.0.post0
python-multipart==0.0.12
pytz==2024.2
PyYAML==6.0.2
qwen-vl-utils==0.0.8
regex==2024.11.6
requests==2.32.3
rich==13.9.4
ruff==0.7.3
safehttpx==0.1.1
safetensors==0.4.5
semantic-version==2.10.0
shellingham==1.5.4
six==1.16.0
sniffio==1.3.1
starlette==0.41.2
sympy==1.13.1
tokenizers==0.20.3
tomlkit==0.12.0
torch==2.5.1
torchvision==0.20.1
tqdm==4.67.0
transformers==4.46.2
triton==3.1.0
typer==0.13.0
typing_extensions==4.12.2
tzdata==2024.2
urllib3==2.2.3
uvicorn==0.32.0
websockets==12.0
# # 如果安装了flash-attn，则会多出这两个库
# einops==0.8.0
# flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.0.post2/flash_attn-2.7.0.post2+cu11torch2.5cxx11abiFALSE-cp310-cp310-linux_x86_64.whl
# # 本底导出会显示下面这个样式，实际上要确认对应python、pytorch、cuda版本后下载下载上面这个
# # flash-attn @ file:///root/flash_attn-2.7.0.post2%2Bcu11torch2.5cxx11abiFALSE-cp310-cp310-linux_x86_64.whl#sha256=7ccce59f987f422d8210587383914057cf7db219988427ef71287e89153ad2fa