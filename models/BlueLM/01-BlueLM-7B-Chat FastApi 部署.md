# BlueLM-7B-Chat FastApi éƒ¨ç½²

## æ¨¡å‹ä»‹ç»

BlueLM-7B æ˜¯ç”± vivo AI å…¨çƒç ”ç©¶é™¢è‡ªä¸»ç ”å‘çš„å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œå‚æ•°è§„æ¨¡ä¸º 70 äº¿ã€‚BlueLM-7B åœ¨ [C-Eval](https://cevalbenchmark.com/index.html) å’Œ [CMMLU](https://github.com/haonan-li/CMMLU) ä¸Šå‡å–å¾—é¢†å…ˆç»“æœï¼Œå¯¹æ¯”åŒå°ºå¯¸å¼€æºæ¨¡å‹ä¸­å…·æœ‰è¾ƒå¼ºçš„ç«äº‰åŠ›(æˆªæ­¢11æœˆ1å·)ã€‚æœ¬æ¬¡å‘å¸ƒå…±åŒ…å« 7B æ¨¡å‹çš„ Base å’Œ Chat ä¸¤ä¸ªç‰ˆæœ¬ã€‚

æ¨¡å‹ä¸‹è½½é“¾æ¥è§ï¼š

|                           åŸºåº§æ¨¡å‹                           |                           å¯¹é½æ¨¡å‹                           |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ğŸ¤— [BlueLM-7B-Base](https://huggingface.co/vivo-ai/BlueLM-7B-Base) | ğŸ¤— [BlueLM-7B-Chat](https://huggingface.co/vivo-ai/BlueLM-7B-Chat) |
| ğŸ¤— [BlueLM-7B-Base-32K](https://huggingface.co/vivo-ai/BlueLM-7B-Base-32K) | ğŸ¤— [BlueLM-7B-Chat-32K](https://huggingface.co/vivo-ai/BlueLM-7B-Chat-32K) |
|                                                              | ğŸ¤— [BlueLM-7B-Chat-4bits](https://huggingface.co/vivo-ai/BlueLM-7B-Chat-4bits) |

## ç¯å¢ƒå‡†å¤‡

è¿™é‡Œåœ¨ [Autodl](https://www.autodl.com/) å¹³å°ä¸­ç§Ÿèµä¸€ä¸ª3090 ç­‰ 24G æ˜¾å­˜çš„æ˜¾å¡æœºå™¨ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºé•œåƒé€‰æ‹© PyTorch-->1.11.0-->3.8(ubuntu20.04)-->11.3ï¼ŒCudaç‰ˆæœ¬åœ¨11.3ä»¥ä¸Šéƒ½å¯ä»¥ã€‚

![image-20240319162858866](./images/202403191628941.png)

æ¥ä¸‹æ¥æ‰“å¼€åˆšåˆšç§Ÿç”¨æœåŠ¡å™¨çš„ JupyterLab(ä¹Ÿå¯ä»¥ä½¿ç”¨vscode sshè¿œç¨‹è¿æ¥æœåŠ¡å™¨)ï¼Œå¹¶ä¸”æ‰“å¼€å…¶ä¸­çš„ç»ˆç«¯å¼€å§‹ç¯å¢ƒé…ç½®ã€æ¨¡å‹ä¸‹è½½å’Œè¿è¡Œ demoã€‚

pip æ¢æºåŠ é€Ÿä¸‹è½½å¹¶å®‰è£…ä¾èµ–åŒ…

```bash
# å‡çº§pip
python -m pip install --upgrade pip
# è®¾ç½®pipé•œåƒæº
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
# å®‰è£…è½¯ä»¶ä¾èµ–
pip install fastapi==0.104.1
pip install uvicorn==0.24.0.post1
pip install requests==2.25.1
pip install modelscope==1.11.0
pip install transformers==4.37.0
pip install streamlit==1.24.0
pip install sentencepiece==0.1.99
pip install accelerate==0.24.1
pip install transformers_stream_generator==0.0.4
```

## æ¨¡å‹ä¸‹è½½

ä½¿ç”¨`Modelscope API` ä¸‹è½½`BlueLM-7B-Chat`æ¨¡å‹ï¼Œæ¨¡å‹è·¯å¾„ä¸º`/root/autodl-tmp`ã€‚åœ¨ /root/autodl-tmp ä¸‹åˆ›å»ºmodel_download.pyæ–‡ä»¶å†…å®¹å¦‚ä¸‹: 

```python
from modelscope import snapshot_download
model_dir = snapshot_download("vivo-ai/BlueLM-7B-Chat", cache_dir='/root/autodl-tmp', revision="master")
```

## ä»£ç å‡†å¤‡

åœ¨ /root/autodl-tmp è·¯å¾„ä¸‹æ–°å»º api.py æ–‡ä»¶å†…å®¹å¦‚ä¸‹: 

```python
from fastapi import FastAPI, Request
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
import uvicorn
import json
import datetime
import torch

# è®¾ç½®è®¾å¤‡å‚æ•°
DEVICE = "cuda"  # ä½¿ç”¨CUDA
DEVICE_ID = "0"  # CUDAè®¾å¤‡IDï¼Œå¦‚æœæœªè®¾ç½®åˆ™ä¸ºç©º
CUDA_DEVICE = f"{DEVICE}:{DEVICE_ID}" if DEVICE_ID else DEVICE  # ç»„åˆCUDAè®¾å¤‡ä¿¡æ¯

# æ¸…ç†GPUå†…å­˜å‡½æ•°
def torch_gc():
    if torch.cuda.is_available():  # æ£€æŸ¥æ˜¯å¦å¯ç”¨CUDA
        with torch.cuda.device(CUDA_DEVICE):  # æŒ‡å®šCUDAè®¾å¤‡
            torch.cuda.empty_cache()  # æ¸…ç©ºCUDAç¼“å­˜
            torch.cuda.ipc_collect()  # æ”¶é›†CUDAå†…å­˜ç¢ç‰‡

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI()

# å¤„ç†POSTè¯·æ±‚çš„ç«¯ç‚¹
@app.post("/")
async def create_item(request: Request):
    global model, tokenizer  # å£°æ˜å…¨å±€å˜é‡ä»¥ä¾¿åœ¨å‡½æ•°å†…éƒ¨ä½¿ç”¨æ¨¡å‹å’Œåˆ†è¯å™¨
    json_post_raw = await request.json()  # è·å–POSTè¯·æ±‚çš„JSONæ•°æ®
    json_post = json.dumps(json_post_raw)  # å°†JSONæ•°æ®è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    json_post_list = json.loads(json_post)  # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºPythonå¯¹è±¡
    prompt = json_post_list.get('prompt')  # è·å–è¯·æ±‚ä¸­çš„æç¤º
    max_length = json_post_list.get('max_length')  # è·å–è¯·æ±‚ä¸­çš„æœ€å¤§é•¿åº¦
    
    # æ„å»º messages      
    messages = f"[|Human|]:{prompt}[|AI|]:"
    # æ„å»ºè¾“å…¥ 
    inputs = tokenizer(messages, return_tensors="pt")
    inputs = inputs.to("cuda:0")
    # é€šè¿‡æ¨¡å‹è·å¾—è¾“å‡º
    outputs = model.generate(**inputs, max_new_tokens=max_length)
    result = tokenizer.decode(outputs.cpu()[0], skip_special_tokens=True)
    
    now = datetime.datetime.now()  # è·å–å½“å‰æ—¶é—´
    time = now.strftime("%Y-%m-%d %H:%M:%S")  # æ ¼å¼åŒ–æ—¶é—´ä¸ºå­—ç¬¦ä¸²
    # æ„å»ºå“åº”JSON
    answer = {
        "response": result,
        "status": 200,
        "time": time
    }
    # æ„å»ºæ—¥å¿—ä¿¡æ¯
    log = "[" + time + "] " + '", prompt:"' + prompt + '", response:"' + repr(result) + '"'
    print(log)  # æ‰“å°æ—¥å¿—
    torch_gc()  # æ‰§è¡ŒGPUå†…å­˜æ¸…ç†
    return answer  # è¿”å›å“åº”

# ä¸»å‡½æ•°å…¥å£
if __name__ == '__main__':
    mode_name_or_path="vivo-ai/BlueLM-7B-Chat"
    # åŠ è½½é¢„è®­ç»ƒçš„åˆ†è¯å™¨å’Œæ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained(mode_name_or_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(mode_name_or_path, trust_remote_code=True,torch_dtype=torch.bfloat16,  device_map="auto")
    model.generation_config = GenerationConfig.from_pretrained(mode_name_or_path)
    model.generation_config.pad_token_id = model.generation_config.eos_token_id
    model.eval()  # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    # å¯åŠ¨FastAPIåº”ç”¨
    # ç”¨6006ç«¯å£å¯ä»¥å°†autodlçš„ç«¯å£æ˜ å°„åˆ°æœ¬åœ°ï¼Œä»è€Œåœ¨æœ¬åœ°ä½¿ç”¨api
    uvicorn.run(app, host='127.0.0.1', port=6006, workers=1)  # åœ¨æŒ‡å®šç«¯å£å’Œä¸»æœºä¸Šå¯åŠ¨åº”ç”¨
```

## Api éƒ¨ç½²

åœ¨bashç»ˆç«¯ä¸­è¾“å…¥ä»¥ä¸‹å‘½ä»¤è¿è¡ŒapiæœåŠ¡: 

```bash
cd /root/autodl-tmp
python api.py
```

ç»ˆç«¯å‡ºç°ä»¥ä¸‹è¾“å‡ºè¡¨ç¤ºæœåŠ¡æ­£åœ¨è¿è¡Œ

![image-20240319181346315](./images/202403191813385.png)

é»˜è®¤æœåŠ¡ç«¯å£ä¸º6006ï¼Œé€šè¿‡ POST æ–¹æ³•è¿›è¡Œè°ƒç”¨ï¼Œå¯ä»¥ä½¿ç”¨ curl è°ƒç”¨ï¼Œæ–°å»ºä¸€ä¸ªç»ˆç«¯åœ¨é‡Œé¢è¾“å…¥ä»¥ä¸‹å†…å®¹: 

```bash
curl -X POST "http://127.0.0.1:6006" \
     -H 'Content-Type: application/json' \
     -d '{"prompt": "ä½ å¥½"}'
```

ä¹Ÿå¯ä»¥ä½¿ç”¨ python ä¸­çš„ requests åº“è¿›è¡Œè°ƒç”¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
import requests
import json

def get_completion(prompt):
    headers = {'Content-Type': 'application/json'}
    data = {"prompt": prompt}
    response = requests.post(url='http://127.0.0.1:6006', headers=headers, data=json.dumps(data))
    return response.json()['response']

if __name__ == '__main__':
    print(get_completion('ä½ å¥½'))
```

è¿è¡Œä»¥åå¾—åˆ°çš„è¿”å›å€¼å¦‚ä¸‹æ‰€ç¤ºï¼š

```json
{"response":"ä½ å¥½ ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ","status":200,"time":"2024-03-20 12:09:29"}
```

![image-20240320121025609](./images/202403201210690.png)