# BlueLM-7B-Chat WebDemo éƒ¨ç½²

## æ¨¡å‹ä»‹ç»

BlueLM-7B æ˜¯ç”± vivo AI å…¨çƒç ”ç©¶é™¢è‡ªä¸»ç ”å‘çš„å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œå‚æ•°è§„æ¨¡ä¸º 70 äº¿ã€‚BlueLM-7B åœ¨ [C-Eval](https://cevalbenchmark.com/index.html) å’Œ [CMMLU](https://github.com/haonan-li/CMMLU) ä¸Šå‡å–å¾—é¢†å…ˆç»“æœï¼Œå¯¹æ¯”åŒå°ºå¯¸å¼€æºæ¨¡å‹ä¸­å…·æœ‰è¾ƒå¼ºçš„ç«äº‰åŠ›(æˆªæ­¢11æœˆ1å·)ã€‚æœ¬æ¬¡å‘å¸ƒå…±åŒ…å« 7B æ¨¡å‹çš„ Base å’Œ Chat ä¸¤ä¸ªç‰ˆæœ¬ã€‚

æ¨¡å‹ä¸‹è½½é“¾æ¥è§ï¼š

|                           åŸºåº§æ¨¡å‹                           |                           å¯¹é½æ¨¡å‹                           |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ğŸ¤— [BlueLM-7B-Base](https://huggingface.co/vivo-ai/BlueLM-7B-Base) | ğŸ¤— [BlueLM-7B-Chat](https://huggingface.co/vivo-ai/BlueLM-7B-Chat) |
| ğŸ¤— [BlueLM-7B-Base-32K](https://huggingface.co/vivo-ai/BlueLM-7B-Base-32K) | ğŸ¤— [BlueLM-7B-Chat-32K](https://huggingface.co/vivo-ai/BlueLM-7B-Chat-32K) |
|                                                              |                  ğŸ¤— [BlueLM-7B-Chat-4bits](https://huggingface.co/vivo-ai/BlueLM-7B-Chat-4bits)                   |

## ç¯å¢ƒå‡†å¤‡

åœ¨ autodl å¹³å°ä¸­ç§Ÿèµä¸€ä¸ª 3090 ç­‰ 24G æ˜¾å­˜çš„æ˜¾å¡æœºå™¨ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºé•œåƒé€‰æ‹© PyTorch-->1.11.0-->3.8(ubuntu20.04)-->11.3ï¼ŒCudaç‰ˆæœ¬åœ¨11.3ä»¥ä¸Šéƒ½å¯ä»¥ã€‚

![](./images/202403191628941.png)

æ¥ä¸‹æ¥æ‰“å¼€åˆšåˆšç§Ÿç”¨æœåŠ¡å™¨çš„ JupyterLab(ä¹Ÿå¯ä»¥ä½¿ç”¨vscode sshè¿œç¨‹è¿æ¥æœåŠ¡å™¨)ï¼Œå¹¶ä¸”æ‰“å¼€å…¶ä¸­çš„ç»ˆç«¯å¼€å§‹ç¯å¢ƒé…ç½®ã€æ¨¡å‹ä¸‹è½½å’Œè¿è¡Œ demoã€‚

pip æ¢æºåŠ é€Ÿä¸‹è½½å¹¶å®‰è£…ä¾èµ–åŒ…

```bash
# å‡çº§pip
python -m pip install --upgrade pip
# è®¾ç½®pipé•œåƒæº
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
# å®‰è£…è½¯ä»¶ä¾èµ–
pip install modelscope==1.11.0
pip install transformers==4.37.0
pip install streamlit==1.24.0
pip install sentencepiece==0.1.99
pip install accelerate==0.24.1
pip install transformers_stream_generator==0.0.4
```

## æ¨¡å‹ä¸‹è½½

ä½¿ç”¨`Modelscope API` ä¸‹è½½`BlueLM-7B-Chat`æ¨¡å‹ï¼Œæ¨¡å‹è·¯å¾„ä¸º`/root/autodl-tmp`ã€‚åœ¨ /root/autodl-tmp ä¸‹åˆ›å»ºmodel_download.pyæ–‡ä»¶å†…å®¹å¦‚ä¸‹: 

```python
from modelscope import snapshot_download
model_dir = snapshot_download("vivo-ai/BlueLM-7B-Chat", cache_dir='/root/autodl-tmp', revision="master")
```

## ä»£ç å‡†å¤‡

åœ¨`/root/autodl-tmp`è·¯å¾„ä¸‹æ–°å»º `chatBot.py` æ–‡ä»¶å¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼š

```python
# å¯¼å…¥æ‰€éœ€çš„åº“
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, TextStreamer
import torch
import streamlit as st

# åœ¨ä¾§è¾¹æ ä¸­åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªé“¾æ¥
with st.sidebar:
    st.markdown("## BlueLM-7B-Chat")
    "[å¼€æºå¤§æ¨¡å‹é£Ÿç”¨æŒ‡å— self-llm](https://github.com/datawhalechina/self-llm.git)"
    # åˆ›å»ºä¸€ä¸ªæ»‘å—ï¼Œç”¨äºé€‰æ‹©æœ€å¤§é•¿åº¦ï¼ŒèŒƒå›´åœ¨0åˆ°1024ä¹‹é—´ï¼Œé»˜è®¤å€¼ä¸º512
    max_length = st.slider("max_length", 0, 1024, 512, step=1)

# åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªå‰¯æ ‡é¢˜
st.title("ğŸ’¬ BlueLM Chatbot")
st.caption("ğŸš€ A streamlit chatbot powered by Self-LLM")

# å®šä¹‰æ¨¡å‹è·¯å¾„
mode_name_or_path = '/root/autodl-tvivo-ai/BlueLM-7B-Chat'

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè·å–æ¨¡å‹å’Œtokenizer
@st.cache_resource
def get_model():
    # ä»é¢„è®­ç»ƒçš„æ¨¡å‹ä¸­è·å–tokenizer
    tokenizer = AutoTokenizer.from_pretrained(mode_name_or_path, trust_remote_code=True)
    # ä»é¢„è®­ç»ƒçš„æ¨¡å‹ä¸­è·å–æ¨¡å‹ï¼Œå¹¶è®¾ç½®æ¨¡å‹å‚æ•°
    model = AutoModelForCausalLM.from_pretrained(mode_name_or_path, trust_remote_code=True,torch_dtype=torch.bfloat16,  device_map="auto")
    # ä»é¢„è®­ç»ƒçš„æ¨¡å‹ä¸­è·å–ç”Ÿæˆé…ç½®
    model.generation_config = GenerationConfig.from_pretrained(mode_name_or_path)
    # è®¾ç½®ç”Ÿæˆé…ç½®çš„pad_token_idä¸ºç”Ÿæˆé…ç½®çš„eos_token_id
    model.generation_config.pad_token_id = model.generation_config.eos_token_id
    # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()  
    return tokenizer, model

# åŠ è½½BlueLMçš„modelå’Œtokenizer
tokenizer, model = get_model()

def build_prompt(messages, prompt):
    """
    æ„å»ºä¼šè¯æç¤ºä¿¡æ¯ã€‚

    å‚æ•°:
    messages - åŒ…å«ä¼šè¯å†å²çš„å…ƒç»„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç»„æ˜¯ï¼ˆç”¨æˆ·æŸ¥è¯¢ï¼ŒAIå“åº”ï¼‰ã€‚
    prompt - å½“å‰ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬ã€‚

    è¿”å›å€¼:
    res - æ„å»ºå¥½çš„åŒ…å«ä¼šè¯å†å²å’Œå½“å‰ç”¨æˆ·æç¤ºçš„å­—ç¬¦ä¸²ã€‚
    """
    res = ""
    # éå†å†å²æ¶ˆæ¯ï¼Œæ„å»ºä¼šè¯å†å²å­—ç¬¦ä¸²
    for query, response in messages:
        res += f"[|Human|]:{query}[|AI|]:{response}</s>"
    # æ·»åŠ å½“å‰ç”¨æˆ·æç¤º
    res += f"[|Human|]:{prompt}[|AI|]:"
    return res


class BlueLMStreamer(TextStreamer):
    """
    BlueLMæµå¼å¤„ç†ç±»ï¼Œç”¨äºå¤„ç†æ¨¡å‹çš„è¾“å…¥è¾“å‡ºæµã€‚

    å‚æ•°:
    tokenizer - ç”¨äºåˆ†è¯å’Œååˆ†è¯çš„tokenizerå®ä¾‹ã€‚
    """
    def __init__(self, tokenizer: "AutoTokenizer"):
        self.tokenizer = tokenizer
        self.tokenIds = []
        self.prompt = ""
        self.response = ""
        self.first = True

    def put(self, value):
        """
        æ·»åŠ token idåˆ°æµä¸­ã€‚

        å‚æ•°:
        value - è¦æ·»åŠ çš„token idã€‚
        """
        if self.first:
            self.first = False
            return
        self.tokenIds.append(value.item())
        # å°†token idsè§£ç ä¸ºæ–‡æœ¬
        text = tokenizer.decode(self.tokenIds, skip_special_tokens=True)

    def end(self):
        """
        ç»“æŸæµå¤„ç†ï¼Œå°†å½“å‰æµä¸­çš„æ–‡æœ¬ä½œä¸ºå“åº”ï¼Œå¹¶é‡ç½®æµçŠ¶æ€ã€‚
        """
        self.first = True
        # å°†token idsè§£ç ä¸ºæ–‡æœ¬
        text = tokenizer.decode(self.tokenIds, skip_special_tokens=True)
        self.response = text
        self.tokenIds = []



# åˆå§‹åŒ–sessionçŠ¶æ€ï¼Œå¦‚æœmessagesä¸å­˜åœ¨åˆ™åˆå§‹åŒ–ä¸ºç©ºï¼Œå¹¶æ·»åŠ æ¬¢è¿ä¿¡æ¯
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.messages.append(("", "ä½ å¥½ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ å—ï¼Ÿ"))


# éå†å¹¶æ˜¾ç¤ºå†å²æ¶ˆæ¯
for msg in st.session_state.messages:
    st.chat_message("assistant").write(msg[1])


# å¤„ç†ç”¨æˆ·è¾“å…¥
if prompt_text := st.chat_input():
    prompt_text = prompt_text.strip()
    st.chat_message("user").write(prompt_text)
    messages = st.session_state.messages
    # ä½¿ç”¨BlueLMStreamerå¤„ç†æµå¼æ¨¡å‹è¾“å…¥
    streamer = BlueLMStreamer(tokenizer=tokenizer)
    # æ„å»ºå½“å‰ä¼šè¯çš„æç¤ºä¿¡æ¯
    prompt = build_prompt(messages=messages, prompt=prompt_text)
    # å°†æç¤ºä¿¡æ¯ç¼–ç ä¸ºæ¨¡å‹è¾“å…¥
    inputs_tensor = tokenizer(prompt, return_tensors="pt")
    inputs_tensor = inputs_tensor.to("cuda:0")
    input_ids = inputs_tensor["input_ids"]
    # é€šè¿‡æ¨¡å‹ç”Ÿæˆå“åº”
    outputs = model.generate(input_ids=input_ids, max_new_tokens=max_length, streamer=streamer)
    # å°†æ¨¡å‹çš„å“åº”æ˜¾ç¤ºç»™ç”¨æˆ·
    st.chat_message("assistant").write(streamer.response)
    # æ›´æ–°ä¼šè¯å†å²
    st.session_state.messages.append((prompt_text, streamer.response))

```

## è¿è¡Œ demo

åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œå¯åŠ¨streamlitæœåŠ¡ï¼Œå¹¶æŒ‰ç…§ `autodl` çš„æŒ‡ç¤ºå°†ç«¯å£æ˜ å°„åˆ°æœ¬åœ°ï¼Œç„¶ååœ¨æµè§ˆå™¨ä¸­æ‰“å¼€é“¾æ¥ http://localhost:6006/ ï¼Œå³å¯çœ‹åˆ°èŠå¤©ç•Œé¢ã€‚

```bash
streamlit run /root/autodl-tmp/chatBot.py --server.address 127.0.0.1 --server.port 6006
```

å¦‚ä¸‹æ‰€ç¤ºï¼š

![image-20240320215320315](./images/202403202153465.png)
