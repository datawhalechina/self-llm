# BlueLM-7B-Chat langchain æ¥å…¥

## æ¨¡å‹ä»‹ç»

BlueLM-7B æ˜¯ç”± vivo AI å…¨çƒç ”ç©¶é™¢è‡ªä¸»ç ”å‘çš„å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œå‚æ•°è§„æ¨¡ä¸º 70 äº¿ã€‚BlueLM-7B åœ¨ [C-Eval](https://cevalbenchmark.com/index.html) å’Œ [CMMLU](https://github.com/haonan-li/CMMLU) ä¸Šå‡å–å¾—é¢†å…ˆç»“æœï¼Œå¯¹æ¯”åŒå°ºå¯¸å¼€æºæ¨¡å‹ä¸­å…·æœ‰è¾ƒå¼ºçš„ç«äº‰åŠ›(æˆªæ­¢11æœˆ1å·)ã€‚æœ¬æ¬¡å‘å¸ƒå…±åŒ…å« 7B æ¨¡å‹çš„ Base å’Œ Chat ä¸¤ä¸ªç‰ˆæœ¬ã€‚

æ¨¡å‹ä¸‹è½½é“¾æ¥è§ï¼š

|                           åŸºåº§æ¨¡å‹                           |                           å¯¹é½æ¨¡å‹                           |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ğŸ¤— [BlueLM-7B-Base](https://huggingface.co/vivo-ai/BlueLM-7B-Base) | ğŸ¤— [BlueLM-7B-Chat](https://huggingface.co/vivo-ai/BlueLM-7B-Chat) |
| ğŸ¤— [BlueLM-7B-Base-32K](https://huggingface.co/vivo-ai/BlueLM-7B-Base-32K) | ğŸ¤— [BlueLM-7B-Chat-32K](https://huggingface.co/vivo-ai/BlueLM-7B-Chat-32K) |
|                                                              |                  ğŸ¤— [BlueLM-7B-Chat-4bits](https://huggingface.co/vivo-ai/BlueLM-7B-Chat-4bits)                   |

## ç¯å¢ƒå‡†å¤‡

åœ¨ autodl å¹³å°ä¸­ç§Ÿèµä¸€ä¸ª 3090 ç­‰ 24G æ˜¾å­˜çš„æ˜¾å¡æœºå™¨ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºé•œåƒé€‰æ‹© PyTorch-->1.11.0-->3.8(ubuntu20.04)-->11.3ï¼ŒCudaç‰ˆæœ¬åœ¨11.3ä»¥ä¸Šéƒ½å¯ä»¥ã€‚

![](./images/202403191628941.png)

æ¥ä¸‹æ¥æ‰“å¼€åˆšåˆšç§Ÿç”¨æœåŠ¡å™¨çš„ JupyterLab(ä¹Ÿå¯ä»¥ä½¿ç”¨vscode sshè¿œç¨‹è¿æ¥æœåŠ¡å™¨)ï¼Œå¹¶ä¸”æ‰“å¼€å…¶ä¸­çš„ç»ˆç«¯å¼€å§‹ç¯å¢ƒé…ç½®ã€æ¨¡å‹ä¸‹è½½å’Œè¿è¡Œ demoã€‚

pip æ¢æºåŠ é€Ÿä¸‹è½½å¹¶å®‰è£…ä¾èµ–åŒ…

```bash
# å‡çº§pip
python -m pip install --upgrade pip
# è®¾ç½®pipé•œåƒæº
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
# å®‰è£…è½¯ä»¶ä¾èµ–
pip install langchain==0.1.12
pip install modelscope==1.11.0
pip install transformers==4.37.0
pip install sentencepiece==0.1.99
pip install accelerate==0.24.1
pip install transformers_stream_generator==0.0.4
```

## æ¨¡å‹ä¸‹è½½

ä½¿ç”¨`Modelscope API` ä¸‹è½½`BlueLM-7B-Chat`æ¨¡å‹ï¼Œæ¨¡å‹è·¯å¾„ä¸º`/root/autodl-tmp`ã€‚åœ¨ /root/autodl-tmp ä¸‹åˆ›å»ºmodel_download.pyæ–‡ä»¶å†…å®¹å¦‚ä¸‹: 

```python
from modelscope import snapshot_download
model_dir = snapshot_download("vivo-ai/BlueLM-7B-Chat", cache_dir='/root/autodl-tmp', revision="master")
```

## ä»£ç å‡†å¤‡

ä¸ºä¾¿æ·æ„å»º LLM åº”ç”¨ï¼Œæˆ‘ä»¬éœ€è¦åŸºäºæœ¬åœ°éƒ¨ç½²çš„ BlueLMï¼Œè‡ªå®šä¹‰ä¸€ä¸ª LLM ç±»ï¼Œå°† BlueLM æ¥å…¥åˆ° LangChain æ¡†æ¶ä¸­ã€‚å®Œæˆè‡ªå®šä¹‰ LLM ç±»ä¹‹åï¼Œå¯ä»¥ä»¥å®Œå…¨ä¸€è‡´çš„æ–¹å¼è°ƒç”¨ LangChain çš„æ¥å£ï¼Œè€Œæ— éœ€è€ƒè™‘åº•å±‚æ¨¡å‹è°ƒç”¨çš„ä¸ä¸€è‡´ã€‚

åŸºäºæœ¬åœ°éƒ¨ç½²çš„ BlueLM è‡ªå®šä¹‰ LLM ç±»å¹¶ä¸å¤æ‚ï¼Œæˆ‘ä»¬åªéœ€ä» `LangChain.llms.base.LLM` ç±»ç»§æ‰¿ä¸€ä¸ªå­ç±»ï¼Œå¹¶é‡å†™æ„é€ å‡½æ•°ä¸ `_call` å‡½æ•°å³å¯ï¼Œåˆ›å»ºä¸€ä¸ªLLM.pyæ–‡ä»¶å†…å®¹å¦‚ä¸‹ï¼š

```bash
from langchain.llms.base import LLM
from typing import Any, List, Optional
from langchain.callbacks.manager import CallbackManagerForLLMRun
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, LlamaTokenizerFast
import torch

class BlueLM(LLM):
    # åŸºäºæœ¬åœ° BlueLM è‡ªå®šä¹‰ LLM ç±»
    tokenizer: AutoTokenizer = None
    model: AutoModelForCausalLM = None
        
    def __init__(self, mode_name_or_path :str):

        super().__init__()
        print("æ­£åœ¨ä»æœ¬åœ°åŠ è½½æ¨¡å‹...")
        self.tokenizer = AutoTokenizer.from_pretrained(mode_name_or_path, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(mode_name_or_path, torch_dtype=torch.float16, trust_remote_code=True).cuda()
        self.model = self.model.eval()
        print("å®Œæˆæœ¬åœ°æ¨¡å‹çš„åŠ è½½")
        
    def _call(self, prompt : str, stop: Optional[List[str]] = None,
                run_manager: Optional[CallbackManagerForLLMRun] = None,
                **kwargs: Any):

        # é‡å†™è°ƒç”¨å‡½æ•°
        messages = f"[|Human|]:{prompt}[|AI|]:"
        # æ„å»ºè¾“å…¥
        inputs_tensor = self.tokenizer(messages, return_tensors="pt")
        inputs_tensor = inputs_tensor.to("cuda:0")
        # é€šè¿‡æ¨¡å‹è·å¾—è¾“å‡º
        outputs = self.model.generate(**inputs_tensor, max_new_tokens=100)
        response = self.tokenizer.decode(outputs.cpu()[0], skip_special_tokens=True)
    
        return response
    @property
    def _llm_type(self) -> str:
        return "BlueLM"

```

## è°ƒç”¨

ç„¶åå°±å¯ä»¥åƒä½¿ç”¨ä»»ä½•å…¶ä»–çš„langchainå¤§æ¨¡å‹åŠŸèƒ½ä¸€æ ·ä½¿ç”¨äº†ã€‚

```python
from LLM import BlueLM
llm = BlueLM('/root/autodl-tmp/vivo-ai/BlueLM-7B-Chat')

llm('ä½ å¥½')
```

å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![image-20240320122929440](./images/202403201229542.png)
