# 04-GLM-4-9B-Chat vLLM éƒ¨ç½²è°ƒç”¨

## **vLLM ç®€ä»‹**

vLLM æ¡†æ¶æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰**æ¨ç†å’Œéƒ¨ç½²æœåŠ¡ç³»ç»Ÿ**ï¼Œå…·å¤‡ä»¥ä¸‹ç‰¹æ€§ï¼š

- **é«˜æ•ˆçš„å†…å­˜ç®¡ç†**ï¼šé€šè¿‡ PagedAttention ç®—æ³•ï¼ŒvLLM å®ç°äº†å¯¹ KV ç¼“å­˜çš„é«˜æ•ˆç®¡ç†ï¼Œå‡å°‘äº†å†…å­˜æµªè´¹ï¼Œä¼˜åŒ–äº†æ¨¡å‹çš„è¿è¡Œæ•ˆç‡ã€‚
- **é«˜ååé‡**ï¼švLLM æ”¯æŒå¼‚æ­¥å¤„ç†å’Œè¿ç»­æ‰¹å¤„ç†è¯·æ±‚ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹æ¨ç†çš„ååé‡ï¼ŒåŠ é€Ÿäº†æ–‡æœ¬ç”Ÿæˆå’Œå¤„ç†é€Ÿåº¦ã€‚
- **æ˜“ç”¨æ€§**ï¼švLLM ä¸ HuggingFace æ¨¡å‹æ— ç¼é›†æˆï¼Œæ”¯æŒå¤šç§æµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç®€åŒ–äº†æ¨¡å‹éƒ¨ç½²å’Œæ¨ç†çš„è¿‡ç¨‹ã€‚å…¼å®¹ OpenAI çš„ API æœåŠ¡å™¨ã€‚
- **åˆ†å¸ƒå¼æ¨ç†**ï¼šæ¡†æ¶æ”¯æŒåœ¨å¤š GPU ç¯å¢ƒä¸­è¿›è¡Œåˆ†å¸ƒå¼æ¨ç†ï¼Œé€šè¿‡æ¨¡å‹å¹¶è¡Œç­–ç•¥å’Œé«˜æ•ˆçš„æ•°æ®é€šä¿¡ï¼Œæå‡äº†å¤„ç†å¤§å‹æ¨¡å‹çš„èƒ½åŠ›ã€‚
- **å¼€æº**ï¼švLLM æ˜¯å¼€æºçš„ï¼Œæ‹¥æœ‰æ´»è·ƒçš„ç¤¾åŒºæ”¯æŒï¼Œä¾¿äºå¼€å‘è€…è´¡çŒ®å’Œæ”¹è¿›ï¼Œå…±åŒæ¨åŠ¨æŠ€æœ¯å‘å±•ã€‚

## **ç¯å¢ƒå‡†å¤‡**

åœ¨ Autodl å¹³å°ä¸­ç§Ÿèµä¸€ä¸ª 3090 ç­‰ 24G æ˜¾å­˜çš„æ˜¾å¡æœºå™¨ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºé•œåƒé€‰æ‹© PyTorch-->2.1.0-->3.10(ubuntu22.04)-->12.1

æ¥ä¸‹æ¥æ‰“å¼€åˆšåˆšç§Ÿç”¨æœåŠ¡å™¨çš„ JupyterLabï¼Œå¹¶ä¸”æ‰“å¼€å…¶ä¸­çš„ç»ˆç«¯å¼€å§‹ç¯å¢ƒé…ç½®ã€æ¨¡å‹ä¸‹è½½å’Œè¿è¡Œæ¼”ç¤ºã€‚

![](images/image04-1.png)

pip æ¢æºåŠ é€Ÿä¸‹è½½å¹¶å®‰è£…ä¾èµ–åŒ…

```bash
# å‡çº§pip
python -m pip install --upgrade pip
# æ›´æ¢ pypi æºåŠ é€Ÿåº“çš„å®‰è£…
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

pip install modelscope==1.11.0
pip install openai==1.17.1
pip install torch==2.1.2+cu121
pip install tqdm==4.64.1
pip install transformers==4.39.3
# ä¸‹è½½flash-attn è¯·ç­‰å¾…å¤§çº¦10åˆ†é’Ÿå·¦å³~
MAX_JOBS=8 pip install flash-attn --no-build-isolation
pip install vllm==0.4.0.post1
```

ç›´æ¥å®‰è£… vLLM ä¼šå®‰è£… CUDA 12.1 ç‰ˆæœ¬ã€‚

```bash
pip install vllm
```

å¦‚æœæˆ‘ä»¬éœ€è¦åœ¨ CUDA 11.8 çš„ç¯å¢ƒä¸‹å®‰è£… vLLMï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼ŒæŒ‡å®š vLLM ç‰ˆæœ¬å’Œ python ç‰ˆæœ¬ä¸‹è½½ã€‚

```bash
export VLLM_VERSION=0.4.0
export PYTHON_VERSION=38
pip install https://github.com/vllm-project/vllm/releases/download/v${VLLM_VERSION}/vllm-${VLLM_VERSION}+cu118-cp${PYTHON_VERSION}-cp${PYTHON_VERSION}-manylinux1_x86_64.whl --extra-index-url https://download.pytorch.org/whl/cu118
```

> è€ƒè™‘åˆ°éƒ¨åˆ†åŒå­¦é…ç½®ç¯å¢ƒå¯èƒ½ä¼šé‡åˆ°ä¸€äº›é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨ AutoDL å¹³å°å‡†å¤‡äº† vLLM çš„ç¯å¢ƒé•œåƒï¼Œè¯¥é•œåƒé€‚ç”¨äºä»»ä½•éœ€è¦ vLLM çš„éƒ¨ç½²ç¯å¢ƒã€‚ç‚¹å‡»ä¸‹æ–¹é“¾æ¥å¹¶ç›´æ¥åˆ›å»º AutoDL ç¤ºä¾‹å³å¯ã€‚ï¼ˆvLLM å¯¹ torch ç‰ˆæœ¬è¦æ±‚è¾ƒé«˜ï¼Œä¸”è¶Šé«˜çš„ç‰ˆæœ¬å¯¹æ¨¡å‹çš„æ”¯æŒæ›´å…¨ï¼Œæ•ˆæœæ›´å¥½ï¼Œæ‰€ä»¥æ–°å»ºä¸€ä¸ªå…¨æ–°çš„é•œåƒã€‚ï¼‰ **https://www.codewithgpu.com/i/datawhalechina/self-llm/GLM-4**

ä½¿ç”¨ modelscope ä¸­çš„ snapshot_download å‡½æ•°ä¸‹è½½æ¨¡å‹ï¼Œç¬¬ä¸€ä¸ªå‚æ•°ä¸ºæ¨¡å‹åç§°ï¼Œå‚æ•° cache_dir ä¸ºæ¨¡å‹çš„ä¸‹è½½è·¯å¾„ã€‚

åœ¨ /root/autodl-tmp è·¯å¾„ä¸‹æ–°å»º model_download.py æ–‡ä»¶å¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼Œç²˜è´´ä»£ç åè¯·åŠæ—¶ä¿å­˜æ–‡ä»¶ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å¹¶è¿è¡Œ `python /root/autodl-tmp/model_download.py` æ‰§è¡Œä¸‹è½½ï¼Œæ¨¡å‹å¤§å°ä¸º 14GBï¼Œä¸‹è½½æ¨¡å‹å¤§æ¦‚éœ€è¦ 2 åˆ†é’Ÿã€‚

```bash
import torch 
from modelscope import snapshot_download, AutoModel, AutoTokenizer
osmodel_dir = snapshot_download('ZhipuAI/glm-4-9b-chat', cache_dir='/root/autodl-tmp', revision='master')
```

## **ä»£ç å‡†å¤‡**

### **python æ–‡ä»¶**

åœ¨ /root/autodl-tmp è·¯å¾„ä¸‹æ–°å»º vllm_model.py æ–‡ä»¶å¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼Œç²˜è´´ä»£ç åè¯·åŠæ—¶ä¿å­˜æ–‡ä»¶ã€‚ä¸‹é¢çš„ä»£ç æœ‰å¾ˆè¯¦ç»†çš„æ³¨é‡Šï¼Œå¤§å®¶å¦‚æœ‰ä¸ç†è§£çš„åœ°æ–¹ï¼Œæ¬¢è¿æå‡º issueã€‚

é¦–å…ˆä» vLLM åº“ä¸­å¯¼å…¥ LLM å’Œ SamplingParams ç±»ã€‚`LLM` ç±»æ˜¯ä½¿ç”¨ vLLM å¼•æ“è¿è¡Œç¦»çº¿æ¨ç†çš„ä¸»è¦ç±»ã€‚`SamplingParams` ç±»æŒ‡å®šé‡‡æ ·è¿‡ç¨‹çš„å‚æ•°ï¼Œç”¨äºæ§åˆ¶å’Œè°ƒæ•´ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§å’Œå¤šæ ·æ€§ã€‚

vLLM æä¾›äº†éå¸¸æ–¹ä¾¿çš„å°è£…ï¼Œæˆ‘ä»¬ç›´æ¥ä¼ å…¥æ¨¡å‹åç§°æˆ–æ¨¡å‹è·¯å¾„å³å¯ï¼Œä¸å¿…æ‰‹åŠ¨åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡è¿™ä¸ª demo ç†Ÿæ‚‰ä¸‹ vLLM å¼•æ“çš„ä½¿ç”¨æ–¹å¼ã€‚è¢«æ³¨é‡Šçš„éƒ¨åˆ†å†…å®¹å¯ä»¥ä¸°å¯Œæ¨¡å‹çš„èƒ½åŠ›ï¼Œä½†ä¸æ˜¯å¿…è¦çš„ï¼Œå¤§å®¶å¯ä»¥æŒ‰éœ€é€‰æ‹©ã€‚

```python
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer
import os
import json

# è‡ªåŠ¨ä¸‹è½½æ¨¡å‹æ—¶ï¼ŒæŒ‡å®šä½¿ç”¨modelscopeã€‚ä¸è®¾ç½®çš„è¯ï¼Œä¼šä» huggingface ä¸‹è½½
# os.environ['VLLM_USE_MODELSCOPE']='True'

def get_completion(prompts, model, tokenizer=None, max_tokens=512, temperature=0.8, top_p=0.95, max_model_len=2048):
    stop_token_ids = [151329, 151336, 151338]
    # åˆ›å»ºé‡‡æ ·å‚æ•°ã€‚temperature æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§ï¼Œtop_p æ§åˆ¶æ ¸å¿ƒé‡‡æ ·çš„æ¦‚ç‡
    sampling_params = SamplingParams(temperature=temperature, top_p=top_p, max_tokens=max_tokens, stop_token_ids=stop_token_ids)
    # åˆå§‹åŒ– vLLM æ¨ç†å¼•æ“
    llm = LLM(model=model, tokenizer=tokenizer, max_model_len=max_model_len,trust_remote_code=True)
    outputs = llm.generate(prompts, sampling_params)
    return outputs


if __name__ == "__main__":    
    # åˆå§‹åŒ– vLLM æ¨ç†å¼•æ“
    model='/root/autodl-tmp/ZhipuAI/glm-4-9b-chat' # æŒ‡å®šæ¨¡å‹è·¯å¾„
    # model="THUDM/glm-4-9b-chat" # æŒ‡å®šæ¨¡å‹åç§°ï¼Œè‡ªåŠ¨ä¸‹è½½æ¨¡å‹
    tokenizer = None
    # tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False) # åŠ è½½åˆ†è¯å™¨åä¼ å…¥vLLM æ¨¡å‹ï¼Œä½†ä¸æ˜¯å¿…è¦çš„ã€‚
    
    text = ["ç»™æˆ‘ä»‹ç»ä¸€ä¸‹å¤§å‹è¯­è¨€æ¨¡å‹ã€‚",
           "å‘Šè¯‰æˆ‘å¦‚ä½•å˜å¼ºã€‚"]
    # messages = [
    #     {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚"},
    #     {"role": "user", "content": prompt}
    # ]
    # ä½œä¸ºèŠå¤©æ¨¡æ¿çš„æ¶ˆæ¯ï¼Œä¸æ˜¯å¿…è¦çš„ã€‚
    # text = tokenizer.apply_chat_template(
    #     messages,
    #     tokenize=False,
    #     add_generation_prompt=True
    # )

    outputs = get_completion(text, model, tokenizer=tokenizer, max_tokens=512, temperature=1, top_p=1, max_model_len=2048)

    # è¾“å‡ºæ˜¯ä¸€ä¸ªåŒ…å« promptã€ç”Ÿæˆæ–‡æœ¬å’Œå…¶ä»–ä¿¡æ¯çš„ RequestOutput å¯¹è±¡åˆ—è¡¨ã€‚
    # æ‰“å°è¾“å‡ºã€‚
    for output in outputs:
        prompt = output.prompt
        generated_text = output.outputs[0].text
        print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
```
ç»“æœå¦‚ä¸‹ï¼š
```bash
Prompt: 'ç»™æˆ‘ä»‹ç»ä¸€ä¸‹å¤§å‹è¯­è¨€æ¨¡å‹ã€‚', Generated text: 'å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ï¼Œå®ƒèƒ½å¤Ÿç†è§£ã€ç”Ÿæˆå’Œç¿»è¯‘è‡ªç„¶è¯­è¨€ï¼Œä¸ºç”¨æˆ·æä¾›ä¸°å¯Œçš„è¯­è¨€äº¤äº’ä½“éªŒã€‚\n\nä»¥ä¸‹æ˜¯å…³äºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸€äº›å…³é”®ä¿¡æ¯ï¼š\n\n### 1. è®¾è®¡ç›®æ ‡\n\n- **ç†è§£è‡ªç„¶è¯­è¨€**ï¼šèƒ½å¤Ÿç†è§£ç”¨æˆ·çš„æ„å›¾å’Œéœ€æ±‚ï¼Œå¹¶è¿›è¡Œç›¸åº”çš„å“åº”ã€‚\n- **ç”Ÿæˆè‡ªç„¶è¯­è¨€**ï¼šæ ¹æ®è¾“å…¥ç”Ÿæˆç¬¦åˆè¯­æ³•å’Œè¯­ä¹‰çš„è‡ªç„¶è¯­è¨€å›ç­”ã€‚\n- **ç¿»è¯‘è‡ªç„¶è¯­è¨€**ï¼šåœ¨å¤šç§è¯­è¨€ä¹‹é—´è¿›è¡Œæµç•…çš„ç¿»è¯‘ã€‚\n\n### 2. æŠ€æœ¯æ¶æ„\n\n- **æ·±åº¦å­¦ä¹ **ï¼šé‡‡ç”¨ç¥ç»ç½‘ç»œæŠ€æœ¯å¯¹å¤§é‡æ–‡æœ¬æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä»¥å®ç°æ¨¡å‹çš„ç†è§£ã€ç”Ÿæˆå’Œç¿»è¯‘èƒ½åŠ›ã€‚\n- **é¢„è®­ç»ƒ**ï¼šé€šè¿‡åœ¨å¤§é‡æ— æ ‡æ³¨æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä½¿æ¨¡å‹å…·å¤‡åˆæ­¥çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚\n- **å¾®è°ƒ**ï¼šåœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒï¼Œè¿›ä¸€æ­¥æå‡æ¨¡å‹åœ¨è¯¥ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚\n\n### 3. åº”ç”¨åœºæ™¯\n\n- **é—®ç­”ç³»ç»Ÿ**ï¼šå¦‚æœç´¢å¼•æ“ã€æ™ºèƒ½å®¢æœã€çŸ¥è¯†é—®ç­”ç­‰ã€‚\n- **æ–‡æœ¬ç”Ÿæˆ**ï¼šå¦‚å†…å®¹åˆ›ä½œã€æ‘˜è¦ç”Ÿæˆã€å¯¹è¯ç”Ÿæˆç­‰ã€‚\n- **æœºå™¨ç¿»è¯‘**ï¼šåœ¨è·¨è¯­è¨€äº¤æµã€å…¨çƒç”µå•†ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚\n\n### 4. å‘å±•è¶‹åŠ¿\n\n- **æ¨¡å‹è§„æ¨¡ä¸æ–­æ‰©å¤§**ï¼šéšç€è®¡ç®—åŠ›çš„æå‡ï¼Œæ¨¡å‹è§„æ¨¡å°†é€æ¸å¢å¤§ï¼Œæ€§èƒ½ä¹Ÿå°†ç›¸åº”æå‡ã€‚\n- **å¤šæ¨¡æ€èåˆ**ï¼šå°†è‡ªç„¶è¯­è¨€å¤„ç†ä¸å›¾åƒã€è¯­éŸ³ç­‰å…¶ä»–æ¨¡æ€ä¿¡æ¯èåˆï¼Œå®ç°æ›´å…¨é¢çš„äº¤äº’ä½“éªŒã€‚\n- **å¯è§£é‡Šæ€§å’Œå…¬å¹³æ€§**ï¼šå…³æ³¨æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œå…¬å¹³æ€§ï¼Œæé«˜ç”¨æˆ·å¯¹æ¨¡å‹çš„ä¿¡ä»»ã€‚\n\nå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ”¹å–„äººæœºäº¤äº’ã€ä¿ƒè¿›ä¿¡æ¯ä¼ æ’­ç­‰æ–¹é¢å…·æœ‰é‡è¦ä½œç”¨ã€‚\n\nå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelsï¼ŒLLMsï¼‰æ˜¯ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNatural Language Processingï¼ŒNLPï¼‰æŠ€æœ¯ï¼Œå…¶æ ¸å¿ƒèƒ½åŠ›åœ¨äºç†è§£å’Œç”Ÿæˆè‡ªç„¶è¯­è¨€ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å…³äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å…³é”®ç‚¹ï¼š\n\n1. **æ¨¡å‹è§„æ¨¡**ï¼šå¤§å‹è¯­è¨€æ¨¡å‹çš„è§„æ¨¡å·¨å¤§ï¼Œé€šå¸¸åŒ…å«æ•°åäº¿è‡³æ•°åƒäº¿ä¸ªå‚æ•°ã€‚ä¾‹å¦‚ï¼ŒGPT-3æ‹¥æœ‰1750äº¿ä¸ªå‚æ•°ï¼Œæ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„è¯­è¨€æ¨¡å‹ã€‚\n\n2. **è®­ç»ƒæ•°æ®**ï¼šè¿™äº›æ¨¡å‹é€šå¸¸åœ¨å¤§é‡æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒ…æ‹¬ä¹¦ç±ã€æ–°é—»ã€ç½‘é¡µç­‰ã€‚è¿™äº›æ•°æ®å¸®åŠ©æ¨¡å‹å­¦ä¹ è¯­è¨€çš„é«˜çº§ç‰¹æ€§ï¼Œæ¯”å¦‚è¯­æ³•ã€è¯­ä¹‰å’Œé£æ ¼ã€‚\n\n3. **ç”Ÿæˆèƒ½åŠ›**ï¼šå¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ç»™å®šæ–‡æœ¬æˆ–ä¸Šä¸‹æ–‡è‡ªä¸»ç”Ÿæˆè¿è´¯ã€å¯Œæœ‰é€»è¾‘æ€§çš„æ–‡æœ¬ã€‚è¿™ç§èƒ½åŠ›åœ¨æ–‡æœ¬ç”Ÿæˆã€æ‘˜è¦ã€æœºå™¨ç¿»è¯‘ç­‰é¢†åŸŸæœ‰å¹¿æ³›çš„åº”ç”¨ã€‚\n\n4. **é€‚åº”èƒ½åŠ›**ï¼šå°½ç®¡è¿™äº›æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆ'


Prompt: 'å‘Šè¯‰æˆ‘å¦‚ä½•å˜å¼ºã€‚', Generated text: '\nå˜å¼ºæ˜¯ä¸€ä¸ªå…¨é¢çš„æå‡è¿‡ç¨‹ï¼Œå®ƒåŒ…æ‹¬èº«ä½“ã€å¿ƒç†å’Œç²¾ç¥çš„å„ä¸ªæ–¹é¢ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å»ºè®®ï¼Œå¸Œæœ›èƒ½å¸®åŠ©ä½ å®ç°å˜å¼ºçš„ç›®æ ‡ï¼š\n\n1. **èº«ä½“å¼ºåŒ–**ï¼š\n   - **è§„å¾‹é”»ç‚¼**ï¼šåˆ¶å®šä¸€ä¸ªåˆç†çš„è®­ç»ƒè®¡åˆ’ï¼ŒåŒ…æ‹¬æœ‰æ°§è¿åŠ¨ï¼ˆå¦‚è·‘æ­¥ã€æ¸¸æ³³ï¼‰å’Œé‡é‡è®­ç»ƒï¼Œå¯ä»¥æé«˜èº«ä½“çš„è€åŠ›å’ŒåŠ›é‡ã€‚\n   - **è¥å…»é¥®é£Ÿ**ï¼šå¥åº·é¥®é£Ÿå¯¹èº«ä½“çš„æ¢å¤å’Œç”Ÿé•¿è‡³å…³é‡è¦ã€‚ä¿è¯æ‘„å…¥è¶³å¤Ÿçš„è›‹ç™½è´¨ã€ç¢³æ°´åŒ–åˆç‰©ã€è„‚è‚ªå’Œå¾®é‡å…ƒç´ ã€‚\n   - **å……è¶³ç¡çœ **ï¼šä¿è¯æ¯æ™š7-9å°æ—¶çš„é«˜è´¨é‡ç¡çœ ï¼Œæœ‰åŠ©äºèº«ä½“æ¢å¤å’Œå†…åˆ†æ³Œå¹³è¡¡ã€‚\n\n2. **å¿ƒç†è°ƒé€‚**ï¼š\n   - **æ­£é¢æ€è€ƒ**ï¼šä¿æŒç§¯æçš„å¿ƒæ€ï¼Œé¢å¯¹æŒ‘æˆ˜æ—¶ï¼Œé€‚æ—¶è°ƒæ•´è‡ªå·±çš„å¿ƒæ€ï¼Œé¿å…æ¶ˆææƒ…ç»ªçš„å½±å“ã€‚\n   - **æŠ—å‹èƒ½åŠ›**ï¼šåŸ¹å…»é¢å¯¹æŒ«æŠ˜å’Œå‹åŠ›çš„èƒ½åŠ›ï¼Œå¯ä»¥é€šè¿‡å†¥æƒ³ã€ç‘œä¼½ç­‰æ–¹å¼æ¥å¢å¼ºæƒ…ç»ªç®¡ç†èƒ½åŠ›ã€‚\n\n3. **ç²¾ç¥æˆé•¿**ï¼š\n   - **æŒç»­å­¦ä¹ **ï¼šé€šè¿‡é˜…è¯»ã€ä¸Šè¯¾ã€å‚åŠ è®¨è®ºç­‰æ–¹å¼ï¼Œä¸æ–­ä¸°å¯Œè‡ªå·±çš„çŸ¥è¯†å’Œè§†é‡ã€‚\n   - **ç›®æ ‡è®¾å®š**ï¼šæ˜ç¡®è‡ªå·±çš„é•¿æœŸå’ŒçŸ­æœŸç›®æ ‡ï¼Œå¹¶åˆ¶å®šå®ç°è¿™äº›ç›®æ ‡çš„è®¡åˆ’ã€‚\n\nå…·ä½“æªæ–½å¦‚ä¸‹ï¼š\n\n- **åˆ¶å®šè®¡åˆ’**ï¼šæ ¹æ®è‡ªèº«æƒ…å†µåˆ¶å®šè¯¦ç»†çš„è®¡åˆ’ï¼Œå¦‚æ¯å‘¨é”»ç‚¼å‡ æ¬¡ï¼Œæ¯æ¬¡é”»ç‚¼å¤šé•¿æ—¶é—´ç­‰ã€‚\n- **è·Ÿè¸ªè¿›åº¦**ï¼šè®°å½•è‡ªå·±çš„è®­ç»ƒå’Œé¥®é£Ÿï¼Œå®šæœŸè¿›è¡Œè‡ªæˆ‘è¯„ä¼°ï¼Œè°ƒæ•´è®¡åˆ’ã€‚\n- **å¯»æ±‚å¸®åŠ©**ï¼šå¦‚æœæ¡ä»¶å…è®¸ï¼Œå¯ä»¥è˜è¯·ä¸“ä¸šæ•™ç»ƒè¿›è¡ŒæŒ‡å¯¼ã€‚\n- **ä¿æŒè€å¿ƒ**ï¼šå˜å¼ºæ˜¯ä¸€ä¸ªé•¿æœŸçš„è¿‡ç¨‹ï¼Œéœ€è¦è€å¿ƒå’Œæ¯…åŠ›ã€‚\n\nè¯·æ ¹æ®è‡ªå·±çš„å®é™…æƒ…å†µï¼Œæœ‰é€‰æ‹©æ€§åœ°é‡‡çº³è¿™äº›å»ºè®®ï¼Œä¸æ–­åŠªåŠ›ï¼Œä½ ä¼šè¶Šæ¥è¶Šå¼ºçš„ã€‚åŠ æ²¹ï¼ğŸŒŸğŸ’ªâœŠğŸ¼ğŸƒ\u200dâ™€ï¸ğŸƒ\u200dâ™‚ï¸ğŸ‘Š\u200dâ™€ï¸ğŸ‘Š\u200dâ™‚ï¸ğŸ‹ï¸\u200dâ™€ï¸ğŸ‹ï¸\u200dâ™‚ï¸ğŸ¥—ğŸğŸ’¤ğŸ“šğŸ’¼ğŸ¥¼ğŸ§˜\u200dâ™€ï¸ğŸ§˜\u200dâ™‚ï¸ğŸ¯ğŸ¯ğŸ¯ã€‚ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰'
```

### **åˆ›å»ºå…¼å®¹ OpenAI API æ¥å£çš„æœåŠ¡å™¨**

GLM4 å…¼å®¹ OpenAI API åè®®ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨ vLLM åˆ›å»º OpenAI API æœåŠ¡å™¨ã€‚vLLM éƒ¨ç½²å®ç° OpenAI API åè®®çš„æœåŠ¡å™¨éå¸¸æ–¹ä¾¿ã€‚é»˜è®¤ä¼šåœ¨ <u>http://localhost:8000</u> å¯åŠ¨æœåŠ¡å™¨ã€‚æœåŠ¡å™¨å½“å‰ä¸€æ¬¡æ‰˜ç®¡ä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶å®ç°åˆ—è¡¨æ¨¡å‹ã€completions å’Œ chat completions ç«¯å£ã€‚

- completionsï¼šæ˜¯åŸºæœ¬çš„æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œæ¨¡å‹ä¼šåœ¨ç»™å®šçš„æç¤ºåç”Ÿæˆä¸€æ®µæ–‡æœ¬ã€‚è¿™ç§ç±»å‹çš„ä»»åŠ¡é€šå¸¸ç”¨äºç”Ÿæˆæ–‡ç« ã€æ•…äº‹ã€é‚®ä»¶ç­‰ã€‚
- chat completionsï¼šæ˜¯é¢å‘å¯¹è¯çš„ä»»åŠ¡ï¼Œæ¨¡å‹éœ€è¦ç†è§£å’Œç”Ÿæˆå¯¹è¯ã€‚è¿™ç§ç±»å‹çš„ä»»åŠ¡é€šå¸¸ç”¨äºæ„å»ºèŠå¤©æœºå™¨äººæˆ–è€…å¯¹è¯ç³»ç»Ÿã€‚

åœ¨åˆ›å»ºæœåŠ¡å™¨æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‡å®šæ¨¡å‹åç§°ã€æ¨¡å‹è·¯å¾„ã€èŠå¤©æ¨¡æ¿ç­‰å‚æ•°ã€‚

- --host å’Œ --port å‚æ•°æŒ‡å®šåœ°å€ã€‚
- --model å‚æ•°æŒ‡å®šæ¨¡å‹åç§°ã€‚
- --chat-template å‚æ•°æŒ‡å®šèŠå¤©æ¨¡æ¿ã€‚
- --served-model-name æŒ‡å®šæœåŠ¡æ¨¡å‹çš„åç§°ã€‚
- --max-model-len æŒ‡å®šæ¨¡å‹çš„æœ€å¤§é•¿åº¦ã€‚

è¿™é‡ŒæŒ‡å®š `--max-model-len=2048` æ˜¯å› ä¸º GLM4-9b-Chat æ¨¡å‹çš„æœ€å¤§é•¿åº¦è¿‡é•¿ 128Kï¼Œå¯¼è‡´ vLLM åˆå§‹åŒ– KV ç¼“å­˜æ—¶æ¶ˆè€—èµ„æºè¿‡å¤§ã€‚

```bash
python -m vllm.entrypoints.openai.api_server --model /root/autodl-tmp/ZhipuAI/glm-4-9b-chat  --served-model-name glm-4-9b-chat --max-model-len=2048 --trust-remote-code
```

1. é€šè¿‡ curl å‘½ä»¤æŸ¥çœ‹å½“å‰çš„æ¨¡å‹åˆ—è¡¨ã€‚

```bash
curl http://localhost:8000/v1/models
```

å¾—åˆ°çš„è¿”å›å€¼å¦‚ä¸‹æ‰€ç¤ºï¼š

```bash
{"object":"list","data":[{"id":"glm-4-9b-chat","object":"model","created":1717567231,"owned_by":"vllm","root":"glm-4-9b-chat","parent":null,"permission":[{"id":"modelperm-4fdf01c1999f4df1a0fe8ef96fd07c2f","object":"model_permission","created":1717567231,"allow_create_engine":false,"allow_sampling":true,"allow_logprobs":true,"allow_search_indices":false,"allow_view":true,"allow_fine_tuning":false,"organization":"*","group":null,"is_blocking":false}]}]}
```

1. ä½¿ç”¨ curl å‘½ä»¤æµ‹è¯• OpenAI Completions API ã€‚

```bash
curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
            "model": "glm-4-9b-chat",
            "prompt": "ä½ å¥½",        
            "max_tokens": 7,        
            "temperature": 0    
         }'
```

å¾—åˆ°çš„è¿”å›å€¼å¦‚ä¸‹æ‰€ç¤ºï¼š

```bash
{"id":"cmpl-8bba2df7cfa1400da705c58946389cc1","object":"text_completion","created":1717568865,"model":"glm-4-9b-chat","choices":[{"index":0,"text":"ï¼Œè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„ï¼Ÿæ‚¨å¥½","logprobs":null,"finish_reason":"length","stop_reason":null}],"usage":{"prompt_tokens":3,"total_tokens":10,"completion_tokens":7}}
```

ä¹Ÿå¯ä»¥ç”¨ python è„šæœ¬è¯·æ±‚ OpenAI Completions API ã€‚è¿™é‡Œé¢è®¾ç½®äº†é¢å¤–å‚æ•° `extra_body`ï¼Œæˆ‘ä»¬ä¼ å…¥äº† `stop_token_ids` åœæ­¢è¯ idã€‚å½“ openai api æ— æ³•æ»¡è¶³æ—¶å¯ä»¥é‡‡ç”¨ vllm å®˜æ–¹æ–‡æ¡£æ–¹å¼æ·»åŠ ã€‚https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html

```python
from openai import OpenAI
client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="token-abc123", # éšä¾¿è®¾ï¼Œåªæ˜¯ä¸ºäº†é€šè¿‡æ¥å£å‚æ•°æ ¡éªŒ
)

completion = client.chat.completions.create(
  model="glm-4-9b-chat",
  messages=[
    {"role": "user", "content": "ä½ å¥½"}
  ],
  # è®¾ç½®é¢å¤–å‚æ•°
  extra_body={
    "stop_token_ids": [151329, 151336, 151338]
  }
)

print(completion.choices[0].message)
```

å¾—åˆ°çš„è¿”å›å€¼å¦‚ä¸‹æ‰€ç¤ºï¼š

```bash
ChatCompletionMessage(content='\nä½ å¥½ğŸ‘‹ï¼å¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ', role='assistant', function_call=None, tool_calls=None)
```

1. ç”¨ curl å‘½ä»¤æµ‹è¯• OpenAI Chat Completions API ã€‚

```bash
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{        
            "model": "glm-4-9b-chat",
            "messages": [            
                  {"role": "system", "content": "You are a helpful assistant."},
                  {"role": "user", "content": "ä½ å¥½"}
            ],
            "max_tokens": 7,        
            "temperature": 0 
            
         }'
```

å¾—åˆ°çš„è¿”å›å€¼å¦‚ä¸‹æ‰€ç¤ºï¼š

```bash
{"id":"cmpl-8b02ae787c7747ecaf1fb6f72144b798","object":"chat.completion","created":1717569334,"model":"glm-4-9b-chat","choices":[{"index":0,"message":{"role":"assistant","content":"\nä½ å¥½ğŸ‘‹ï¼å¾ˆé«˜å…´"},"logprobs":null,"finish_reason":"length","stop_reason":null}],"usage":{"prompt_tokens":16,"total_tokens":23,"completion_tokens":7}
```

ä¹Ÿå¯ä»¥ç”¨ python è„šæœ¬è¯·æ±‚ OpenAI Chat Completions API ã€‚

```python
from openai import OpenAIopenai_api_key = "EMPTY" # éšä¾¿è®¾ï¼Œåªæ˜¯ä¸ºäº†é€šè¿‡æ¥å£å‚æ•°æ ¡éªŒopenai_api_base = "http://localhost:8000/v1"client = OpenAI(
    api_key=openai_api_key,
    base_url=openai_api_base,
)

chat_outputs = client.chat.completions.create(
    model="glm-4-9b-chat",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "ä½ å¥½"},
    ],
      # è®¾ç½®é¢å¤–å‚æ•°
      extra_body={
        "stop_token_ids": [151329, 151336, 151338]
  }
)
print(chat_outputs)
```

å¾—åˆ°çš„è¿”å›å€¼å¦‚ä¸‹æ‰€ç¤ºï¼š

```bash
ChatCompletion(id='cmpl-16b1c36dc695426cacee23b79d179d52', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='\nä½ å¥½ğŸ‘‹ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ', role='assistant', function_call=None, tool_calls=None), stop_reason=151336)], created=1717569519, model='glm-4-9b-chat', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=12, prompt_tokens=16, total_tokens=28))
```

åœ¨å¤„ç†è¯·æ±‚æ—¶ API åç«¯ä¹Ÿä¼šæ‰“å°ä¸€äº›æ—¥å¿—å’Œç»Ÿè®¡ä¿¡æ¯ã€‚

## **é€Ÿåº¦æµ‹è¯•**

æ—¢ç„¶è¯´ vLLM æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†å’Œéƒ¨ç½²æœåŠ¡ç³»ç»Ÿï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±æ¥æµ‹è¯•ä¸€ä¸‹æ¨¡å‹çš„ç”Ÿæˆé€Ÿåº¦ã€‚çœ‹çœ‹å’ŒåŸå§‹çš„é€Ÿåº¦æœ‰å¤šå¤§çš„å·®è·ã€‚è¿™é‡Œç›´æ¥ä½¿ç”¨ vLLM è‡ªå¸¦çš„ benchmark_throughput.py è„šæœ¬è¿›è¡Œæµ‹è¯•ã€‚å¯ä»¥å°†å½“å‰æ–‡ä»¶å¤¹ benchmark_throughput.py è„šæœ¬æ”¾åœ¨ /root/autodl-tmp/ ä¸‹ã€‚æˆ–è€…å¤§å®¶å¯ä»¥è‡ªè¡Œ<u>ä¸‹è½½è„šæœ¬</u>ã€‚

ä¸‹é¢æ˜¯ä¸€äº› benchmark_throughput.py è„šæœ¬çš„å‚æ•°è¯´æ˜ï¼š

- --model å‚æ•°æŒ‡å®šæ¨¡å‹è·¯å¾„æˆ–åç§°ã€‚
- --backend æ¨ç†åç«¯ï¼Œå¯ä»¥æ˜¯ vllmã€hf å’Œ miiã€‚åˆ†å¸ƒå¯¹åº” vLLMã€HuggingFace å’Œ Mii æ¨ç†åç«¯ã€‚
- --input-len è¾“å…¥é•¿åº¦
- --output-len è¾“å‡ºé•¿åº¦
- --num-prompts ç”Ÿæˆçš„ prompt æ•°é‡
- --seed 2024 éšæœºç§å­
- --dtype float16 æµ®ç‚¹æ•°ç²¾åº¦
- --max-model-len æ¨¡å‹æœ€å¤§é•¿åº¦
- --hf_max_batch_size transformers åº“çš„æœ€å¤§æ‰¹å¤„ç†å¤§å°ï¼ˆåªæœ‰ hf æ¨ç†åç«¯æœ‰æ•ˆï¼Œä¸”å¿…é¡»ï¼‰
- --dataset æ•°æ®é›†è·¯å¾„ã€‚ï¼ˆæœªè®¾ç½®ä¼šè‡ªåŠ¨ç”Ÿæˆæ•°æ®ï¼‰

æµ‹è¯• vLLM çš„é€Ÿåº¦ï¼š

```bash
python benchmark_throughput.py \
        --model /root/autodl-tmp/ZhipuAI/glm-4-9b-chat \
        --backend vllm \
        --input-len 64 \
        --output-len 128 \
        --num-prompts 25 \
        --seed 2024 \
    --dtype float16 \
    --max-model-len 512 \
    --trust-remote-code
```

å¾—åˆ°çš„ç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š

```
Throughput: 7.41 requests/s, 1423.36 tokens/s
```

æµ‹è¯•åŸå§‹æ–¹å¼ï¼ˆä½¿ç”¨ hunggingface çš„ transformers åº“ï¼‰çš„é€Ÿåº¦ï¼š

```bash
python benchmark_throughput.py \
        --model /root/autodl-tmp/ZhipuAI/glm-4-9b-chat \
        --backend hf \
        --input-len 64 \
        --output-len 128 \
        --num-prompts 25 \
        --seed 2024 \
        --dtype float16 \
    --hf-max-batch-size 25 \
    --trust-remote-code
```

å¾—åˆ°çš„ç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š

```
Throughput: 3.40 requests/s, 652.15 tokens/s
```

å¯¹æ¯”ä¸¤è€…çš„é€Ÿåº¦ï¼Œåœ¨æœ¬æ¬¡æµ‹è¯•ä¸­ vLLM çš„é€Ÿåº¦è¦æ¯”åŸå§‹çš„é€Ÿåº¦å¿« **100%** ä»¥ä¸Šï¼ˆæœ¬æ¬¡æµ‹è¯•ç›¸å¯¹æ¯”è¾ƒéšæ„ï¼Œä»…ä¾›æœ¬ case å‚è€ƒï¼Œä¸å¯¹å…¶ä»– case æœ‰å‚è€ƒæ„ä¹‰ï¼‰ã€‚

| æ¨ç†æ¡†æ¶ | Throughput | tokens/s |
| :---: | :---: | :---: |
| vllm | 7.41 requests/s | 1423.36 tokens/s |
| hf | 3.40 requests/s | 652.15 tokens/s |
| diff | 117.94% | 118.26% |
