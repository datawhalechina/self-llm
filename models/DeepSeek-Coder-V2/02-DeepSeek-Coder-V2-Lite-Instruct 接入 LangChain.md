# DeepSeek-Coder-V2-Lite-Instruct æ¥å…¥ LangChain

## **ç¯å¢ƒå‡†å¤‡**

æœ¬æ–‡åŸºç¡€ç¯å¢ƒå¦‚ä¸‹ï¼š

```
----------------
ubuntu 22.04
python 3.12
cuda 12.1
pytorch 2.3.0
----------------
```

> æœ¬æ–‡é»˜è®¤å­¦ä¹ è€…å·²å®‰è£…å¥½ä»¥ä¸Š Pytorch(cuda) ç¯å¢ƒï¼Œå¦‚æœªå®‰è£…è¯·è‡ªè¡Œå®‰è£…ã€‚

æ¥ä¸‹æ¥å¼€å§‹ç¯å¢ƒé…ç½®ã€æ¨¡å‹ä¸‹è½½å’Œè¿è¡Œæ¼”ç¤º ~

`pip` æ¢æºåŠ é€Ÿä¸‹è½½å¹¶å®‰è£…ä¾èµ–åŒ…

```shell
# å‡çº§pip
python -m pip install --upgrade pip
# æ›´æ¢ pypi æºåŠ é€Ÿåº“çš„å®‰è£…
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

pip install modelscope==1.16.1
pip install langchain==0.2.3
pip install transformers==4.43.2
pip install accelerate==0.32.1
```

> è€ƒè™‘åˆ°éƒ¨åˆ†åŒå­¦é…ç½®ç¯å¢ƒå¯èƒ½ä¼šé‡åˆ°ä¸€äº›é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨AutoDLå¹³å°å‡†å¤‡äº†DeepSeek-Coder-V2-Lite-Instructçš„ç¯å¢ƒé•œåƒï¼Œç‚¹å‡»ä¸‹æ–¹é“¾æ¥å¹¶ç›´æ¥åˆ›å»ºAutodlç¤ºä¾‹å³å¯ã€‚
> ***https://www.codewithgpu.com/i/datawhalechina/self-llm/Deepseek-coder-v2***



## æ¨¡å‹ä¸‹è½½

ä½¿ç”¨ `modelscope` ä¸­çš„ `snapshot_download` å‡½æ•°ä¸‹è½½æ¨¡å‹ï¼Œç¬¬ä¸€ä¸ªå‚æ•°ä¸ºæ¨¡å‹åç§°ï¼Œå‚æ•° `cache_dir` ä¸ºè‡ªå®šä¹‰çš„æ¨¡å‹ä¸‹è½½è·¯å¾„ï¼Œå‚æ•°`revision`ä¸ºæ¨¡å‹ä»“åº“åˆ†æ”¯ç‰ˆæœ¬ï¼Œmasterä»£è¡¨ä¸»åˆ†æ”¯ï¼Œä¹Ÿæ˜¯ä¸€èˆ¬æ¨¡å‹ä¸Šä¼ çš„é»˜è®¤åˆ†æ”¯ã€‚

å…ˆåˆ‡æ¢åˆ° `autodl-tmp` ç›®å½•ï¼Œ`cd /root/autodl-tmp` 

ç„¶åæ–°å»ºåä¸º `model_download.py` çš„ `python` è„šæœ¬ï¼Œå¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹å¹¶ä¿å­˜

```python
# model_download.py
import torch
from modelscope import snapshot_download, AutoModel, AutoTokenizer

model_dir = snapshot_download('deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct', cache_dir='/root/autodl-tmp', revision='master')
```

ç„¶ååœ¨ç»ˆç«¯ä¸­è¾“å…¥ `python model_download.py` æ‰§è¡Œä¸‹è½½ï¼Œæ³¨æ„è¯¥æ¨¡å‹æƒé‡æ–‡ä»¶æ¯”è¾ƒå¤§ï¼Œå› æ­¤è¿™é‡Œéœ€è¦è€å¿ƒç­‰å¾…ä¸€æ®µæ—¶é—´ç›´åˆ°æ¨¡å‹ä¸‹è½½å®Œæˆã€‚



## ä»£ç å‡†å¤‡

ä¸ºä¾¿æ·æ„å»º `LLM` åº”ç”¨ï¼Œæˆ‘ä»¬éœ€è¦åŸºäºæœ¬åœ°éƒ¨ç½²çš„ `DeepSeek_Coder_LLM`ï¼Œè‡ªå®šä¹‰ä¸€ä¸ª LLM ç±»ï¼Œå°† `DeepSeek-Coder-V2-Lite-Instruct` æ¥å…¥åˆ° `LangChain` æ¡†æ¶ä¸­ã€‚å®Œæˆè‡ªå®šä¹‰ `LLM` ç±»ä¹‹åï¼Œå¯ä»¥ä»¥å®Œå…¨ä¸€è‡´çš„æ–¹å¼è°ƒç”¨ `LangChain` çš„æ¥å£ï¼Œè€Œæ— éœ€è€ƒè™‘åº•å±‚æ¨¡å‹è°ƒç”¨çš„ä¸ä¸€è‡´ã€‚

åŸºäºæœ¬åœ°éƒ¨ç½²çš„ `DeepSeek-Coder-V2-Lite-Instruct` è‡ªå®šä¹‰ `LLM` ç±»å¹¶ä¸å¤æ‚ï¼Œæˆ‘ä»¬åªéœ€ä» `LangChain.llms.base.LLM` ç±»ç»§æ‰¿ä¸€ä¸ªå­ç±»ï¼Œå¹¶é‡å†™æ„é€ å‡½æ•°ä¸ `_call` å‡½æ•°å³å¯ï¼š

```python
# langchain_deepseek_coder.ipynb [1]
from langchain.llms.base import LLM
from typing import Any, List, Optional
from langchain.callbacks.manager import CallbackManagerForLLMRun
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, LlamaTokenizerFast
import torch

class DeepSeek_Coder_LLM(LLM):  # å®šä¹‰ä¸€ä¸ªç»§æ‰¿è‡ªLLMçš„DeepSeek_Coder_LLMç±»
    # ç±»å˜é‡ï¼Œåˆå§‹åŒ–ä¸ºNoneï¼Œå°†åœ¨åˆå§‹åŒ–æ–¹æ³•ä¸­è¢«èµ‹å€¼
    tokenizer: AutoTokenizer = None
    model: AutoModelForCausalLM = None
        
    def __init__(self, mode_name_or_path: str):  # åˆå§‹åŒ–æ–¹æ³•ï¼Œæ¥å—æ¨¡å‹è·¯å¾„æˆ–åç§°ä½œä¸ºå‚æ•°

        super().__init__()  # è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•
        print("æ­£åœ¨ä»æœ¬åœ°åŠ è½½æ¨¡å‹...")  # æ‰“å°åŠ è½½æ¨¡å‹çš„æç¤ºä¿¡æ¯
        # ä½¿ç”¨AutoTokenizerä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(mode_name_or_path, use_fast=False, trust_remote_code=True)
        # ä½¿ç”¨AutoModelForCausalLMä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½è¯­è¨€æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            mode_name_or_path,
            torch_dtype=torch.bfloat16,  # è®¾ç½®PyTorchæ•°æ®ç±»å‹ä¸ºbfloat16
            device_map="auto",  # è®©æ¨¡å‹è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
            trust_remote_code=True  # ä¿¡ä»»è¿œç¨‹ä»£ç 
        )
        # ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ç”Ÿæˆé…ç½®
        self.model.generation_config = GenerationConfig.from_pretrained(mode_name_or_path)
        print("å®Œæˆæœ¬åœ°æ¨¡å‹çš„åŠ è½½")  # æ‰“å°æ¨¡å‹åŠ è½½å®Œæˆçš„æç¤ºä¿¡æ¯
        
    def _call(self, prompt: str, stop: Optional[List[str]] = None,
               run_manager: Optional[CallbackManagerForLLMRun] = None,
               **kwargs: Any):  # å®šä¹‰_callæ–¹æ³•ï¼Œç”¨äºç”Ÿæˆæ–‡æœ¬

        messages = [{"role": "user", "content": prompt }]  # å®šä¹‰æ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å«ç”¨æˆ·çš„è§’è‰²å’Œå†…å®¹
        # åº”ç”¨èŠå¤©æ¨¡æ¿ï¼Œç”Ÿæˆè¾“å…¥ID
        input_ids = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        # å°†è¾“å…¥IDè½¬æ¢ä¸ºæ¨¡å‹éœ€è¦çš„è¾“å…¥æ ¼å¼ï¼Œå¹¶è½¬æ¢ä¸ºPyTorchå¼ é‡
        model_inputs = self.tokenizer([input_ids], return_tensors="pt").to('cuda')
        # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ï¼Œè®¾ç½®ç”Ÿæˆå‚æ•°
        generated_ids = self.model.generate(
            model_inputs.input_ids,
            max_new_tokens=512,  # æœ€å¤§æ–°ç”Ÿæˆçš„tokenæ•°
            top_k=5,  # æ¯æ¬¡é‡‡æ ·çš„tokenæ•°
            top_p=0.8,  # æŒ‰æ¦‚ç‡åˆ†å¸ƒé‡‡æ ·
            temperature=0.3,  # æ¸©åº¦å‚æ•°ï¼Œå½±å“ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§
            repetition_penalty=1.1,  # é‡å¤æƒ©ç½šï¼Œé¿å…é‡å¤ç”Ÿæˆç›¸åŒçš„æ–‡æœ¬
            do_sample=True  # æ˜¯å¦ä½¿ç”¨é‡‡æ ·
        )
        # ä»ç”Ÿæˆçš„IDä¸­æå–å®é™…ç”Ÿæˆçš„æ–‡æœ¬ID
        generated_ids = [
            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        # å°†ç”Ÿæˆçš„IDè§£ç ä¸ºæ–‡æœ¬ï¼Œå¹¶è·³è¿‡ç‰¹æ®Štoken
        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        return response  # è¿”å›ç”Ÿæˆçš„æ–‡æœ¬

    @property
    def _llm_type(self) -> str:  # å®šä¹‰ä¸€ä¸ªå±æ€§ï¼Œè¿”å›æ¨¡å‹çš„ç±»å‹
        return "DeepSeek_Coder_LLM"
```

åœ¨ä¸Šè¿°ç±»å®šä¹‰ä¸­ï¼Œæˆ‘ä»¬åˆ†åˆ«é‡å†™äº†æ„é€ å‡½æ•°å’Œ `_call` å‡½æ•°ï¼š

- å¯¹äºæ„é€ å‡½æ•°ï¼Œæˆ‘ä»¬åœ¨å¯¹è±¡å®ä¾‹åŒ–çš„ä¸€å¼€å§‹åŠ è½½æœ¬åœ°éƒ¨ç½²çš„ `DeepSeek-Coder-V2-Lite-Instruct` æ¨¡å‹ï¼Œä»è€Œé¿å…æ¯ä¸€æ¬¡è°ƒç”¨éƒ½éœ€è¦é‡æ–°åŠ è½½æ¨¡å‹å¸¦æ¥çš„æ—¶é—´æµªè´¹ï¼›

- `_call` å‡½æ•°æ˜¯ LLM ç±»çš„æ ¸å¿ƒå‡½æ•°ï¼Œ`Langchain` ä¼šè°ƒç”¨æ”¹å‡½æ•°æ¥è°ƒç”¨ `LLM`ï¼Œåœ¨æ”¹å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬è°ƒç”¨å·²å®ä¾‹åŒ–æ¨¡å‹çš„ `generate` æ–¹æ³•ï¼Œä»è€Œå®ç°å¯¹æ¨¡å‹çš„è°ƒç”¨å¹¶è¿”å›è°ƒç”¨ç»“æœã€‚

æ­¤å¤–ï¼Œåœ¨å®ç°è‡ªå®šä¹‰ `LLM` ç±»æ—¶ï¼ŒæŒ‰ç…§ `Langchain` æ¡†æ¶çš„è¦æ±‚ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ `_identifying_params` å±æ€§ã€‚è¿™ä¸ªå±æ€§çš„ä½œç”¨æ˜¯è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œè¯¥å­—å…¸åŒ…å«äº†èƒ½å¤Ÿå”¯ä¸€æ ‡è¯†è¿™ä¸ª `LLM` å®ä¾‹çš„å‚æ•°ã€‚è¿™ä¸ªåŠŸèƒ½å¯¹äºç¼“å­˜å’Œè¿½è¸ªéå¸¸é‡è¦ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿå¸®åŠ©ç³»ç»Ÿè¯†åˆ«ä¸åŒçš„æ¨¡å‹é…ç½®ï¼Œä»è€Œè¿›è¡Œæœ‰æ•ˆçš„ç¼“å­˜ç®¡ç†å’Œæ—¥å¿—è¿½è¸ªã€‚

åœ¨æ•´ä½“é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬å°†ä¸Šè¯‰ä»£ç å°è£…ä¸º `deepseek_langchain.py`ï¼Œåç»­å°†ç›´æ¥ä»è¯¥æ–‡ä»¶ä¸­å¼•å…¥è‡ªå®šä¹‰çš„ `DeepSeek_Coder_LLM` ç±»ã€‚



## æ¨¡å‹è°ƒç”¨

ç„¶åå°±å¯ä»¥åƒä½¿ç”¨ä»»ä½•å…¶ä»–çš„ `Langchain` å¤§æ¨¡å‹åŠŸèƒ½ä¸€æ ·ä½¿ç”¨äº†ã€‚

æˆ‘ä»¬å…ˆæ¥ç®€å•æµ‹è¯•ä¸€ä¸‹æ¨¡å‹çš„æ—¥å¸¸å¯¹è¯èƒ½åŠ›ğŸ˜Š

```python
# langchain_deepseek_coder.ipynb [2]
# from deepseek_langchain import DeepSeek_Coder_LLM
llm = DeepSeek_Coder_LLM(mode_name_or_path = "/root/autodl-tmp/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct")
print(llm("ä½ æ˜¯è°ï¼Ÿ"))
```

å¾—åˆ°çš„è¿”å›å€¼å¦‚ä¸‹æ‰€ç¤ºï¼š

```
æˆ‘æ˜¯DeepSeek Coderï¼Œä¸€ä¸ªç”±æ·±åº¦æ±‚ç´¢å…¬å¸å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹ã€‚æˆ‘æ—¨åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€å¤„ç†å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯æ¥æä¾›ä¿¡æ¯æŸ¥è¯¢ã€å¯¹è¯äº¤æµå’Œè§£ç­”é—®é¢˜ç­‰æœåŠ¡ã€‚
```

è°ƒç”¨ç¤ºä¾‹ç»“æœå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![fig2-1](images/fig2-1.png)

å¦å¤–ï¼Œæ ¹æ®å®˜æ–¹çš„æ–‡æ¡£è¯´æ˜ï¼Œ`DeepSeek-Coder-V2-Lite-Instruct` æ¨¡å‹ä¹Ÿæœ‰ç€æå¼ºçš„ä»£ç èƒ½åŠ›

![fig2-3](images/fig2-3.png)

æ‰€ä»¥æˆ‘ä»¬ä¸å¦¨ä¹Ÿæ¥æµ‹è¯•ä¸€ä¸ªç»å…¸çš„ä»£ç é—®é¢˜ğŸ¥°

```python
# langchain_deepseek_coder.ipynb [3]
print(llm("è¯·å¸®æˆ‘å†™ä¸€æ®µè´ªåƒè›‡æ¸¸æˆçš„Pythonä»£ç ï¼Œå¹¶ç»™å‡ºå¿…è¦çš„æ³¨é‡Š"))

```

å¾—åˆ°çš„è¿”å›å€¼å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
å½“ç„¶ï¼ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„è´ªåƒè›‡æ¸¸æˆPythonä»£ç ï¼Œå¸¦æœ‰è¯¦ç»†çš„æ³¨é‡Šã€‚è¿™ä¸ªç‰ˆæœ¬ä½¿ç”¨`curses`åº“æ¥å¤„ç†é”®ç›˜è¾“å…¥å’Œå›¾å½¢ç•Œé¢ã€‚

```python
import curses
import random

# åˆå§‹åŒ–å±å¹•
stdscr = curses.initscr()
curses.curs_set(0)  # éšè—å…‰æ ‡
sh, sw = stdscr.getmaxyx()  # è·å–çª—å£å¤§å°

# åˆ›å»ºä¸€ä¸ªçª—å£ï¼Œç”¨äºæ˜¾ç¤ºæ¸¸æˆåŒºåŸŸ
win = curses.newwin(sh, sw, 0, 0)
win.keypad(1)  # å¯ç”¨é”®ç›˜è¾“å…¥
win.timeout(100)  # è®¾ç½®è¶…æ—¶æ—¶é—´ï¼Œæ§åˆ¶åˆ·æ–°é¢‘ç‡

# å®šä¹‰è´ªåƒè›‡çš„åˆå§‹ä½ç½®å’Œé•¿åº¦
snake = [(4, 4), (4, 3), (4, 2)]
food = (random.randint(1, sh-1), random.randint(1, sw-1))
win.addch(food[0], food[1], '#')

# å®šä¹‰æ–¹å‘é”®
key = curses.KEY_RIGHT

while True:
    win.border(0)  # ç”»è¾¹æ¡†
    win.clear()  # æ¸…é™¤çª—å£å†…å®¹
    
    for y, x in snake:
        win.addch(y, x, '*')  # ç»˜åˆ¶è´ªåƒè›‡çš„èº«ä½“
    
    win.addch(food[0], food[1], '#')  # ç»˜åˆ¶é£Ÿç‰©
    
    event = win.getch()  # è·å–é”®ç›˜äº‹ä»¶
    if event == -1:  # å¦‚æœæ²¡æœ‰æŒ‰é”®ï¼Œç»§ç»­å½“å‰æ–¹å‘
        key = key
    else:
        key = event  # æ›´æ–°æ–¹å‘
    
    # è§£ææ–°çš„å¤´éƒ¨ä½ç½®
    head = snake[0]
    if key == curses.KEY_DOWN:
        new_head = (head[0] + 1, head[1])
    elif key == curses.KEY_UP:
        new_head = (head[0] - 1, head[1])
    elif key == curses.KEY_LEFT:
        new_head = (head[0], head[1] - 1)
    elif key == curses.KEY_RIGHT:
```

è°ƒç”¨ç¤ºä¾‹ç»“æœå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![fig2-2](images/fig2-2.png)
