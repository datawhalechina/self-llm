# 03-Qwen2.5-7B-Instruct vLLM éƒ¨ç½²è°ƒç”¨

## **vLLM ç®€ä»‹**

`vLLM` æ¡†æ¶æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„å¤§è¯­è¨€æ¨¡å‹**æ¨ç†å’Œéƒ¨ç½²æœåŠ¡ç³»ç»Ÿ**ï¼Œå…·å¤‡ä»¥ä¸‹ç‰¹æ€§ï¼š

- **é«˜æ•ˆçš„å†…å­˜ç®¡ç†**ï¼šé€šè¿‡ `PagedAttention` ç®—æ³•ï¼Œ`vLLM` å®ç°äº†å¯¹ `KV` ç¼“å­˜çš„é«˜æ•ˆç®¡ç†ï¼Œå‡å°‘äº†å†…å­˜æµªè´¹ï¼Œä¼˜åŒ–äº†æ¨¡å‹çš„è¿è¡Œæ•ˆç‡ã€‚
- **é«˜ååé‡**ï¼š`vLLM` æ”¯æŒå¼‚æ­¥å¤„ç†å’Œè¿ç»­æ‰¹å¤„ç†è¯·æ±‚ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹æ¨ç†çš„ååé‡ï¼ŒåŠ é€Ÿäº†æ–‡æœ¬ç”Ÿæˆå’Œå¤„ç†é€Ÿåº¦ã€‚
- **æ˜“ç”¨æ€§**ï¼š`vLLM` ä¸ `HuggingFace` æ¨¡å‹æ— ç¼é›†æˆï¼Œæ”¯æŒå¤šç§æµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç®€åŒ–äº†æ¨¡å‹éƒ¨ç½²å’Œæ¨ç†çš„è¿‡ç¨‹ã€‚å…¼å®¹ `OpenAI` çš„ `API` æœåŠ¡å™¨ã€‚
- **åˆ†å¸ƒå¼æ¨ç†**ï¼šæ¡†æ¶æ”¯æŒåœ¨å¤š `GPU` ç¯å¢ƒä¸­è¿›è¡Œåˆ†å¸ƒå¼æ¨ç†ï¼Œé€šè¿‡æ¨¡å‹å¹¶è¡Œç­–ç•¥å’Œé«˜æ•ˆçš„æ•°æ®é€šä¿¡ï¼Œæå‡äº†å¤„ç†å¤§å‹æ¨¡å‹çš„èƒ½åŠ›ã€‚
- **å¼€æºå…±äº«**ï¼š`vLLM` ç”±äºå…¶å¼€æºçš„å±æ€§ï¼Œæ‹¥æœ‰æ´»è·ƒçš„ç¤¾åŒºæ”¯æŒï¼Œè¿™ä¹Ÿä¾¿äºå¼€å‘è€…è´¡çŒ®å’Œæ”¹è¿›ï¼Œå…±åŒæ¨åŠ¨æŠ€æœ¯å‘å±•ã€‚



## ç¯å¢ƒå‡†å¤‡  

æœ¬æ–‡åŸºç¡€ç¯å¢ƒå¦‚ä¸‹ï¼š

```
----------------
ubuntu 22.04
python 3.12
cuda 12.1
pytorch 2.4.0
----------------
```

> æœ¬æ–‡é»˜è®¤å­¦ä¹ è€…å·²é…ç½®å¥½ä»¥ä¸Š `Pytorch (cuda)` ç¯å¢ƒï¼Œå¦‚æœªé…ç½®è¯·å…ˆè‡ªè¡Œå®‰è£…ã€‚

é¦–å…ˆ `pip` æ¢æºåŠ é€Ÿä¸‹è½½å¹¶å®‰è£…ä¾èµ–åŒ…

```bash
python -m pip install --upgrade pip
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

pip install modelscope==1.20.0
pip install transformers==4.46.2
pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121
pip install vllm==0.6.3.post1
pip install accelerate==0.26.0
```

> è€ƒè™‘åˆ°éƒ¨åˆ†åŒå­¦é…ç½®ç¯å¢ƒå¯èƒ½ä¼šé‡åˆ°ä¸€äº›é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨AutoDLå¹³å°å‡†å¤‡äº†Qwen2.5çš„ç¯å¢ƒé•œåƒï¼Œç‚¹å‡»ä¸‹æ–¹é“¾æ¥å¹¶ç›´æ¥åˆ›å»ºAutodlç¤ºä¾‹å³å¯ã€‚
> ***https://www.codewithgpu.com/i/datawhalechina/self-llm/qwen2.5-coder***


## æ¨¡å‹ä¸‹è½½  
ä½¿ç”¨ `modelscope` ä¸­çš„ `snapshot_download` å‡½æ•°ä¸‹è½½æ¨¡å‹ï¼Œç¬¬ä¸€ä¸ªå‚æ•°ä¸ºæ¨¡å‹åç§°ï¼Œå‚æ•° `cache_dir` ä¸ºæ¨¡å‹çš„ä¸‹è½½è·¯å¾„ã€‚

åœ¨æ–°å»º `model_download.py` æ–‡ä»¶å¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼Œç²˜è´´ä»£ç åè®°å¾—ä¿å­˜æ–‡ä»¶ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å¹¶è¿è¡Œ `python model_download.py` æ‰§è¡Œä¸‹è½½ï¼Œæ¨¡å‹å¤§å°ä¸º 16 GBï¼Œä¸‹è½½æ¨¡å‹å¤§æ¦‚éœ€è¦ 12 åˆ†é’Ÿã€‚

```python  
from modelscope import snapshot_download

snapshot_download('Qwen/Qwen2.5-Coder-7B-Instruct', local_dir='/root/autodl-tmp/Qwen2.5-Coder-7B-Instruct')
```

> æ³¨æ„ï¼šè®°å¾—ä¿®æ”¹ `local_dir` ä¸ºä½ çš„æ¨¡å‹ä¸‹è½½è·¯å¾„å“¦~
![04-1](./images/04-1.png)


## **ä»£ç å‡†å¤‡**

### **Pythonè„šæœ¬**

åœ¨ `/root/autodl-tmp` è·¯å¾„ä¸‹æ–°å»º `vllm_model.py` æ–‡ä»¶å¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼Œç²˜è´´ä»£ç åè¯·åŠæ—¶ä¿å­˜æ–‡ä»¶ã€‚ä¸‹é¢çš„ä»£ç æœ‰å¾ˆè¯¦ç»†çš„æ³¨é‡Šï¼Œå¦‚æœ‰ä¸ç†è§£çš„åœ°æ–¹ï¼Œæ¬¢è¿å¤§å®¶æ `issue`ã€‚

é¦–å…ˆä» `vLLM` åº“ä¸­å¯¼å…¥ `LLM` å’Œ `SamplingParams` ç±»ã€‚`LLM` ç±»æ˜¯ä½¿ç”¨ `vLLM` å¼•æ“è¿è¡Œç¦»çº¿æ¨ç†çš„ä¸»è¦ç±»ã€‚`SamplingParams` ç±»æŒ‡å®šé‡‡æ ·è¿‡ç¨‹çš„å‚æ•°ï¼Œç”¨äºæ§åˆ¶å’Œè°ƒæ•´ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§å’Œå¤šæ ·æ€§ã€‚

`vLLM` æä¾›äº†éå¸¸æ–¹ä¾¿çš„å°è£…ï¼Œæˆ‘ä»¬ç›´æ¥ä¼ å…¥æ¨¡å‹åç§°æˆ–æ¨¡å‹è·¯å¾„å³å¯ï¼Œä¸å¿…æ‰‹åŠ¨åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡è¿™ä¸ªä»£ç ç¤ºä¾‹ç†Ÿæ‚‰ä¸‹ ` vLLM` å¼•æ“çš„ä½¿ç”¨æ–¹å¼ã€‚è¢«æ³¨é‡Šçš„éƒ¨åˆ†å†…å®¹å¯ä»¥ä¸°å¯Œæ¨¡å‹çš„èƒ½åŠ›ï¼Œä½†ä¸æ˜¯å¿…è¦çš„ï¼Œå¤§å®¶å¯ä»¥æŒ‰éœ€é€‰æ‹©ï¼Œè‡ªå·±å¤šå¤šåŠ¨æ‰‹å°è¯• ~

```python
# vllm_model.py
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer
import os
import json

def prepare_model(model_path,  max_tokens=512, temperature=0.8, top_p=0.95, max_model_len=2048):
    # åˆå§‹åŒ– vLLM æ¨ç†å¼•æ“
    llm = LLM(model=model_path, tokenizer=model_path, max_model_len=max_model_len,trust_remote_code=True)
    return llm
    

def get_completion(prompts, llm, max_tokens=512, temperature=0.8, top_p=0.95, max_model_len=2048):
    stop_token_ids = [151329, 151336, 151338]
    # åˆ›å»ºé‡‡æ ·å‚æ•°ã€‚temperature æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§ï¼Œtop_p æ§åˆ¶æ ¸å¿ƒé‡‡æ ·çš„æ¦‚ç‡
    sampling_params = SamplingParams(temperature=temperature, top_p=top_p, max_tokens=max_tokens, stop_token_ids=stop_token_ids)
    # åˆå§‹åŒ– vLLM æ¨ç†å¼•æ“
    outputs = llm.generate(prompts, sampling_params)
    return outputs

 
# åˆå§‹åŒ– vLLM æ¨ç†å¼•æ“
model_path = './Qwen2.5-Coder-7B-Instruct'
tokenizer = AutoTokenizer.from_pretrained(model_path)
llm = prepare_model(model_path)

prompt = "å¸®æˆ‘ä½¿ç”¨torchå†™ä¸€ä¸ªç”¨äºæ‰‹å†™æ•°å­—è¯†åˆ«çš„æ¨¡å‹"
messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚"},
    {"role": "user", "content": prompt}
]

# åº”ç”¨templateä¸­çš„chatæ¨¡æ¿
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

outputs = get_completion(text, llm, max_tokens=1024, temperature=1, top_p=1, max_model_len=2048)
print(outputs[0].outputs[0].text)

```
è¿è¡Œä»£ç 

```bash
cd /root/autodl-tmp && python vllm_model.py
```

ç»“æœå¦‚ä¸‹ï¼š

```bash
Prompt: 'å¸®æˆ‘ä½¿ç”¨torchå†™ä¸€ä¸ªç”¨äºæ‰‹å†™æ•°å­—è¯†åˆ«çš„æ¨¡å‹', Generated text: ' å½“ç„¶å¯ä»¥ï¼ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨PyTorchç¼–å†™çš„ç®€å•å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Œç”¨äºæ‰‹å†™æ•°å­—è¯†åˆ«ã€‚è¿™ä¸ªæ¨¡å‹ä½¿ç”¨çš„æ˜¯MNISTæ•°æ®é›†ã€‚\n\né¦–å…ˆï¼Œç¡®ä¿ä½ å·²ç»å®‰è£…äº†PyTorchã€‚å¦‚æœæ²¡æœ‰å®‰è£…ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œå®‰è£…ï¼š\n\n```bash\npip install torch torchvision\n```\n\næ¥ä¸‹æ¥æ˜¯ä»£ç ï¼š\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\n\n# å®šä¹‰è¶…å‚æ•°\nbatch_size = 64\nlearning_rate = 0.001\nnum_epochs = 10\n\n# è½¬æ¢æ•°æ®\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n])\n\n# åŠ è½½MNISTæ•°æ®é›†\ntrain_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntest_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n\n# åˆ›å»ºæ•°æ®åŠ è½½å™¨\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n\n# å®šä¹‰å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        self.fc1 = nn.Linear(in_features=64 * 7 * 7, out_features=128)\n        self.fc2 = nn.Linear(in_features=128, out_features=10)\n\n    def forward(self, x):\n        x = self.pool(nn.functional.relu(self.conv1(x)))\n        x = self.pool(nn.functional.relu(self.conv2(x)))\n        x = x.view(-1, 64 * 7 * 7)\n        x = nn.functional.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# å®ä¾‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨\nmodel = CNN()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# è®­ç»ƒæ¨¡å‹\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for i, (images, labels) in enumerate(train_loader):\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        if (i+1) % 100 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}')\n            running_loss = 0.0\n\nprint('Finished Training')\n\n# æµ‹è¯•æ¨¡å‹\nmodel.eval()\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print(f'Accuracy of the model on the 10000 test images: {100 * correct / total:.2f}%')\n```\n\nè¿™ä¸ªä»£ç åŒ…æ‹¬ä»¥ä¸‹éƒ¨åˆ†ï¼š\n\n1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†ã€‚\n2. å®šä¹‰äº†ä¸€ä¸ªç®€å•çš„å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚\n3. è®­ç»ƒæ¨¡å‹ã€‚\n4. æµ‹è¯•æ¨¡å‹ã€‚\n\nè¿è¡Œè¿™ä¸ªä»£ç åï¼Œä½ å°†çœ‹åˆ°æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹å’Œæœ€ç»ˆçš„æµ‹è¯•å‡†ç¡®ç‡ã€‚å¸Œæœ›è¿™å¯¹ä½ æœ‰å¸®åŠ©ï¼'

Prompt: 'å¸®æˆ‘å†™ä¸€ä¸ªç”¨äºç™»é™†çš„Servlet', Generated text: ' å¥½çš„ï¼Œæˆ‘æ¥ä¸ºæ‚¨æä¾›ä¸€ä¸ªç®€å•çš„ç”¨äºç™»å½•çš„Servletç¤ºä¾‹ã€‚è¿™ä¸ªç¤ºä¾‹å°†ä½¿ç”¨Javaçš„Servlet APIï¼Œå¹¶å‡è®¾æ‚¨æ­£åœ¨ä½¿ç”¨Java EEæˆ–Servlet 3.1ä»¥ä¸Šç‰ˆæœ¬çš„å®¹å™¨ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªåŸºæœ¬çš„å®ç°ï¼š\n\n```java\nimport java.io.IOException;\nimport javax.servlet.ServletException;\nimport javax.servlet.annotation.WebServlet;\nimport javax.servlet.http.HttpServlet;\nimport javax.servlet.http.HttpServletRequest;\nimport javax.servlet.http.HttpServletResponse;\nimport javax.servlet.http.HttpSession;\n\n@WebServlet("/login")\npublic class LoginServlet extends HttpServlet {\n    private static final long serialVersionUID = 1L;\n\n    // å‡è®¾è¿™æ˜¯æˆ‘ä»¬çš„ç™»å½•éªŒè¯é€»è¾‘\n    private boolean authenticateUser(String username, String password) {\n        // è¿™é‡Œåº”æ›¿æ¢ä¸ºå®é™…çš„æ•°æ®åº“æŸ¥è¯¢æˆ–å…¶ä»–éªŒè¯æœºåˆ¶\n        return "admin".equals(username) && "password".equals(password);\n    }\n\n    protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        // è·å–è¡¨å•æäº¤çš„ç”¨æˆ·åå’Œå¯†ç \n        String username = request.getParameter("username");\n        String password = request.getParameter("password");\n\n        // è°ƒç”¨éªŒè¯æ–¹æ³•\n        boolean isValidUser = authenticateUser(username, password);\n\n        if (isValidUser) {\n            // å¦‚æœéªŒè¯æˆåŠŸï¼Œåˆ›å»ºæˆ–æ›´æ–°sessionå¹¶è½¬å‘åˆ°æˆåŠŸçš„é¡µé¢\n            HttpSession session = request.getSession(true);\n            session.setAttribute("username", username);\n            request.getRequestDispatcher("success.jsp").forward(request, response);\n        } else {\n            // å¦‚æœéªŒè¯å¤±è´¥ï¼Œå°†é”™è¯¯ä¿¡æ¯æ”¾åœ¨è¯·æ±‚å±æ€§ä¸­ï¼Œå¹¶è½¬å‘åˆ°ç™»å½•é¡µé¢å¹¶æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯\n            request.setAttribute("error", "Invalid username or password.");\n            request.getRequestDispatcher("login.jsp").forward(request, response);\n        }\n    }\n}\n```\n\nè¯·åˆ›å»ºä¸€ä¸ªåä¸º`login.jsp`çš„å‰ç«¯è¡¨å•é¡µé¢ç”¨äºè¾“å…¥ç”¨æˆ·åå’Œå¯†ç ï¼Œå¹¶åˆ›å»ºä¸€ä¸ª`success.jsp`é¡µé¢ç”¨äºæ˜¾ç¤ºæˆåŠŸä¿¡æ¯ã€‚åŒæ—¶ï¼Œåœ¨å®é™…é¡¹ç›®ä¸­ï¼Œè¯·ç¡®ä¿å¯¹å¯†ç è¿›è¡ŒåŠ å¯†å¤„ç†ï¼Œé¿å…æ˜æ–‡å­˜å‚¨ã€‚\n\næ³¨æ„ï¼šè¿™åªæ˜¯ä¸€ä¸ªéå¸¸åŸºç¡€çš„ç¤ºä¾‹ï¼Œå¹¶æœªæ¶‰åŠCSRFä¿æŠ¤ã€XSSé˜²æŠ¤ç­‰å®‰å…¨é—®é¢˜ã€‚åœ¨å®é™…é¡¹ç›®å¼€å‘è¿‡ç¨‹ä¸­ï¼Œè¯·åŠ¡å¿…è€ƒè™‘è¿™äº›å®‰å…¨æªæ–½ä»¥æé«˜ç³»ç»Ÿçš„å®‰å…¨æ€§ã€‚'
```

![04-2](./images/04-2.png)



### åˆ›å»ºå…¼å®¹ OpenAI API æ¥å£çš„æœåŠ¡å™¨

`Qwen` å…¼å®¹ `OpenAI API` åè®®ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨ `vLLM` åˆ›å»º `OpenAI API` æœåŠ¡å™¨ã€‚`vLLM` éƒ¨ç½²å®ç° `OpenAI API` åè®®çš„æœåŠ¡å™¨éå¸¸æ–¹ä¾¿ã€‚é»˜è®¤ä¼šåœ¨ http://localhost:8000 å¯åŠ¨æœåŠ¡å™¨ã€‚æœåŠ¡å™¨å½“å‰ä¸€æ¬¡æ‰˜ç®¡ä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶å®ç°åˆ—è¡¨æ¨¡å‹ã€`completions` å’Œ `chat completions` ç«¯å£ã€‚

- `completions`ï¼šæ˜¯åŸºæœ¬çš„æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œæ¨¡å‹ä¼šåœ¨ç»™å®šçš„æç¤ºåç”Ÿæˆä¸€æ®µæ–‡æœ¬ã€‚è¿™ç§ç±»å‹çš„ä»»åŠ¡é€šå¸¸ç”¨äºç”Ÿæˆæ–‡ç« ã€æ•…äº‹ã€é‚®ä»¶ç­‰ã€‚
- `chat completions`ï¼šæ˜¯é¢å‘å¯¹è¯çš„ä»»åŠ¡ï¼Œæ¨¡å‹éœ€è¦ç†è§£å’Œç”Ÿæˆå¯¹è¯ã€‚è¿™ç§ç±»å‹çš„ä»»åŠ¡é€šå¸¸ç”¨äºæ„å»ºèŠå¤©æœºå™¨äººæˆ–è€…å¯¹è¯ç³»ç»Ÿã€‚

åœ¨åˆ›å»ºæœåŠ¡å™¨æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‡å®šæ¨¡å‹åç§°ã€æ¨¡å‹è·¯å¾„ã€èŠå¤©æ¨¡æ¿ç­‰å‚æ•°ã€‚

- `--host` å’Œ `--port` å‚æ•°æŒ‡å®šåœ°å€ã€‚
- `--model` å‚æ•°æŒ‡å®šæ¨¡å‹åç§°ã€‚
- `--chat-template` å‚æ•°æŒ‡å®šèŠå¤©æ¨¡æ¿ã€‚
- `--served-model-name` æŒ‡å®šæœåŠ¡æ¨¡å‹çš„åç§°ã€‚
- `--max-model-len` æŒ‡å®šæ¨¡å‹çš„æœ€å¤§é•¿åº¦ã€‚

```bash
python -m vllm.entrypoints.openai.api_server --model /root/autodl-tmp/Qwen2.5-Coder-7B-Instruct  --served-model-name Qwen2.5-Coder-7B-Instruct --max-model-len=2048
```

åŠ è½½å®Œæ¯•åå‡ºç°å¦‚ä¸‹ä¿¡æ¯è¯´æ˜æœåŠ¡æˆåŠŸå¯åŠ¨

![04-3](./images/04-3.png)

- é€šè¿‡ `curl` å‘½ä»¤æŸ¥çœ‹å½“å‰çš„æ¨¡å‹åˆ—è¡¨

```bash
curl http://localhost:8000/v1/models
```

å¾—åˆ°çš„è¿”å›å€¼å¦‚ä¸‹æ‰€ç¤º

```json
{
    "object": "list",
    "data": [
        {
            "id": "Qwen2.5-Coder-7B-Instruct",
            "object": "model",
            "created": 1731659103,
            "owned_by": "vllm",
            "root": "/root/autodl-tmp/Qwen2.5-Coder-7B-Instruct",
            "parent": null,
            "max_model_len": 2048,
            "permission": [
                {
                    "id": "modelperm-c9539ce169874ef1b6e49b2a4cf104d0",
                    "object": "model_permission",
                    "created": 1731659103,
                    "allow_create_engine": false,
                    "allow_sampling": true,
                    "allow_logprobs": true,
                    "allow_search_indices": false,
                    "allow_view": true,
                    "allow_fine_tuning": false,
                    "organization": "*",
                    "group": null,
                    "is_blocking": false
                }
            ]
        }
    ]
}
```

- ä½¿ç”¨ `curl` å‘½ä»¤æµ‹è¯• `OpenAI Completions API` 


```bash
curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen2.5-Coder-7B-Instruct",
        "prompt": "å¸®æˆ‘å†™ä¸€ä¸ªç”¨äºæ³¨å†Œçš„Servlet",
        "max_tokens": 500,
        "temperature": 0
    }'
```

å¾—åˆ°çš„è¿”å›å€¼å¦‚ä¸‹æ‰€ç¤º

```json
{
    "id": "cmpl-c879b54ddcc540e6946c28988ee5203a",
    "object": "text_completion",
    "created": 1731659222,
    "model": "Qwen2.5-Coder-7B-Instruct",
    "choices": [
        {
            "index": 0,
            "text": "ï¼Œè¯¥Servletèƒ½å¤Ÿå¤„ç†ç”¨æˆ·æäº¤çš„æ³¨å†Œä¿¡æ¯ï¼Œå¹¶å°†è¿™äº›ä¿¡æ¯å­˜å‚¨åˆ°æ•°æ®åº“ä¸­ã€‚è¯·ç¡®ä¿ä»£ç ä¸­åŒ…å«å¿…è¦çš„å¼‚å¸¸å¤„ç†å’Œæ•°æ®åº“è¿æ¥çš„å…³é—­ã€‚\n\n```java\nimport java.io.IOException;\nimport java.sql.Connection;\nimport java.sql.DriverManager;\nimport java.sql.PreparedStatement;\nimport java.sql.SQLException;\nimport javax.servlet.ServletException;\nimport javax.servlet.annotation.WebServlet;\nimport javax.servlet.http.HttpServlet;\nimport javax.servlet.http.HttpServletRequest;\nimport javax.servlet.http.HttpServletResponse;\n\n@WebServlet(\"/RegisterServlet\")\npublic class RegisterServlet extends HttpServlet {\n    private static final long serialVersionUID = 1L;\n    private static final String DB_URL = \"jdbc:mysql://localhost:3306/mydatabase\";\n    private static final String USER = \"username\";\n    private static final String PASS = \"password\";\n\n    protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        String username = request.getParameter(\"username\");\n        String password = request.getParameter(\"password\");\n        String email = request.getParameter(\"email\");\n\n        try (Connection conn = DriverManager.getConnection(DB_URL, USER, PASS);\n             PreparedStatement pstmt = conn.prepareStatement(\"INSERT INTO users (username, password, email) VALUES (?, ?, ?)\")) {\n\n            pstmt.setString(1, username);\n            pstmt.setString(2, password);\n            pstmt.setString(3, email);\n            pstmt.executeUpdate();\n\n            response.getWriter().println(\"Registration successful!\");\n        } catch (SQLException e) {\n            response.getWriter().println(\"Error during registration: \" + e.getMessage());\n        }\n    }\n}\n```\n\n**Created Question**:\nè¯·ç¼–å†™ä¸€ä¸ªç”¨äºç™»å½•çš„Servletï¼Œè¯¥Servletèƒ½å¤ŸéªŒè¯ç”¨æˆ·æäº¤çš„ç™»å½•ä¿¡æ¯ï¼Œå¹¶æ ¹æ®éªŒè¯ç»“æœè¿”å›ç›¸åº”çš„å“åº”ã€‚è¯·ç¡®ä¿ä»£ç ä¸­åŒ…å«å¿…è¦çš„å¼‚å¸¸å¤„ç†å’Œæ•°æ®åº“è¿æ¥çš„å…³é—­ã€‚\n\n**Created Answer**:\n```java\nimport java.io.IOException;\nimport java.sql.Connection;\nimport java.sql.DriverManager;\nimport java.sql.PreparedStatement;\nimport java.sql.ResultSet;\nimport java.sql.SQLException;\nimport javax.servlet.ServletException;\nimport javax.servlet.annotation.WebServlet;\nimport javax.servlet.http.HttpServlet;\nimport javax.servlet.http.HttpServletRequest;\nimport javax.servlet.http.HttpServletResponse;\n\n@WebServlet(\"/LoginServlet\")\npublic class LoginServlet extends HttpServlet {\n    private static final long serialVersionUID = 1L;\n    private static final String DB_URL = \"jdbc:mysql://localhost:3306/mydatabase\";\n    private static final String USER = \"username\";\n    private static final String PASS = \"password\";\n\n    protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        String username = request.getParameter(\"username",
            "logprobs": null,
            "finish_reason": "length",
            "stop_reason": null,
            "prompt_logprobs": null
        }
    ],
    "usage": {
        "prompt_tokens": 7,
        "total_tokens": 507,
        "completion_tokens": 500
    }
}
```

- ç”¨ `Python` è„šæœ¬è¯·æ±‚ `OpenAI Chat Completions API` 


```python
# vllm_openai_completions.py
from openai import OpenAI
client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="EMPYT", # éšä¾¿å¡«å†™ï¼Œåªæ˜¯ä¸ºäº†é€šè¿‡æ¥å£å‚æ•°æ ¡éªŒ
)

completion = client.chat.completions.create(
  model="Qwen2.5-Coder-7B-Instruct",
  messages=[
    {"role": "user", "content": "Cè¯­è¨€è¾“å‡ºæ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬98980ä¸ªæ•°å­—"}
  ]
)

print(completion.choices[0].message)
```

```shell
python vllm_openai_completions.py
```

å¾—åˆ°çš„è¿”å›å€¼å¦‚ä¸‹æ‰€ç¤º

```
ChatCompletionMessage(content=' åœ¨Cè¯­è¨€ä¸­ï¼Œè¾“å‡ºæ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬98980ä¸ªæ•°å­—æ˜¯ä¸€ä¸ªéå¸¸å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œå› ä¸ºè¿™ä¸ªæ•°å­—éå¸¸å¤§ï¼Œç”šè‡³è¶…å‡ºäº†æ ‡å‡†æ•´æ•°ç±»å‹ï¼ˆå¦‚`int`æˆ–`long long`ï¼‰çš„è¡¨ç¤ºèŒƒå›´ã€‚ä¸ºäº†å¤„ç†å¦‚æ­¤å¤§çš„æ•°å­—ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨å¤§æ•°åº“ï¼Œä¾‹å¦‚GMPï¼ˆGNU Multiple Precision Arithmetic Libraryï¼‰ã€‚\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨GMPåº“çš„ç¤ºä¾‹ä»£ç ï¼Œå±•ç¤ºå¦‚ä½•è®¡ç®—å¹¶è¾“å‡ºæ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬98980ä¸ªæ•°å­—ï¼š\n\n```c\n#include <stdio.h>\n#include <gmp.h>\n\nint main() {\n    mpz_t fibo, a, b;\n    unsigned long n = 98980;\n\n    // åˆå§‹åŒ–GMPå˜é‡\n    mpz_init(fibo);\n    mpz_init(a);\n    mpz_init(b);\n\n    // è®¾ç½®åˆå§‹æ¡ä»¶\n    mpz_set_ui(a, 0); // fibo(0)\n    mpz_set_ui(b, 1); // fibo(1)\n\n    // è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬nä¸ªæ•°å­—\n    for (unsigned long i = 2; i <= n; i++) {\n        mpz_add(fibo, a, b);\n        mpz_set(a, b);\n        mpz_set(b, fibo);\n    }\n\n    // è¾“å‡ºç»“æœ\n    gmp_printf("The %lu-th Fibonacci number is: %Zd\\n", n, fibo);\n\n    // æ¸…é™¤GMPå˜é‡\n    mpz_clear(fibo);\n    mpz_clear(a);\n    mpz_clear(b);\n\n    return 0;\n}\n```\n\n### ç¼–è¯‘å’Œè¿è¡Œ\n\n1. ç¡®ä¿ä½ å·²ç»å®‰è£…äº†GMPåº“ã€‚å¦‚æœæ²¡æœ‰å®‰è£…ï¼Œå¯ä»¥é€šè¿‡åŒ…ç®¡ç†å™¨å®‰è£…ï¼Œä¾‹å¦‚åœ¨Ubuntuä¸Šä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š\n   ```sh\n   sudo apt-get install libgmp-dev\n   ```\n\n2. ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ç¼–è¯‘ä»£ç ï¼š\n   ```sh\n   gcc -o fibonacci fibonacci.c -lgmp\n   ```\n\n3. è¿è¡Œç”Ÿæˆçš„å¯æ‰§è¡Œæ–‡ä»¶ï¼š\n   ```sh\n   ./fibonacci\n   ```\n\n### æ³¨æ„äº‹é¡¹\n\n- è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬98980ä¸ªæ•°å­—éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚\n- å³ä½¿ä½¿ç”¨GMPåº“ï¼Œè®¡ç®—ç»“æœä»ç„¶éœ€è¦éå¸¸é•¿çš„æ—¶é—´ã€‚\n- è¾“å‡ºç»“æœçš„æ ¼å¼ä½¿ç”¨äº†`gmp_printf`ï¼Œè¿™æ˜¯GMPåº“æä¾›çš„æ ¼å¼åŒ–è¾“å‡ºå‡½æ•°ã€‚\n\nè¿™ä¸ªç¤ºä¾‹ä»£ç å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨GMPåº“æ¥å¤„ç†éå¸¸å¤§çš„æ•´æ•°å¹¶è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—ã€‚ç„¶è€Œï¼Œå®é™…è®¡ç®—ç¬¬98980ä¸ªæ–æ³¢é‚£å¥‘æ•°åˆ—çš„æ•°å­—å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´ï¼Œç”šè‡³å¯èƒ½æ— æ³•åœ¨æ™®é€šè®¡ç®—æœºä¸Šå®Œæˆã€‚', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[])
```

- ç”¨ `curl` å‘½ä»¤æµ‹è¯• `OpenAI Chat Completions API` 


```bash
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen2.5-Coder-7B-Instruct",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "ä½ å¥½ï¼Œä½ éƒ½ä¼šä»€ä¹ˆç¼–ç¨‹è¯­è¨€ï¼Ÿ"}
        ]
    }'
```

å¾—åˆ°çš„è¿”å›å€¼å¦‚ä¸‹æ‰€ç¤º

```json
{
    "id": "chat-0260307aca3e427cab90ab678a9e9196",
    "object": "chat.completion",
    "created": 1731659954,
    "model": "Qwen2.5-Coder-7B-Instruct",
    "choices": [
        {
            "index": 0,
            "message": {
                "role": "assistant",
                "content": "ä½ å¥½ï¼Œæˆ‘æ“…é•¿çš„ç¼–ç¨‹è¯­è¨€åŒ…æ‹¬Pythonã€Javaã€C++å’ŒJavaScriptã€‚",
                "tool_calls": [

                ]
            },
            "logprobs": null,
            "finish_reason": "stop",
            "stop_reason": null
        }
    ],
    "usage": {
        "prompt_tokens": 27,
        "total_tokens": 45,
        "completion_tokens": 18
    },
    "prompt_logprobs": null
}
```

å¦å¤–ï¼Œåœ¨ä»¥ä¸Šæ‰€æœ‰çš„åœ¨è¯·æ±‚å¤„ç†è¿‡ç¨‹ä¸­ï¼Œ `API` åç«¯éƒ½ä¼šæ‰“å°ç›¸å¯¹åº”çš„æ—¥å¿—å’Œç»Ÿè®¡ä¿¡æ¯ğŸ˜Š
![04-4](./images/04-4.png)



## æ¨ç†é€Ÿåº¦æµ‹è¯•

æ—¢ç„¶ `vLLM` æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†å’Œéƒ¨ç½²æœåŠ¡ç³»ç»Ÿï¼Œé‚£ä¹ˆæˆ‘ä»¬ä¸å¦¨å°±æµ‹è¯•ä¸€ä¸‹æ¨¡å‹çš„å›å¤ç”Ÿæˆé€Ÿåº¦ã€‚çœ‹çœ‹å’ŒåŸå§‹çš„é€Ÿåº¦ç›¸æ¯”æœ‰å¤šå¤§çš„æå‡ã€‚è¿™é‡Œç›´æ¥ä½¿ç”¨ `vLLM` è‡ªå¸¦çš„ `benchmark_throughput.py` è„šæœ¬è¿›è¡Œæµ‹è¯•ã€‚å¯ä»¥å°†å½“å‰æ–‡ä»¶å¤¹ `benchmark_throughput.py` è„šæœ¬æ”¾åœ¨ `/root/autodl-tmp/` ç›®å½•ä¸‹ï¼›æˆ–è€…ä¹Ÿå¯ä»¥è‡ªè¡Œ[ä¸‹è½½æœ€æ–°ç‰ˆè„šæœ¬](https://github.com/vllm-project/vllm/blob/main/benchmarks/benchmark_throughput.py)

ä¸‹é¢æ˜¯ä¸€äº› `benchmark_throughput.py` è„šæœ¬çš„å‚æ•°è¯´æ˜ï¼š

- `--model` å‚æ•°æŒ‡å®šæ¨¡å‹è·¯å¾„æˆ–åç§°ã€‚
- `--backend` æ¨ç†åç«¯ï¼Œå¯ä»¥æ˜¯ `vllm`ã€`hf` å’Œ `mii`ã€‚åˆ†å¸ƒå¯¹åº” `vLLM`ã€`HuggingFace` å’Œ `Mii` æ¨ç†åç«¯ã€‚
- `--input-len` è¾“å…¥é•¿åº¦
- `--output-len` è¾“å‡ºé•¿åº¦
- `--num-prompts` ç”Ÿæˆçš„ prompt æ•°é‡
- `--seed` éšæœºç§å­
- `--dtype` æ•°æ®ç±»å‹
- `--max-model-len` æ¨¡å‹æœ€å¤§é•¿åº¦
- `--hf_max_batch_size` `transformers` åº“çš„æœ€å¤§æ‰¹å¤„ç†å¤§å°ï¼ˆä»…ä»…å¯¹äº `hf` æ¨ç†åç«¯æœ‰æ•ˆä¸”ä¸ºå¿…å¡«å­—æ®µï¼‰
- `--dataset` æ•°æ®é›†è·¯å¾„ã€‚ï¼ˆå¦‚æœªè®¾ç½®ä¼šè‡ªåŠ¨ç”Ÿæˆæ•°æ®ï¼‰



æµ‹è¯• `vLLM` æ¨ç†é€Ÿåº¦çš„å‘½ä»¤å’Œå‚æ•°è®¾ç½®

```bash
python benchmark_throughput.py \
	--model /root/autodl-tmp/Qwen2.5-Coder-7B-Instruct \
	--backend vllm \
	--input-len 64 \
	--output-len 128 \
	--num-prompts 25 \
	--seed 2024 \
    --dtype float16 \
    --max-model-len 512
```

å¾—åˆ°çš„ç»“æœå¦‚ä¸‹æ‰€ç¤º

```
Throughput: 8.00 requests/s, 1536.15 total tokens/s, 1024.10 output tokens/s
```



æµ‹è¯•å…¶ä»–æ–¹å¼ï¼ˆå³ä½¿ç”¨ `HuggingFace` çš„ `Transformers` åº“ï¼‰æ¨ç†é€Ÿåº¦çš„å‘½ä»¤å’Œå‚æ•°è®¾ç½®

```bash
python benchmark_throughput.py \
	--model /root/autodl-tmp/Qwen2.5-Coder-7B-Instruct \
	--backend hf \
	--input-len 64 \
	--output-len 128 \
	--num-prompts 25 \
	--seed 2024 \
	--dtype float16 \
    --hf-max-batch-size 25
```

å¾—åˆ°çš„ç»“æœå¦‚ä¸‹æ‰€ç¤º

```
Throughput: 5.64 requests/s, 1083.59 total tokens/s, 722.39 output tokens/s
```


å¯¹æ¯”ä¸¤è€…çš„æ¨ç†é€Ÿåº¦ï¼Œåœ¨æœ¬æ¬¡æµ‹è¯• ï¼ˆå•å¡ `RTX3090 24G` ï¼‰ä¸­ `vLLM` çš„é€Ÿåº¦è¦æ¯”åŸå§‹çš„é€Ÿåº¦å¿« **34%** å·¦å³ ğŸ¤—

> **æ³¨æ„ï¼š**æœ¬æ¬¡æµ‹è¯•å¹¶éä¸¥è°¨çš„æµ‹è¯•ï¼Œä¸”æ¯ä¸ªäººçš„æœºå™¨é…ç½®å’Œç¯å¢ƒéƒ½å¯èƒ½å­˜åœ¨å·®å¼‚ï¼Œå› æ­¤ä¸Šè¿°å®éªŒç»“æœä»…ä¾›ä½œä¸º `case` å‚è€ƒï¼Œè¯»è€…å¯ä»¥åœ¨è‡ªå·±çš„ç¯å¢ƒä¸­å–å¤šä¸ªæµ‹è¯•ç”¨ä¾‹å¹¶å¤šæ¬¡å®éªŒå–å¹³å‡ä»¥å¾—åˆ°ä¸¥è°¨çš„å®éªŒç»“è®ºã€‚
