# Index-1.9B-Chat Lora å¾®è°ƒ

æœ¬èŠ‚æˆ‘ä»¬ç®€è¦ä»‹ç»å¦‚ä½•åŸºäº `transformers`ã€`peft` ç­‰æ¡†æ¶ï¼Œå¯¹å“”å“©å“”å“©çš„ `Index-1.9B-Chat` å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œ `Lora` å¾®è°ƒã€‚`Lora` æ˜¯ä¸€ç§é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œæ·±å…¥äº†è§£å…¶åŸç†å¯å‚è§åšå®¢ï¼š[çŸ¥ä¹|æ·±å…¥æµ…å‡ºLora](https://zhuanlan.zhihu.com/p/650197598)ã€‚

è¿™ä¸ªæ•™ç¨‹ä¼šåœ¨åŒç›®å½•ä¸‹ç»™å¤§å®¶æä¾›ä¸€ä¸ª [Notebook](./04-Index-1.9B-Chat%20Lora.ipynb) æ–‡ä»¶ï¼Œæ¥å¸®åŠ©å¤§å®¶æ›´å¥½åœ°å­¦ä¹ ã€‚



## **ç¯å¢ƒå‡†å¤‡**

æœ¬æ–‡åŸºç¡€ç¯å¢ƒå¦‚ä¸‹ï¼š

```
----------------
ubuntu 22.04
python 3.12
cuda 12.1
pytorch 2.3.0
----------------
```

> æœ¬æ–‡é»˜è®¤å­¦ä¹ è€…å·²å®‰è£…å¥½ä»¥ä¸Š `PyTorch` (`cuda`) ç¯å¢ƒï¼Œå¦‚æœªå®‰è£…è¯·è‡ªè¡Œå®‰è£…ã€‚

æ¥ä¸‹æ¥å¼€å§‹ç¯å¢ƒé…ç½®ã€æ¨¡å‹ä¸‹è½½å’Œè¿è¡Œæ¼”ç¤º ~

`pip` æ¢æºåŠ é€Ÿä¸‹è½½å¹¶å®‰è£…ä¾èµ–åŒ…

```bash
python -m pip install --upgrade pip
# æ›´æ¢ pypi æºåŠ é€Ÿåº“çš„å®‰è£…
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

pip install modelscope==1.16.1
pip install transformers==4.43.2
pip install accelerate==0.32.1
pip install peft==0.11.1
pip install datasets==2.20.0
```

> è€ƒè™‘åˆ°éƒ¨åˆ†åŒå­¦é…ç½®ç¯å¢ƒå¯èƒ½ä¼šé‡åˆ°ä¸€äº›é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨ `AutoDL` å¹³å°å‡†å¤‡äº† `Index-1.9B-Chat` çš„ç¯å¢ƒé•œåƒã€‚ç‚¹å‡»ä¸‹æ–¹é“¾æ¥å¹¶ç›´æ¥åˆ›å»º `Autodl` ç¤ºä¾‹å³å¯ã€‚
> ***https://www.codewithgpu.com/i/datawhalechina/self-llm/Index***

åœ¨æœ¬èŠ‚æ•™ç¨‹é‡Œï¼Œæˆ‘ä»¬å°†å¾®è°ƒæ•°æ®é›†æ”¾ç½®åœ¨æ ¹ç›®å½• [/dataset](../../dataset/huanhuan.json)ã€‚



## æ¨¡å‹ä¸‹è½½  

ä½¿ç”¨ `modelscope` ä¸­çš„ `snapshot_download` å‡½æ•°ä¸‹è½½æ¨¡å‹ï¼Œç¬¬ä¸€ä¸ªå‚æ•°ä¸ºæ¨¡å‹åç§°ï¼Œå‚æ•° `cache_dir` ä¸ºè‡ªå®šä¹‰çš„æ¨¡å‹ä¸‹è½½è·¯å¾„ï¼Œå‚æ•°`revision`ä¸ºæ¨¡å‹ä»“åº“åˆ†æ”¯ç‰ˆæœ¬ï¼Œ`master `ä»£è¡¨ä¸»åˆ†æ”¯ï¼Œä¹Ÿæ˜¯ä¸€èˆ¬æ¨¡å‹ä¸Šä¼ çš„é»˜è®¤åˆ†æ”¯ã€‚

å…ˆåˆ‡æ¢åˆ° `autodl-tmp` ç›®å½•ï¼Œ`cd /root/autodl-tmp` 

ç„¶åæ–°å»ºåä¸º `model_download.py` çš„ `python` æ–‡ä»¶ï¼Œå¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹å¹¶ä¿å­˜

```python
# model_download.py
from modelscope import snapshot_download

model_dir = snapshot_download('deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct', cache_dir='/root/autodl-tmp', revision='master')
```

ç„¶ååœ¨ç»ˆç«¯ä¸­è¾“å…¥ `python model_download.py` æ‰§è¡Œä¸‹è½½ï¼Œæ³¨æ„è¯¥æ¨¡å‹æƒé‡æ–‡ä»¶æ¯”è¾ƒå¤§ï¼Œå› æ­¤è¿™é‡Œéœ€è¦è€å¿ƒç­‰å¾…ä¸€æ®µæ—¶é—´ç›´åˆ°æ¨¡å‹ä¸‹è½½å®Œæˆã€‚

> æ³¨æ„ï¼šè®°å¾—ä¿®æ”¹ `cache_dir` ä¸ºä½ çš„æ¨¡å‹ä¸‹è½½è·¯å¾„å“¦~

![](images/image01-0.png)



## æŒ‡ä»¤é›†æ„å»º

`LLM` çš„å¾®è°ƒä¸€èˆ¬æŒ‡æŒ‡ä»¤å¾®è°ƒè¿‡ç¨‹ã€‚æ‰€è°“æŒ‡ä»¤å¾®è°ƒï¼Œæ˜¯è¯´æˆ‘ä»¬ä½¿ç”¨çš„å¾®è°ƒæ•°æ®å½¢å¦‚ğŸ‘‡

```json
{
    "instruction":"å›ç­”ä»¥ä¸‹ç”¨æˆ·é—®é¢˜ï¼Œä»…è¾“å‡ºç­”æ¡ˆã€‚",
    "input":"1+1ç­‰äºå‡ ?",
    "output":"2"
}
```

å…¶ä¸­ï¼Œ`instruction` æ˜¯ç”¨æˆ·æŒ‡ä»¤ï¼Œå‘ŠçŸ¥æ¨¡å‹å…¶éœ€è¦å®Œæˆçš„ä»»åŠ¡ï¼›`input` æ˜¯ç”¨æˆ·è¾“å…¥ï¼Œæ˜¯å®Œæˆç”¨æˆ·æŒ‡ä»¤æ‰€å¿…é¡»çš„è¾“å…¥å†…å®¹ï¼›`output` æ˜¯æ¨¡å‹åº”è¯¥ç»™å‡ºçš„è¾“å‡ºã€‚

å³æˆ‘ä»¬çš„æ ¸å¿ƒè®­ç»ƒç›®æ ‡æ˜¯è®©æ¨¡å‹å…·æœ‰ç†è§£å¹¶éµå¾ªç”¨æˆ·æŒ‡ä»¤çš„èƒ½åŠ›ã€‚å› æ­¤ï¼Œåœ¨æŒ‡ä»¤é›†æ„å»ºæ—¶ï¼Œæˆ‘ä»¬åº”é’ˆå¯¹æˆ‘ä»¬çš„ç›®æ ‡ä»»åŠ¡ï¼Œé’ˆå¯¹æ€§æ„å»ºä»»åŠ¡æŒ‡ä»¤é›†ã€‚ä¾‹å¦‚ï¼Œåœ¨æœ¬èŠ‚æˆ‘ä»¬ä½¿ç”¨ [Chat-ç”„å¬›](https://github.com/KMnO4-zx/huanhuan-chat) é¡¹ç›®ä½œä¸ºç¤ºä¾‹ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ„å»ºä¸€ä¸ªèƒ½å¤Ÿæ¨¡æ‹Ÿç”„å¬›å¯¹è¯é£æ ¼çš„ä¸ªæ€§åŒ– `LLM`ï¼Œå› æ­¤æˆ‘ä»¬æ„é€ çš„æŒ‡ä»¤å½¢å¦‚ï¼š

```json
{
    "instruction": "ä½ æ˜¯è°ï¼Ÿ",
    "input":"",
    "output":"å®¶çˆ¶æ˜¯å¤§ç†å¯ºå°‘å¿ç”„è¿œé“ã€‚"
}
```

æˆ‘ä»¬æ‰€æ„é€ çš„å…¨éƒ¨æŒ‡ä»¤æ•°æ®é›†åœ¨æ ¹ç›®å½•ä¸‹ã€‚




## æ•°æ®æ ¼å¼åŒ–

`Lora` è®­ç»ƒçš„æ•°æ®æ˜¯éœ€è¦ç»è¿‡æ ¼å¼åŒ–ã€ç¼–ç ä¹‹åå†è¾“å…¥ç»™æ¨¡å‹è¿›è¡Œè®­ç»ƒçš„ï¼Œå¦‚æœæ˜¯ç†Ÿæ‚‰ `Pytorch` æ¨¡å‹è®­ç»ƒæµç¨‹çš„åŒå­¦ä¼šçŸ¥é“ï¼Œæˆ‘ä»¬ä¸€èˆ¬éœ€è¦å°†è¾“å…¥æ–‡æœ¬ç¼–ç ä¸º `input_ids`ï¼Œå°†è¾“å‡ºæ–‡æœ¬ç¼–ç ä¸º `labels`ï¼Œç¼–ç ä¹‹åçš„ç»“æœéƒ½æ˜¯å¤šç»´çš„å‘é‡ã€‚æˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€ä¸ªé¢„å¤„ç†å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°ç”¨äºå¯¹æ¯ä¸€ä¸ªæ ·æœ¬ï¼Œç¼–ç å…¶è¾“å…¥ã€è¾“å‡ºæ–‡æœ¬å¹¶è¿”å›ä¸€ä¸ªç¼–ç åçš„å­—å…¸ï¼š

```python
def process_func(example):
    MAX_LENGTH = 384    # åˆ†è¯å™¨ä¼šå°†ä¸€ä¸ªä¸­æ–‡å­—åˆ‡åˆ†ä¸ºå¤šä¸ªtokenï¼Œå› æ­¤éœ€è¦æ”¾å¼€ä¸€äº›æœ€å¤§é•¿åº¦ï¼Œä¿è¯æ•°æ®çš„å®Œæ•´æ€§
    input_ids, attention_mask, labels = [], [], []
    instruction = tokenizer(f"<unk>systemç°åœ¨ä½ è¦æ‰®æ¼”çš‡å¸èº«è¾¹çš„å¥³äºº--ç”„å¬›reserved_0user{example['instruction'] + example['input']}reserved_1assistant", add_special_tokens=False)  # add_special_tokens ä¸åœ¨å¼€å¤´åŠ  special_tokens
    response = tokenizer(f"{example['output']}", add_special_tokens=False)
    input_ids = instruction["input_ids"] + response["input_ids"] + [tokenizer.pad_token_id]
    attention_mask = instruction["attention_mask"] + response["attention_mask"] + [1]  # å› ä¸ºeos tokenå’±ä»¬ä¹Ÿæ˜¯è¦å…³æ³¨çš„æ‰€ä»¥ è¡¥å……ä¸º1
    labels = [-100] * len(instruction["input_ids"]) + response["input_ids"] + [tokenizer.pad_token_id]  
    if len(input_ids) > MAX_LENGTH:  # åšä¸€ä¸ªæˆªæ–­
        input_ids = input_ids[:MAX_LENGTH]
        attention_mask = attention_mask[:MAX_LENGTH]
        labels = labels[:MAX_LENGTH]
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }
```

`Index-1.9B-Chat` é‡‡ç”¨çš„`Prompt Template`æ ¼å¼å¦‚ä¸‹ğŸ‘‡

```text
<unk>systemç°åœ¨ä½ è¦æ‰®æ¼”çš‡å¸èº«è¾¹çš„å¥³äºº--ç”„å¬› reserved_0 userå°å§ï¼Œåˆ«çš„ç§€å¥³éƒ½åœ¨æ±‚ä¸­é€‰ï¼Œå”¯æœ‰å’±ä»¬å°å§æƒ³è¢«æ’‚ç‰Œå­ï¼Œè©è¨ä¸€å®šè®°å¾—çœŸçœŸå„¿çš„â€”â€” reserved_1 assistantå˜˜â€”â€”éƒ½è¯´è®¸æ„¿è¯´ç ´æ˜¯ä¸çµçš„ã€‚<unk>
```



## åŠ è½½tokenizerå’ŒåŠç²¾åº¦æ¨¡å‹

æ¨¡å‹ä»¥åŠç²¾åº¦å½¢å¼åŠ è½½ï¼Œå¦‚æœä½ çš„æ˜¾å¡æ¯”è¾ƒæ–°çš„è¯ï¼Œå¯ä»¥ç”¨`torch.bfolat`å½¢å¼åŠ è½½ã€‚å¯¹äºè‡ªå®šä¹‰çš„æ¨¡å‹ä¸€å®šè¦æŒ‡å®š`trust_remote_code`å‚æ•°ä¸º`True`ã€‚

```python
tokenizer = AutoTokenizer.from_pretrained('/root/autodl-tmp/Tsumugii24/Index-1.9B-Chat/', use_fast=False, trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained('/root/autodl-tmp/Tsumugii24/Index-1.9B-Chat/', device_map="auto", trust_remote_code=True, torch_dtype=torch.bfloat16)
```



## å®šä¹‰LoraConfig

`LoraConfig`è¿™ä¸ªç±»ä¸­å¯ä»¥è®¾ç½®å¾ˆå¤šå‚æ•°ï¼Œä½†ä¸»è¦çš„å‚æ•°æ²¡å¤šå°‘ï¼Œç®€å•è®²ä¸€è®²ï¼Œæ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥ç›´æ¥çœ‹æºç ã€‚

- `task_type`ï¼šæ¨¡å‹ç±»å‹
- `target_modules`ï¼šéœ€è¦è®­ç»ƒçš„æ¨¡å‹å±‚çš„åå­—ï¼Œä¸»è¦å°±æ˜¯`attention`éƒ¨åˆ†çš„å±‚ï¼Œä¸åŒçš„æ¨¡å‹å¯¹åº”çš„å±‚çš„åå­—ä¸åŒï¼Œå¯ä»¥ä¼ å…¥æ•°ç»„ï¼Œä¹Ÿå¯ä»¥å­—ç¬¦ä¸²ï¼Œä¹Ÿå¯ä»¥æ­£åˆ™è¡¨è¾¾å¼ã€‚
- `r`ï¼š`lora`çš„ç§©ï¼Œå…·ä½“å¯ä»¥çœ‹`Lora`åŸç†
- `lora_alpha`ï¼š`Lora alaph`ï¼Œå…·ä½“ä½œç”¨å‚è§ `Lora` åŸç† 

`Lora`çš„ç¼©æ”¾æ˜¯å•¥å˜ï¼Ÿå½“ç„¶ä¸æ˜¯`r`ï¼ˆç§©ï¼‰ï¼Œè¿™ä¸ªç¼©æ”¾å°±æ˜¯`lora_alpha/r`, åœ¨è¿™ä¸ª`LoraConfig`ä¸­ç¼©æ”¾å°±æ˜¯4å€ã€‚

```python
config = LoraConfig(
    task_type=TaskType.CAUSAL_LM, 
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    inference_mode=False, # è®­ç»ƒæ¨¡å¼
    r=8, # Lora ç§©
    lora_alpha=32, # Lora alaphï¼Œå…·ä½“ä½œç”¨å‚è§ Lora åŸç†
    lora_dropout=0.1 # Dropout æ¯”ä¾‹
)
```



## è‡ªå®šä¹‰ TrainingArguments å‚æ•°

`TrainingArguments`è¿™ä¸ªç±»çš„æºç ä¹Ÿä»‹ç»äº†æ¯ä¸ªå‚æ•°çš„å…·ä½“ä½œç”¨ï¼Œå½“ç„¶å¤§å®¶å¯ä»¥æ¥è‡ªè¡Œæ¢ç´¢ï¼Œè¿™é‡Œå°±ç®€å•è¯´å‡ ä¸ªå¸¸ç”¨çš„ã€‚

- `output_dir`ï¼šæ¨¡å‹çš„è¾“å‡ºè·¯å¾„
- `per_device_train_batch_size`ï¼šé¡¾åæ€ä¹‰ `batch_size`
- `gradient_accumulation_steps`:  æ¢¯åº¦ç´¯åŠ ï¼Œå¦‚æœæ˜¾å­˜è¾ƒå°ï¼Œé‚£å¯ä»¥æŠŠ `batch_size` è®¾ç½®å°ä¸€ç‚¹ï¼Œæ¢¯åº¦ç´¯åŠ å¢å¤§ä¸€äº›
- `logging_steps`ï¼šè¾“å‡ºä¸€æ¬¡æ—¥å¿—æ‰€éœ€çš„æ­¥æ•°
- `num_train_epochs`ï¼šé¡¾åæ€ä¹‰ `epoch`
- `gradient_checkpointing`ï¼šæ¢¯åº¦æ£€æŸ¥ï¼Œè¿™ä¸ªä¸€æ—¦å¼€å¯ï¼Œæ¨¡å‹å°±å¿…é¡»æ‰§è¡Œ`model.enable_input_require_grads()`ï¼Œè¿™ä¸ªåŸç†å¤§å®¶å¯ä»¥è‡ªè¡Œæ¢ç´¢ï¼Œè¿™é‡Œå°±ä¸ç»†è¯´äº†

```python
args = TrainingArguments(
    output_dir="./output/Index-1.9B-Chat-lora",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    logging_steps=10,
    num_train_epochs=3,
    save_steps=100,
    learning_rate=1e-4,
    save_on_each_node=True,
    gradient_checkpointing=True
)
```



## ä½¿ç”¨ Trainer è®­ç»ƒ

```python
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_id,
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),
)
trainer.train()
```



## åŠ è½½ lora æƒé‡æ¨ç†

è®­ç»ƒå¥½äº†ä¹‹åå¯ä»¥ä½¿ç”¨å¦‚ä¸‹æ–¹å¼åŠ è½½ `lora` æƒé‡è¿›è¡Œæ¨ç†ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from peft import PeftModel

model_path = '/root/autodl-tmp/Tsumugii24/Index-1.9B-Chat/'
lora_path = 'lora_path'

# åŠ è½½tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path)

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(model_path, device_map="auto",torch_dtype=torch.bfloat16)

# åŠ è½½loraæƒé‡
model = PeftModel.from_pretrained(model, model_id=lora_path, config=config)

prompt = "ä½ æ˜¯è°ï¼Ÿ"
messages = [
    {"role": "system", "content": "ç°åœ¨ä½ è¦æ‰®æ¼”çš‡å¸èº«è¾¹çš„å¥³äºº--ç”„å¬›"},
    {"role": "user", "content": prompt}
]

text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

model_inputs = tokenizer([text], return_tensors="pt").to('cuda')

generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

print(response)
```

