# Yuan2.0-M32 WebDemoéƒ¨ç½²

## ç¯å¢ƒå‡†å¤‡

åœ¨ Autodl å¹³å°ä¸­ç§Ÿèµä¸€ä¸ª RTX 3090/24G æ˜¾å­˜çš„æ˜¾å¡æœºå™¨ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œé•œåƒé€‰æ‹© PyTorch-->2.1.0-->3.10(ubuntu22.04)-->12.1ã€‚

![å¼€å¯æœºå™¨é…ç½®é€‰æ‹©](images/01-1.png)

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ‰“å¼€åˆšåˆšç§Ÿç”¨æœåŠ¡å™¨çš„ JupyterLabï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![å¼€å¯JupyterLab](images/01-2.png)

ç„¶åæ‰“å¼€å…¶ä¸­çš„ç»ˆç«¯ï¼Œå¼€å§‹ç¯å¢ƒé…ç½®ã€æ¨¡å‹ä¸‹è½½å’Œè¿è¡Œæ¼”ç¤ºã€‚  

![å¼€å¯ç»ˆç«¯](images/01-3.png)

## ç¯å¢ƒé…ç½®

Yuan2-M32-HF-INT4æ˜¯ç”±åŸå§‹çš„Yuan2-M32-HFç»è¿‡auto-gptqé‡åŒ–è€Œæ¥çš„æ¨¡å‹ã€‚

é€šè¿‡æ¨¡å‹é‡åŒ–ï¼Œéƒ¨ç½²Yuan2-M32-HF-INT4å¯¹æ˜¾å­˜å’Œç¡¬ç›˜çš„è¦æ±‚éƒ½ä¼šæ˜¾è‘—å‡ä½ã€‚

æ³¨ï¼šç”±äºpipç‰ˆæœ¬çš„auto-gptqç›®å‰è¿˜ä¸æ”¯æŒYuan2.0 M32ï¼Œå› æ­¤éœ€è¦ç¼–è¯‘å®‰è£…

```shell
# å‡çº§pip
python -m pip install --upgrade pip

# æ›´æ¢ pypi æºåŠ é€Ÿåº“çš„å®‰è£…
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

# æ‹‰å–Yuan2.0-M32é¡¹ç›®
git clone https://github.com/IEIT-Yuan/Yuan2.0-M32.git

# è¿›å…¥AutoGPTQ
cd  Yuan2.0-M32/3rd_party/AutoGPTQ

# å®‰è£…autogptq
pip install --no-build-isolation -e .

# å®‰è£… einops modelscope streamlit
pip install einops modelscope streamlit==1.24.0
```

> è€ƒè™‘åˆ°éƒ¨åˆ†åŒå­¦é…ç½®ç¯å¢ƒå¯èƒ½ä¼šé‡åˆ°ä¸€äº›é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨AutoDLå¹³å°å‡†å¤‡äº†Yuan2.0-M32çš„é•œåƒï¼Œç‚¹å‡»ä¸‹æ–¹é“¾æ¥å¹¶ç›´æ¥åˆ›å»ºAutodlç¤ºä¾‹å³å¯ã€‚
> ***https://www.codewithgpu.com/i/datawhalechina/self-llm/Yuan2.0-M32***


## æ¨¡å‹ä¸‹è½½  

ä½¿ç”¨ modelscope ä¸­çš„ snapshot_download å‡½æ•°ä¸‹è½½æ¨¡å‹ï¼Œç¬¬ä¸€ä¸ªå‚æ•°ä¸ºæ¨¡å‹åç§°ï¼Œå‚æ•° cache_dir ä¸ºæ¨¡å‹çš„ä¸‹è½½è·¯å¾„ã€‚

è¿™é‡Œå¯ä»¥å…ˆè¿›å…¥autodlå¹³å°ï¼Œåˆå§‹åŒ–æœºå™¨å¯¹åº”åŒºåŸŸçš„çš„æ–‡ä»¶å­˜å‚¨ï¼Œæ–‡ä»¶å­˜å‚¨è·¯å¾„ä¸º'/root/autodl-fs'ã€‚
è¯¥å­˜å‚¨ä¸­çš„æ–‡ä»¶ä¸ä¼šéšç€æœºå™¨çš„å…³é—­è€Œä¸¢å¤±ï¼Œè¿™æ ·å¯ä»¥é¿å…æ¨¡å‹äºŒæ¬¡ä¸‹è½½ã€‚

![autodl-fs](images/autodl-fs.png)

ç„¶åè¿è¡Œä¸‹é¢ä»£ç ï¼Œæ‰§è¡Œæ¨¡å‹ä¸‹è½½ã€‚

```python
from modelscope import snapshot_download
model_dir = snapshot_download('YuanLLM/Yuan2-M32-HF-INT4', cache_dir='/root/autodl-fs')
``` 

## æ¨¡å‹åˆå¹¶

ä¸‹è½½åçš„æ¨¡å‹ä¸ºå¤šä¸ªæ–‡ä»¶ï¼Œéœ€è¦å°†å…¶è¿›è¡Œåˆå¹¶ã€‚

```shell
cat /root/autodl-fs/YuanLLM/Yuan2-M32-HF-INT4/gptq_model-4bit-128g.safetensors*  > /root/autodl-fs/YuanLLM/Yuan2-M32-HF-INT4/gptq_model-4bit-128g.safetensors
```

## ä»£ç å‡†å¤‡

åœ¨`/root/autodl-tmp`è·¯å¾„ä¸‹æ–°å»º `chatBot.py` æ–‡ä»¶å¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼Œç²˜è´´ä»£ç åè®°å¾—ä¿å­˜æ–‡ä»¶ã€‚ä¸‹é¢çš„ä»£ç æœ‰å¾ˆè¯¦ç»†çš„æ³¨é‡Šï¼Œå¤§å®¶å¦‚æœ‰ä¸ç†è§£çš„åœ°æ–¹ï¼Œæ¬¢è¿æå‡ºissueã€‚

chatBot.pyä»£ç å¦‚ä¸‹

```python
# å¯¼å…¥æ‰€éœ€çš„åº“
from auto_gptq import AutoGPTQForCausalLM
from transformers import LlamaTokenizer
import torch
import streamlit as st

# åœ¨ä¾§è¾¹æ ä¸­åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªé“¾æ¥
with st.sidebar:
    st.markdown("## Yuan2.0-M32 LLM")
    "[å¼€æºå¤§æ¨¡å‹é£Ÿç”¨æŒ‡å— self-llm](https://github.com/datawhalechina/self-llm.git)"
    # åˆ›å»ºä¸€ä¸ªæ»‘å—ï¼Œç”¨äºé€‰æ‹©æœ€å¤§é•¿åº¦ï¼ŒèŒƒå›´åœ¨0åˆ°1024ä¹‹é—´ï¼Œé»˜è®¤å€¼ä¸º512
    max_length = st.slider("max_length", 0, 1024, 512, step=1)

# åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªå‰¯æ ‡é¢˜
st.title("ğŸ’¬ Yuan2.0-M32 Chatbot")
st.caption("ğŸš€ A streamlit chatbot powered by Self-LLM")

# å®šä¹‰æ¨¡å‹è·¯å¾„
path = '/root/autodl-fs/YuanLLM/Yuan2-M32-HF-INT4'

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè·å–æ¨¡å‹å’Œtokenizer
@st.cache_resource
def get_model():
    print("Creat tokenizer...")
    tokenizer = LlamaTokenizer.from_pretrained(path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')
    tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>','<commit_before>','<commit_msg>','<commit_after>','<jupyter_start>','<jupyter_text>','<jupyter_code>','<jupyter_output>','<empty_output>'], special_tokens=True)

    print("Creat model...")
    model = AutoGPTQForCausalLM.from_quantized(path, trust_remote_code=True).cuda()
  
    return tokenizer, model

# åŠ è½½modelå’Œtokenizer
tokenizer, model = get_model()

# å¦‚æœsession_stateä¸­æ²¡æœ‰"messages"ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªåŒ…å«é»˜è®¤æ¶ˆæ¯çš„åˆ—è¡¨
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„ï¼Ÿ"}]

# éå†session_stateä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œå¹¶æ˜¾ç¤ºåœ¨èŠå¤©ç•Œé¢ä¸Š
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# å¦‚æœç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ¡†ä¸­è¾“å…¥äº†å†…å®¹ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
if prompt := st.chat_input():
    # å°†ç”¨æˆ·çš„è¾“å…¥æ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "user", "content": prompt})

    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºç”¨æˆ·çš„è¾“å…¥
    st.chat_message("user").write(prompt)

    # è°ƒç”¨æ¨¡å‹
    input_str = "<n>".join(msg["content"] for msg in st.session_state.messages) + "<sep>"
    inputs = tokenizer(input_str, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, do_sample=False, max_new_tokens=256)
    output = tokenizer.decode(outputs[0])
    response = output.split("<sep>")[-1].replace("<eod>", '')

    # å°†æ¨¡å‹çš„è¾“å‡ºæ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "assistant", "content": response})

    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
    st.chat_message("assistant").write(response)

    # print(st.session_state)
```

# é…ç½®vscode ssh

å¤åˆ¶æœºå™¨sshç™»å½•æŒ‡ä»¤

![](images/03-0.png)

ç²˜è´´åˆ°æœ¬åœ°ç”µè„‘çš„.ssh/configï¼Œå¹¶ä¿®æ”¹æˆå¦‚ä¸‹æ ¼å¼

![](images/03-1.png)

ç„¶åè¿æ¥åˆ°æ­¤sshï¼Œé€‰æ‹©linx

![](images/03-2.png)

å¤åˆ¶å¯†ç å¹¶è¾“å…¥ï¼ŒæŒ‰ä¸‹å›è½¦å³å¯ç™»å½•åˆ°æœºå™¨

## è¿è¡Œdemo

åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œå¯åŠ¨streamlitæœåŠ¡

```shell
streamlit run chatBot.py --server.address 127.0.0.1 --server.port 6006
```

![](images/03-3.png)


ç‚¹å‡»åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ï¼Œå³å¯çœ‹åˆ°èŠå¤©ç•Œé¢ã€‚

è¿è¡Œæ•ˆæœå¦‚ä¸‹ï¼š

![](images/03-4.png)

