# Gemma3-4B GRPOå¾®è°ƒæ•™ç¨‹

> è¯ä¸å¤šè¯´ï¼Œç›´æ¥å¼€å§‹ï¼

æœ¬æ–‡ä½¿ç”¨çš„æµ‹è¯•ç¯å¢ƒä¸ºå•å¼  A100ï¼Œæ˜¾å­˜ 80GBï¼Œå¯æ ¹æ®éœ€æ±‚åˆ‡æ¢ä¸åŒå‚æ•°é‡çš„æ¨¡å‹ï¼Œå®æµ‹4B 24Gæ˜¾å­˜ is enoughï¼
ä½¿ç”¨çš„æ¡†æ¶ä¸º Unsloth
![image06-1](./images/image06-1.png)
Unsloth æ˜¯ä¸€ä¸ªæå…¶å¼ºè°ƒèµ„æºèŠ‚çœçš„æ¡†æ¶ï¼ŒæŠŠæ‰€æœ‰çš„èµ„æºèŠ‚çœåšåˆ°äº†æè‡´ï¼Œå…·ä½“æ¥è®²Unslothèƒ½å¤Ÿå°† Llama-3ã€Mistralã€Phi-4 å’Œ Gemma ç­‰å¤§å‹è¯­è¨€æ¨¡å‹çš„å¾®è°ƒé€Ÿåº¦æå‡ 2 å€ï¼Œå†…å­˜å ç”¨å‡å°‘ 70%ï¼Œå¹¶ä¸”å‡†ç¡®ç‡æ²¡æœ‰ä»»ä½•ä¸‹é™ï¼
å®˜æ–¹æ–‡æ¡£éå¸¸å…¨é¢ï¼Œè¯¦ç»†æŒ‡å¯¼äº†å¦‚ä½•è®­ç»ƒè‡ªå·±çš„å®šåˆ¶æ¨¡å‹ã€‚å…¶ä¸­æ¶µç›–äº†å®‰è£…å’Œæ›´æ–° Unslothã€åˆ›å»ºæ•°æ®é›†ã€è¿è¡Œå’Œéƒ¨ç½²æ¨¡å‹ç­‰åŸºæœ¬è¦ç´ ã€‚ Unsloth è®©å¤§å®¶åœ¨æœ¬åœ°æˆ–åœ¨ Google Colab å’Œ Kaggle ç­‰å¹³å°ä¸Šè®­ç»ƒåƒ Llama 3 è¿™æ ·çš„æ¨¡å‹å˜å¾—æå…¶ç®€å•ã€‚Unslothç®€åŒ–äº†æ•´ä¸ªè®­ç»ƒå·¥ä½œæµç¨‹ï¼ŒåŒ…æ‹¬æ¨¡å‹åŠ è½½ã€é‡åŒ–ã€è®­ç»ƒã€è¯„ä¼°ã€è¿è¡Œã€ä¿å­˜ã€å¯¼å‡ºï¼Œä»¥åŠä¸ Ollamaã€llama.cpp å’Œ vLLM ç­‰æ¨ç†å¼•æ“çš„é›†æˆã€‚
Unslothå®šæœŸä¸ Hugging Faceã€Google å’Œ Meta çš„å›¢é˜Ÿåˆä½œï¼Œä»¥ä¿®å¤ LLM è®­ç»ƒå’Œæ¨¡å‹ä¸­çš„é”™è¯¯ã€‚å› æ­¤ï¼Œå½“ä½¿ç”¨ Unsloth è¿›è¡Œè®­ç»ƒæˆ–ä½¿ç”¨æ¨¡å‹æ—¶ï¼Œå¯ä»¥æœŸå¾…è·å¾—æœ€å‡†ç¡®çš„ç»“æœã€‚ Unsloth å…·æœ‰é«˜åº¦å¯å®šåˆ¶æ€§ï¼Œå…è®¸æ›´æ”¹èŠå¤©æ¨¡æ¿æˆ–æ•°æ®é›†æ ¼å¼ç­‰å†…å®¹ã€‚Unslothè¿˜ä¸ºè§†è§‰ã€æ–‡æœ¬è½¬è¯­éŸ³ (TTS)ã€BERTã€å¼ºåŒ–å­¦ä¹  (RL) ç­‰æä¾›äº†é¢„æ„å»ºçš„è„šæœ¬ï¼æ­¤å¤–ï¼ŒUnslothæ”¯æŒæ‰€æœ‰è®­ç»ƒæ–¹æ³•å’Œæ‰€æœ‰åŸºäº Transformer çš„æ¨¡å‹ã€‚

unslothä½¿Phi-4å¾®è°ƒé€Ÿåº¦æé«˜2å€ï¼ŒVRAMä½¿ç”¨å‡å°‘70%ï¼Œå¹¶ä¸”æ¯”æ‰€æœ‰ä½¿ç”¨Flash Attention 2çš„ç¯å¢ƒæ”¯æŒé•¿8å€çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚ä½¿ç”¨unslothï¼ŒPhi-4æ¨¡å‹å¯ä»¥èˆ’é€‚åœ°åœ¨ä»…24GB VRAMçš„ç¯å¢ƒä¸­è¿è¡Œã€‚
unslothä¸ºPhi-4æä¾›äº†Dynamic 2.0é‡åŒ–æ–¹æ³•ï¼Œåœ¨5-shot MMLUå’ŒKLæ•£åº¦åŸºå‡†æµ‹è¯•ä¸­æä¾›æœ€ä½³æ€§èƒ½ã€‚è¿™æ„å‘³ç€å¯ä»¥è¿è¡Œå’Œå¾®è°ƒé‡åŒ–åçš„Phi-4 LLMï¼ŒåŒæ—¶ä¿æŒæœ€å°çš„ç²¾åº¦æŸå¤±ã€‚
unslothè¿˜ä¸Šä¼ äº†æ”¯æŒåŸç”Ÿé•¿ä¸Šä¸‹æ–‡çš„Phi-4ç‰ˆæœ¬ã€‚

## æ•™ç¨‹æ¦‚è§ˆ

æœ¬æ•™ç¨‹å°†æŒ‡å¯¼æ‚¨å®Œæˆ **Phi-4ï¼ˆ14Bï¼‰ æ¨¡å‹çš„ GRPOï¼ˆGroup Relative Policy Optimizationï¼‰å¾®è°ƒ**ï¼Œè¿™æ˜¯ä¸€ç§å…ˆè¿›çš„å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼Œä¸“é—¨ç”¨äºæå‡å¤§è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

### ä»€ä¹ˆæ˜¯GRPOï¼Ÿ

GRPOï¼ˆGroup Relative Policy Optimizationï¼‰æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æŠ€æœ¯ï¼Œé€šè¿‡è®¾è®¡å¤šä¸ªå¥–åŠ±å‡½æ•°æ¥è¯„ä¼°æ¨¡å‹è¾“å‡ºçš„ä¸åŒæ–¹é¢ï¼Œä»è€ŒæŒ‡å¯¼æ¨¡å‹å­¦ä¹ æœŸæœ›çš„è¡Œä¸ºæ¨¡å¼ã€‚åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ï¼ŒGRPOå¯ä»¥å¸®åŠ©æ¨¡å‹ï¼š

- å­¦ä¼šæŒ‰ç…§ç‰¹å®šæ ¼å¼è¾“å‡ºç­”æ¡ˆ
- æé«˜æ¨ç†è¿‡ç¨‹çš„é€»è¾‘æ€§
- å¢å¼ºç­”æ¡ˆçš„å‡†ç¡®æ€§
- æ”¹å–„è¾“å‡ºçš„ç»“æ„åŒ–ç¨‹åº¦

### æœ¬æ•™ç¨‹çš„å­¦ä¹ å†…å®¹

1. **ç¯å¢ƒè®¾ç½®**: å®‰è£…Unslothå’Œç›¸å…³ä¾èµ–
2. **æ¨¡å‹åŠ è½½**: åŠ è½½Phi-4ï¼ˆ14Bï¼‰é¢„è®­ç»ƒæ¨¡å‹
3. **LoRAé…ç½®**: è®¾ç½®é«˜æ•ˆçš„å‚æ•°å¾®è°ƒ
4. **æ•°æ®å¤„ç†**: å¤„ç†GSM8Kæ•°å­¦æ¨ç†æ•°æ®é›†
5. **æ ¼å¼è®¾è®¡**: å®šä¹‰ç»“æ„åŒ–çš„è¾“å‡ºæ ¼å¼
6. **å¥–åŠ±å‡½æ•°**: è®¾è®¡å¤šç»´åº¦è¯„ä¼°ä½“ç³»
7. **GRPOè®­ç»ƒ**: æ‰§è¡Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒ
8. **æ•ˆæœéªŒè¯**: æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹
9. **æ¨¡å‹ä¿å­˜**: ä¿å­˜è®­ç»ƒç»“æœ
10. **å¯è§†åŒ–ç›‘æ§**: ä½¿ç”¨SwanLabè·Ÿè¸ªè®­ç»ƒè¿‡ç¨‹

# Phi-4 GRPO å¼ºåŒ–å­¦ä¹ è®­ç»ƒåŠ SwanLab å¯è§†åŒ–æ•™ç¨‹

## æ¦‚è¿°
æœ¬æ•™ç¨‹å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ GRPO (Generalized Reward-based Policy Optimization) ç®—æ³•å¯¹ Phi-4 æ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œå¹¶ä½¿ç”¨ SwanLab è¿›è¡Œè®­ç»ƒè¿‡ç¨‹çš„å¯è§†åŒ–ç›‘æ§ã€‚

GRPO æ˜¯ä¸€ç§åŸºäºå¥–åŠ±çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œèƒ½å¤Ÿå¸®åŠ©æ¨¡å‹å­¦ä¹ æ›´å¥½çš„æ¨ç†èƒ½åŠ›å’Œæ ¼å¼åŒ–è¾“å‡ºã€‚

## ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒå‡†å¤‡å’Œä¾èµ–å®‰è£…

é¦–å…ˆéœ€è¦å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…ï¼ŒåŒ…æ‹¬ Unsloth å’Œ vLLMã€‚


```python
# å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…
# unsloth: ç”¨äºå¿«é€Ÿæ¨¡å‹è®­ç»ƒå’Œæ¨ç†çš„ä¼˜åŒ–åº“
# vllm: é«˜æ€§èƒ½çš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†å¼•æ“
# pip install unsloth vllm
```
## ç¬¬äºŒæ­¥ï¼šæ¨¡å‹åŠ è½½å’Œ LoRA é…ç½®

åœ¨è¿™ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬å°†ï¼š
1. åŠ è½½é¢„è®­ç»ƒçš„ Phi-4 æ¨¡å‹
2. é…ç½® LoRA (Low-Rank Adaptation) å‚æ•°ç”¨äºé«˜æ•ˆå¾®è°ƒ
3. è®¾ç½®æ¨¡å‹çš„åŸºæœ¬è®­ç»ƒå‚æ•°

LoRA æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œåªè®­ç»ƒå°‘é‡å‚æ•°å°±èƒ½è·å¾—è‰¯å¥½çš„æ€§èƒ½ã€‚


```python
# å¯¼å…¥å¿…è¦çš„åº“
from unsloth import FastLanguageModel, is_bfloat16_supported
import torch

# æ¨¡å‹é…ç½®å‚æ•°
max_seq_length = 512  # æœ€å¤§åºåˆ—é•¿åº¦ï¼Œå¯ä»¥å¢åŠ ä»¥æ”¯æŒæ›´é•¿çš„æ¨ç†é“¾
lora_rank = 16  # LoRA ç§©ï¼Œæ›´å¤§çš„ç§©ä¼šè®©æ¨¡å‹æ›´èªæ˜ä½†è®­ç»ƒæ›´æ…¢

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Phi-4",  # Phi-4 æ¨¡å‹è·¯å¾„ï¼Œä½¿ç”¨é‡åŒ–æ¨¡å‹ä»¥èŠ‚çœå†…å­˜
    max_seq_length = max_seq_length,  # è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦
    load_in_4bit = True,  # å¯ç”¨ 4bit é‡åŒ–ä»¥å‡å°‘å†…å­˜ä½¿ç”¨ï¼Œè®¾ä¸º False åˆ™ä½¿ç”¨ 16bit LoRA
    fast_inference = True,  # å¯ç”¨ vLLM å¿«é€Ÿæ¨ç†å¼•æ“
    max_lora_rank = lora_rank,  # è®¾ç½®æœ€å¤§ LoRA ç§©
    gpu_memory_utilization = 0.7,  # GPU å†…å­˜ä½¿ç”¨ç‡ï¼Œå¦‚æœå†…å­˜ä¸è¶³å¯ä»¥å‡å°‘
)

# é…ç½® PEFT (Parameter-Efficient Fine-Tuning) æ¨¡å‹
model = FastLanguageModel.get_peft_model(
    model,
    r = lora_rank,  # LoRA ç§©ï¼Œå»ºè®®å€¼ï¼š8, 16, 32, 64, 128
    target_modules = ["gate_proj", "up_proj", "down_proj"],  # ç›®æ ‡æ¨¡å—ï¼Œè¿™äº›æ˜¯ MLP å±‚çš„å…³é”®ç»„ä»¶
    lora_alpha = lora_rank,  # LoRA alpha å‚æ•°ï¼Œé€šå¸¸è®¾ä¸ºä¸ç§©ç›¸åŒçš„å€¼
    use_gradient_checkpointing = "unsloth",  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥æ”¯æŒé•¿ä¸Šä¸‹æ–‡å¾®è°ƒ
    random_state = 3407,  # éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°
)
```
    ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
    ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
    INFO 07-02 17:05:40 [__init__.py:239] Automatically detected platform cuda.
    ==((====))==  Unsloth 2025.6.12: Fast Llama patching. Transformers: 4.52.4. vLLM: 0.8.2.
       \\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.151 GB. Platform: Linux.
    O^O/ \_/ \    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0
    \        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = True]
     "-____-"     Free license: http://github.com/unslothai/unsloth
    Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
    Unsloth: vLLM loading /opt/tiger/test0/Phi-4 with actual GPU utilization = 69.6%
    Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 79.15 GB.
    Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 512. Num Sequences = 320.
    Unsloth: vLLM's KV Cache can use up to 27.66 GB. Also swap space = 6 GB.
    INFO 07-02 17:07:52 [config.py:585] This model supports multiple tasks: {'classify', 'generate', 'embed', 'score', 'reward'}. Defaulting to 'generate'.
    WARNING 07-02 17:07:52 [arg_utils.py:1854] --quantization bitsandbytes is not supported by the V1 Engine. Falling back to V0. 
    Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'fp4', 'bnb_4bit_use_double_quant': False, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': [], 'llm_int8_threshold': 6.0}
    INFO 07-02 17:07:52 [llm_engine.py:241] Initializing a V0 LLM engine (v0.8.2) with config: model='/opt/tiger/test0/Phi-4', speculative_config=None, tokenizer='/opt/tiger/test0/Phi-4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=512, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/opt/tiger/test0/Phi-4, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":0,"backend":"inductor","splitting_ops":[],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"debug":false,"dce":true,"coordinate_descent_tuning":true,"trace.enabled":false,"trace.graph_diagram":false,"triton.cudagraphs":true,"compile_threads":48,"max_autotune":false,"disable_progress":false,"verbose_progress":true,"enable_auto_functionalized_v2":false},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":320}, use_cached_outputs=False, 
    INFO 07-02 17:07:53 [cuda.py:291] Using Flash Attention backend.
    INFO 07-02 17:07:53 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
    INFO 07-02 17:07:53 [model_runner.py:1110] Starting to load model /opt/tiger/test0/Phi-4...
    INFO 07-02 17:07:53 [loader.py:1155] Loading weights with BitsAndBytes quantization. May take a while ...



    Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]



    INFO 07-02 17:08:01 [punica_selector.py:18] Using PunicaWrapperGPU.
    INFO 07-02 17:08:01 [model_runner.py:1146] Model loading took 8.6253 GB and 7.670299 seconds
    INFO 07-02 17:08:07 [worker.py:267] Memory profiling takes 5.35 seconds
    INFO 07-02 17:08:07 [worker.py:267] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.70) = 55.09GiB
    INFO 07-02 17:08:07 [worker.py:267] model weights take 8.63GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.16GiB; the rest of the memory reserved for KV Cache is 45.21GiB.
    INFO 07-02 17:08:07 [executor_base.py:111] # cuda blocks: 14815, # CPU blocks: 1966
    INFO 07-02 17:08:07 [executor_base.py:116] Maximum concurrency for 512 tokens per request: 462.97x
    INFO 07-02 17:08:12 [model_runner.py:1442] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.


    Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:58<00:00,  1.37s/it]

    INFO 07-02 17:09:11 [model_runner.py:1570] Graph capturing finished in 59 secs, took 1.12 GiB
    INFO 07-02 17:09:11 [llm_engine.py:447] init engine (profile, create kv cache, warmup model) took 70.01 seconds


    


    Unsloth: Just some info: will skip parsing ['pre_feedforward_layernorm', 'k_norm', 'q_norm', 'post_feedforward_layernorm']
    Unsloth: Just some info: will skip parsing ['pre_feedforward_layernorm', 'k_norm', 'q_norm', 'post_feedforward_layernorm']


    Not an error, but Unsloth cannot patch Attention layers with our manual autograd engine since either LoRA adapters
    are not enabled or a bias term (like in Qwen) is used.
    Not an error, but Unsloth cannot patch O projection layer with our manual autograd engine since either LoRA adapters
    are not enabled or a bias term (like in Qwen) is used.
    Unsloth 2025.6.12 patched 40 layers with 0 QKV layers, 0 O layers and 40 MLP layers.



## ç¬¬ä¸‰æ­¥ï¼šæ•°æ®é›†å‡†å¤‡å’Œå¥–åŠ±å‡½æ•°å®šä¹‰

åœ¨è¿™ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬å°†ï¼š
1. åŠ è½½å’Œé¢„å¤„ç† GSM8K æ•°å­¦é—®é¢˜æ•°æ®é›†
2. å®šä¹‰ç³»ç»Ÿæç¤ºè¯å’Œè¾“å‡ºæ ¼å¼
3. åˆ›å»ºå¤šä¸ªå¥–åŠ±å‡½æ•°æ¥æŒ‡å¯¼æ¨¡å‹å­¦ä¹ 

å¥–åŠ±å‡½æ•°æ˜¯ GRPO è®­ç»ƒçš„æ ¸å¿ƒï¼Œå®ƒä»¬ä¼šè¯„ä¼°æ¨¡å‹è¾“å‡ºçš„è´¨é‡å¹¶ç»™å‡ºåé¦ˆã€‚


```python
import re
from datasets import load_dataset, Dataset

# Load and prep dataset
SYSTEM_PROMPT = """
Respond in the following format:
<reasoning>
...
</reasoning>
<answer>
...
</answer>
"""

XML_COT_FORMAT = """\
<reasoning>
{reasoning}
</reasoning>
<answer>
{answer}
</answer>
"""

def extract_xml_answer(text: str) -> str:
    answer = text.split("<answer>")[-1]
    answer = answer.split("</answer>")[0]
    return answer.strip()

def extract_hash_answer(text: str) -> str | None:
    if "####" not in text:
        return None
    return text.split("####")[1].strip()

# uncomment middle messages for 1-shot prompting
def get_gsm8k_questions(split = "train") -> Dataset:
    data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore
    data = data.map(lambda x: { # type: ignore
        'prompt': [
            {'role': 'system', 'content': SYSTEM_PROMPT},
            {'role': 'user', 'content': x['question']}
        ],
        'answer': extract_hash_answer(x['answer'])
    }) # type: ignore
    return data # type: ignore

dataset = get_gsm8k_questions()

# Reward functions
def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:
    responses = [completion[0]['content'] for completion in completions]
    q = prompts[0][-1]['content']
    extracted_responses = [extract_xml_answer(r) for r in responses]
    print('-'*20, f"Question:\n{q}", f"\nAnswer:\n{answer[0]}", f"\nResponse:\n{responses[0]}", f"\nExtracted:\n{extracted_responses[0]}")
    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]

def int_reward_func(completions, **kwargs) -> list[float]:
    responses = [completion[0]['content'] for completion in completions]
    extracted_responses = [extract_xml_answer(r) for r in responses]
    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]

def strict_format_reward_func(completions, **kwargs) -> list[float]:
    """Reward function that checks if the completion has a specific format."""
    pattern = r"^<reasoning>\n.*?\n</reasoning>\n<answer>\n.*?\n</answer>\n$"
    responses = [completion[0]["content"] for completion in completions]
    matches = [re.match(pattern, r) for r in responses]
    return [0.5 if match else 0.0 for match in matches]

def soft_format_reward_func(completions, **kwargs) -> list[float]:
    """Reward function that checks if the completion has a specific format."""
    pattern = r"<reasoning>.*?</reasoning>\s*<answer>.*?</answer>"
    responses = [completion[0]["content"] for completion in completions]
    matches = [re.match(pattern, r) for r in responses]
    return [0.5 if match else 0.0 for match in matches]

def count_xml(text) -> float:
    count = 0.0
    if text.count("<reasoning>\n") == 1:
        count += 0.125
    if text.count("\n</reasoning>\n") == 1:
        count += 0.125
    if text.count("\n<answer>\n") == 1:
        count += 0.125
        count -= len(text.split("\n</answer>\n")[-1])*0.001
    if text.count("\n</answer>") == 1:
        count += 0.125
        count -= (len(text.split("\n</answer>")[-1]) - 1)*0.001
    return count

def xmlcount_reward_func(completions, **kwargs) -> list[float]:
    contents = [completion[0]["content"] for completion in completions]
    return [count_xml(c) for c in contents]
```


```python
from trl import GRPOConfig, GRPOTrainer
training_args = GRPOConfig(
    use_vllm = True, # use vLLM for fast inference!
    learning_rate = 5e-6,
    adam_beta1 = 0.9,
    adam_beta2 = 0.99,
    weight_decay = 0.1,
    warmup_ratio = 0.1,
    lr_scheduler_type = "cosine",
    optim = "paged_adamw_8bit",
    logging_steps = 1,
    per_device_train_batch_size = 1,
    gradient_accumulation_steps = 1, # Increase to 4 for smoother training
    num_generations = 6, # Decrease if out of memory
    max_prompt_length = 256,
    max_completion_length = 200,
    # num_train_epochs = 1, # Set to 1 for a full training run
    max_steps = 100,
    save_steps = 250,
    max_grad_norm = 0.1,
    report_to = "swanlab", # Can use Weights & Biases
    output_dir = "outputs",
)
```
    Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.
    We will change the batch size of 1 to the `num_generations` of 6


And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!

You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!

| Step | Training Loss | reward    | reward_std | completion_length | kl       |
|------|---------------|-----------|------------|-------------------|----------|
| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |
| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |
| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |

```python
trainer = GRPOTrainer(
    model = model,
    processing_class = tokenizer,
    reward_funcs = [
        xmlcount_reward_func,
        soft_format_reward_func,
        strict_format_reward_func,
        int_reward_func,
        correctness_reward_func,
    ],
    args = training_args,
    train_dataset = dataset,
)
trainer.train()
```

    Detected kernel version 5.4.143, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
    ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
       \\   /|    Num examples = 7,473 | Num Epochs = 1 | Total steps = 100
    O^O/ \_/ \    Batch size per device = 6 | Gradient accumulation steps = 1
    \        /    Data Parallel GPUs = 1 | Total batch size (6 x 1 x 1) = 6
     "-____-"     Trainable parameters = 44,236,800 of 7,888,000,000 (0.56% trained)


    [1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.6.4                                   
    [1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/opt/tiger/test0/swanlog/run-20250702_170943-0e8cd89d[0m[0m
    [1m[34mswanlab[0m[0m: ğŸ‘‹ Hi [1m[39mtwosugar[0m[0m, welcome to swanlab!
    [1m[34mswanlab[0m[0m: Syncing run [33moutputs[0m to the cloud
    [1m[34mswanlab[0m[0m: ğŸ  View project at [34m[4mhttps://swanlab.cn/@twosugar/test0[0m[0m
    [1m[34mswanlab[0m[0m: ğŸš€ View run at [34m[4mhttps://swanlab.cn/@twosugar/test0/runs/rnvceekqy652of27de2o9[0m[0m
```python
text = tokenizer.apply_chat_template([
    {"role" : "user", "content" : "Which is bigger? 9.11 or 9.9?"},
], tokenize = False, add_generation_prompt = True)

from vllm import SamplingParams
sampling_params = SamplingParams(
    temperature = 0.8,
    top_p = 0.95,
    max_tokens = 1024,
)
output = model.fast_generate(
    [text],
    sampling_params = sampling_params,
    lora_request = None,
)[0].outputs[0].text

output
```

    Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:16<00:00, 16.59s/it, est. speed input: 1.27 toks/s, output: 9.89 toks/s]





    '9.11 is bigger than 9.9. When comparing decimal numbers, you look at the digits from left to right. Both numbers have the same whole number part (9), so you compare the digits in the tenths place next. In 9.11, the tenths place is 1, and in 9.9, the tenths place is 9. Since 1 is less than 9, you might initially think 9.9 is larger, but you also need to consider the hundredths place in 9.11, which is 1. When you express 9.9 as 9.90 for comparison, you see that 9.11 is greater than 9.90. Therefore, 9.11 is bigger than 9.9.'




```python
model.save_lora("grpo_saved_lora")
```


```python
text = tokenizer.apply_chat_template([
    {"role" : "system", "content" : SYSTEM_PROMPT},
    {"role" : "user", "content" : "Which is bigger? 9.11 or 9.9?"},
], tokenize = False, add_generation_prompt = True)

from vllm import SamplingParams
sampling_params = SamplingParams(
    temperature = 0.8,
    top_p = 0.95,
    max_tokens = 1024,
)
output = model.fast_generate(
    text,
    sampling_params = sampling_params,
    lora_request = model.load_lora("grpo_saved_lora"),
)[0].outputs[0].text

output
```

    Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:27<00:00, 27.72s/it, est. speed input: 1.70 toks/s, output: 10.03 toks/s]





    '<reasoning>\nTo determine which number is bigger between 9.11 and 9.9, we should compare the two numbers digit by digit from left to right. \n\n1. First, compare the digits in the units place:\n   - Both numbers have a 9 in the units place.\n\n2. Next, compare the digits in the tenths place:\n   - The number 9.11 has a 1 in the tenths place.\n   - The number 9.9 has a 9 in the tenths place.\n\nSince 1 is less than 9, the number 9.11 is less than 9.9 based on the tenths place comparison.\n\n3. For thoroughness, consider the hundredths place:\n   - The number 9.11 has a 1 in the hundredths place.\n   - The number 9.9 can be written as 9.90, which has a 0 in the hundredths place.\n\nEven if we compare the hundredths place, 1 is greater than 0, but this is irrelevant since the comparison in the tenths place already determines that 9.11 is smaller than 9.9.\n\nTherefore, 9.9 is greater than 9.11.\n</reasoning>\n\n<answer>\n9.9 is bigger than 9.11.\n</answer>'




```python
print(output)
```

    <reasoning>
    To determine which number is bigger between 9.11 and 9.9, we should compare the two numbers digit by digit from left to right. 
    
    1. First, compare the digits in the units place:
       - Both numbers have a 9 in the units place.
    
    2. Next, compare the digits in the tenths place:
       - The number 9.11 has a 1 in the tenths place.
       - The number 9.9 has a 9 in the tenths place.
    
    Since 1 is less than 9, the number 9.11 is less than 9.9 based on the tenths place comparison.
    
    3. For thoroughness, consider the hundredths place:
       - The number 9.11 has a 1 in the hundredths place.
       - The number 9.9 can be written as 9.90, which has a 0 in the hundredths place.
    
    Even if we compare the hundredths place, 1 is greater than 0, but this is irrelevant since the comparison in the tenths place already determines that 9.11 is smaller than 9.9.
    
    Therefore, 9.9 is greater than 9.11.
    </reasoning>
    
    <answer>
    9.9 is bigger than 9.11.
    </answer>


Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!


```python
# Merge to 16bit
if False: model.save_pretrained_merged("model", tokenizer, save_method = "merged_16bit",)
if False: model.push_to_hub_merged("hf/model", tokenizer, save_method = "merged_16bit", token = "")

# Merge to 4bit
if False: model.save_pretrained_merged("model", tokenizer, save_method = "merged_4bit",)
if False: model.push_to_hub_merged("hf/model", tokenizer, save_method = "merged_4bit", token = "")

# Just LoRA adapters
if False:
    model.save_pretrained("model")
    tokenizer.save_pretrained("model")
if False:
    model.push_to_hub("hf/model", token = "")
    tokenizer.push_to_hub("hf/model", token = "")

```


```python
# Save to 8bit Q8_0
if False: model.save_pretrained_gguf("model", tokenizer,)
# Remember to go to https://huggingface.co/settings/tokens for a token!
# And change hf to your username!
if False: model.push_to_hub_gguf("hf/model", tokenizer, token = "")

# Save to 16bit GGUF
if False: model.save_pretrained_gguf("model", tokenizer, quantization_method = "f16")
if False: model.push_to_hub_gguf("hf/model", tokenizer, quantization_method = "f16", token = "")

# Save to q4_k_m GGUF
if False: model.save_pretrained_gguf("model", tokenizer, quantization_method = "q4_k_m")
if False: model.push_to_hub_gguf("hf/model", tokenizer, quantization_method = "q4_k_m", token = "")

# Save to multiple GGUF options - much faster if you want multiple!
if False:
    model.push_to_hub_gguf(
        "hf/model", # Change hf to your username!
        tokenizer,
        quantization_method = ["q4_k_m", "q8_0", "q5_k_m",],
        token = "",
    )
```



## Swanlab

![image06-2](./images/image06-2.png)

> ++[SwanLab](https://github.com/swanhubx/swanlab)++ æ˜¯ä¸€ä¸ªå¼€æºçš„æ¨¡å‹è®­ç»ƒè®°å½•å·¥å…·ï¼Œé¢å‘ AI ç ”ç©¶è€…ï¼Œæä¾›äº†è®­ç»ƒå¯è§†åŒ–ã€è‡ªåŠ¨æ—¥å¿—è®°å½•ã€è¶…å‚æ•°è®°å½•ã€å®éªŒå¯¹æ¯”ã€å¤šäººååŒç­‰åŠŸèƒ½ã€‚åœ¨ `SwanLab` ä¸Šï¼Œç ”ç©¶è€…èƒ½åŸºäºç›´è§‚çš„å¯è§†åŒ–å›¾è¡¨å‘ç°è®­ç»ƒé—®é¢˜ï¼Œå¯¹æ¯”å¤šä¸ªå®éªŒæ‰¾åˆ°ç ”ç©¶çµæ„Ÿï¼Œå¹¶é€šè¿‡åœ¨çº¿é“¾æ¥çš„åˆ†äº«ä¸åŸºäºç»„ç»‡çš„å¤šäººååŒè®­ç»ƒï¼Œæ‰“ç ´å›¢é˜Ÿæ²Ÿé€šçš„å£å’ã€‚

### ä¸ºä»€ä¹ˆè¦è®°å½•è®­ç»ƒï¼Ÿ

ç›¸è¾ƒäºè½¯ä»¶å¼€å‘ï¼Œæ¨¡å‹è®­ç»ƒæ›´åƒä¸€ä¸ªå®éªŒç§‘å­¦ã€‚ä¸€ä¸ªå“è´¨ä¼˜ç§€çš„æ¨¡å‹èƒŒåï¼Œå¾€å¾€æ˜¯æˆåƒä¸Šä¸‡æ¬¡å®éªŒã€‚ç ”ç©¶è€…éœ€è¦ä¸æ–­å°è¯•ã€è®°å½•ã€å¯¹æ¯”ï¼Œç§¯ç´¯ç»éªŒï¼Œæ‰èƒ½æ‰¾åˆ°æœ€ä½³çš„æ¨¡å‹ç»“æ„ã€è¶…å‚æ•°ä¸æ•°æ®é…æ¯”ã€‚åœ¨è¿™ä¹‹ä¸­ï¼Œå¦‚ä½•é«˜æ•ˆè¿›è¡Œè®°å½•ä¸å¯¹æ¯”ï¼Œå¯¹äºç ”ç©¶æ•ˆç‡çš„æå‡è‡³å…³é‡è¦ã€‚

### åœ¨å“ªé‡Œç”¨ï¼Ÿ

å»ºè®®å…ˆåœ¨ ++[SwanLab å®˜ç½‘](https://swanlab.cn/)++ æ³¨å†Œè´¦å·ï¼Œç„¶ååœ¨GRPOè®­ç»ƒåˆå§‹åŒ–é˜¶æ®µé€‰æ‹©

```Python
from trl import GRPOConfig, GRPOTrainer
training_args = GRPOConfig(
    # ä¼˜åŒ–å™¨å‚æ•°
    learning_rate = 5e-6,           # å­¦ä¹ ç‡ï¼šGRPOé€šå¸¸ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡
    adam_beta1 = 0.9,               # Adamä¼˜åŒ–å™¨çš„beta1å‚æ•°
    adam_beta2 = 0.99,              # Adamä¼˜åŒ–å™¨çš„beta2å‚æ•°
    weight_decay = 0.1,             # æƒé‡è¡°å‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    optim = "adamw_torch_fused",    # ä½¿ç”¨èåˆçš„AdamWä¼˜åŒ–å™¨ï¼Œæ›´é«˜æ•ˆ
    
    # å­¦ä¹ ç‡è°ƒåº¦
    warmup_ratio = 0.1,             # å­¦ä¹ ç‡é¢„çƒ­æ¯”ä¾‹
    lr_scheduler_type = "cosine",   # ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦
    
    # è®­ç»ƒæ‰¹æ¬¡è®¾ç½®
    per_device_train_batch_size = 1,        # æ¯ä¸ªè®¾å¤‡çš„æ‰¹æ¬¡å¤§å°
    gradient_accumulation_steps = 1,        # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆå¯ä»¥å¢åŠ åˆ°4è·å¾—æ›´å¹³æ»‘çš„è®­ç»ƒï¼‰
    num_generations = 4,                    # æ¯ä¸ªæç¤ºç”Ÿæˆçš„å€™é€‰æ•°é‡ï¼ˆæ˜¾å­˜ä¸è¶³æ—¶å¯å‡å°‘ï¼‰
    
    # åºåˆ—é•¿åº¦æ§åˆ¶
    max_prompt_length = max_prompt_length,                      # æç¤ºçš„æœ€å¤§é•¿åº¦
    max_completion_length = max_seq_length - max_prompt_length, # å®Œæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦
    
    # è®­ç»ƒæ§åˆ¶
    max_steps = 50,                 # æœ€å¤§è®­ç»ƒæ­¥æ•°ï¼ˆæ¼”ç¤ºç”¨ï¼Œå®é™…è®­ç»ƒå»ºè®®æ›´å¤šï¼‰
    save_steps = 50,                # ä¿å­˜æ¨¡å‹çš„æ­¥æ•°é—´éš”
    max_grad_norm = 0.1,            # æ¢¯åº¦è£å‰ªé˜ˆå€¼
    
    # æ—¥å¿—å’Œç›‘æ§
    logging_steps = 1,              # æ—¥å¿—è®°å½•é—´éš”
    report_to = "swanlab",          # è¿™é‡Œæ”¹æˆswanlab
    output_dir = "outputs",         # è¾“å‡ºç›®å½•
)
```

### æœ¬è¯•éªŒçš„è¯•éªŒè®°å½•

#### GRPOé˜¶æ®µ

![image06-3](./images/image06-3.png)
400ä¸ªstepä¹‹ålossä¼šæœ‰æ˜æ˜¾å˜åŒ–

## æ•™ç¨‹æ€»ç»“

ğŸ‰ æ­å–œï¼ä½ å·²ç»æˆåŠŸå®Œæˆäº†Phi-4ï¼ˆ14Bï¼‰çš„GRPOå¾®è°ƒæ•™ç¨‹ã€‚

### æœ¬æ•™ç¨‹æ¶µç›–çš„æ ¸å¿ƒæ¦‚å¿µï¼š

1. **GRPOå¾®è°ƒ**: ä½¿ç”¨å¥–åŠ±å‡½æ•°æŒ‡å¯¼æ¨¡å‹å­¦ä¹ ç‰¹å®šè¾“å‡ºæ ¼å¼
2. **LoRAæŠ€æœ¯**: é«˜æ•ˆçš„å‚æ•°å¾®è°ƒæ–¹æ³•ï¼ŒèŠ‚çœæ˜¾å­˜å’Œæ—¶é—´
3. **å¥–åŠ±å‡½æ•°è®¾è®¡**: å¤šå±‚æ¬¡è¯„ä¼°ä½“ç³»ï¼Œä»æ ¼å¼åˆ°å†…å®¹çš„å…¨é¢è¯„ä»·
4. **ç»“æ„åŒ–è¾“å‡º**: è®­ç»ƒæ¨¡å‹æŒ‰ç…§ç‰¹å®šæ ¼å¼è¾“å‡ºæ¨ç†è¿‡ç¨‹å’Œç­”æ¡ˆ
5. **SwanLabç›‘æ§**: å®æ—¶è·Ÿè¸ªè®­ç»ƒè¿›åº¦å’ŒæŒ‡æ ‡å˜åŒ–

### å­¦åˆ°çš„æŠ€èƒ½ï¼š

- âœ… è®¾ç½®GRPOè®­ç»ƒç¯å¢ƒ
- âœ… è®¾è®¡å¤šç»´åº¦å¥–åŠ±å‡½æ•°
- âœ… é…ç½®LoRAå‚æ•°è¿›è¡Œé«˜æ•ˆå¾®è°ƒ
- âœ… å¤„ç†æ•°å­¦æ¨ç†æ•°æ®é›†
- âœ… ç›‘æ§å’Œåˆ†æè®­ç»ƒè¿‡ç¨‹
- âœ… ä¿å­˜å’Œéƒ¨ç½²å¾®è°ƒæ¨¡å‹

### è¿›ä¸€æ­¥æ¢ç´¢ï¼š

1. **è°ƒæ•´å¥–åŠ±å‡½æ•°**: è®¾è®¡æ›´å¤æ‚çš„è¯„ä¼°æœºåˆ¶
2. **æ‰©å±•æ•°æ®é›†**: ä½¿ç”¨æ›´å¤§æˆ–ä¸åŒç±»å‹çš„æ•°æ®é›†
3. **ä¼˜åŒ–å‚æ•°**: å°è¯•ä¸åŒçš„LoRAé…ç½®å’Œè®­ç»ƒå‚æ•°
4. **æ¨¡å‹è¯„ä¼°**: åœ¨æµ‹è¯•é›†ä¸Šç³»ç»Ÿè¯„ä¼°æ¨¡å‹æ€§èƒ½
5. **åº”ç”¨éƒ¨ç½²**: å°†æ¨¡å‹é›†æˆåˆ°å®é™…åº”ç”¨ä¸­

### æ³¨æ„äº‹é¡¹ï¼š

- æœ¬æ•™ç¨‹ä½¿ç”¨äº†è¾ƒå°‘çš„è®­ç»ƒæ­¥æ•°ä½œä¸ºæ¼”ç¤ºï¼Œå®é™…åº”ç”¨ä¸­å»ºè®®ä½¿ç”¨æ›´å¤šæ­¥æ•°
- å¯ä»¥æ ¹æ®æ˜¾å­˜æƒ…å†µè°ƒæ•´æ‰¹æ¬¡å¤§å°å’Œç”Ÿæˆæ•°é‡
- SwanLabæä¾›äº†ä¸°å¯Œçš„å¯è§†åŒ–åŠŸèƒ½ï¼Œå»ºè®®æ·±å…¥æ¢ç´¢

æ„Ÿè°¢ä½ çš„å­¦ä¹ ï¼å¦‚æœæœ‰ä»»ä½•é—®é¢˜ï¼Œæ¬¢è¿æŸ¥çœ‹SwanLabçš„å®éªŒè®°å½•æˆ–é‡æ–°è¿è¡Œä»£ç ã€‚

# æ€»ç»“

Congratulationsï¼çœ‹åˆ°äº†è¿™ï¼Œä½ å·²ç»åˆæ­¥å®ç°äº†ä¸€ä¸ªç®€å•çš„RLå®æˆ˜ï¼ŒæŒæ¡äº†ä½¿ç”¨ Unsloth å¯¹ Phi-4ï¼ˆ14Bï¼‰ è¿™ç±»å¤§æ¨¡å‹è¿›è¡Œ GRPO å¾®è°ƒçš„å…·ä½“æ“ä½œæ­¥éª¤ï¼Œæ›´èƒ½ä½“ä¼šåˆ° Unsloth åœ¨å¤§å¹…æå‡è®­ç»ƒé€Ÿåº¦ã€æ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨æ–¹é¢çš„å¼ºå¤§ä¼˜åŠ¿ï¼Œä»è€Œä½¿åœ¨æœ‰é™èµ„æºä¸‹è¿›è¡Œå¤æ‚å¼ºåŒ–å­¦ä¹ å®éªŒæˆä¸ºå¯èƒ½ï¼å¦‚æœæ”¯æŒæˆ‘ä»¬çš„å·¥ä½œå¸Œæœ›å¾—åˆ°ä½ çš„starï¼ï¼è¿™æ˜¯æˆ‘ä»¬æŒç»­æ›´æ–°çš„æœ€å¤§åŠ¨åŠ›ï¼ï¼ï¼

# ç›¸å…³é“¾æ¥

- å®Œæ•´å¯è¿è¡Œçš„ä»£ç ï¼š[Github](https://github.com/datawhalechina/self-llm/blob/master/models/phi4/06-Phi-4-GRPOåŠswanlabå¯è§†åŒ–.ipynb)
- ç»¼è¿°ï¼šhttps://arxiv.org/abs/2001.06921
- deepseek-r1ï¼šhttps://arxiv.org/abs/2501.12948
- æ•°å­¦åŸç†ï¼šhttps://blog.csdn.net/weixin\_38991876/article/details/146474767
- Unslothï¼šhttps://docs.unsloth.ai/
