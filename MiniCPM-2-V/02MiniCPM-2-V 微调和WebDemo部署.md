# MiniCPM-2-Vå¾®è°ƒå’ŒWebDemoéƒ¨ç½²

## ç¯å¢ƒå‡†å¤‡

åœ¨ Autodl å¹³å°ä¸­ç§Ÿèµä¸€ä¸ª3å¡ RTX 3090/24G æ˜¾å­˜çš„æ˜¾å¡æœºå™¨ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œé•œåƒå…ˆé€‰pytorch2.3.0->python3.12(ubuntu22.04)->cuda12.1ï¼Œæˆ‘ä»¬åæœŸç›´æ¥é€šè¿‡mambaåŒ…ç®¡ç†å·¥å…·åˆ›å»ºå³å¯ã€‚

![](assets/2024-06-12-10-09-28-image.png)

![](assets/2024-06-12-10-12-18-image.png)æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ‰“å¼€åˆšåˆšç§Ÿç”¨æœåŠ¡å™¨çš„ JupyterLabï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œç„¶åæ‰“å¼€å…¶ä¸­çš„ç»ˆç«¯ï¼Œå¼€å§‹ç¯å¢ƒé…ç½®ã€æ¨¡å‹ä¸‹è½½å’Œè¿è¡Œæ¼”ç¤ºã€‚æˆ‘ä»¬ç›´æ¥ä½¿ç”¨ä»£ç ä»“åº“ä¸­çš„minicpm-2-v.yaml

```python
cd /
curl -Ls https://micro.mamba.pm/api/micromamba/linux-64/latest | tar -xvj bin/micromamba
./bin/micromamba shell init -s bash -p /micromamba

vi ~/.bashrc
åŠ ä¸Š
alias mamba=micromamba

mamba env create -f minicpm-2-v.yaml -y
mamba activate minicpm-2-v
pip install ipykernel
python -m ipykernel install --name=minicpm-2-v --display-name minicpm-2-v # ä¸æ·»åŠ --userå±æ€§ï¼Œå¯ä¾›æ‰€æœ‰ç”¨æˆ·ä½¿ç”¨
```

![](assets/2024-06-12-10-31-00-image.png)

åˆ·æ–°ä¸€ä¸‹ï¼Œå°±èƒ½çœ‹åˆ°æˆ‘ä»¬çš„ç¯å¢ƒäº†

![](assets/2024-06-12-10-39-54-image.png)

## æ¨¡å‹ä¸‹è½½

ä½¿ç”¨ modelscope ä¸­çš„snapshot_downloadå‡½æ•°ä¸‹è½½æ¨¡å‹ï¼Œç¬¬ä¸€ä¸ªå‚æ•°ä¸ºæ¨¡å‹åç§°ï¼Œå‚æ•°cache_dirä¸ºæ¨¡å‹çš„ä¸‹è½½è·¯å¾„ã€‚

åœ¨ /root/autodl-tmp è·¯å¾„ä¸‹æ–°å»º download.py æ–‡ä»¶å¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼Œç²˜è´´ä»£ç åè®°å¾—ä¿å­˜æ–‡ä»¶ã€‚

download.pyä»£ç å¦‚ä¸‹

```python
import torch
from modelscope import snapshot_download, AutoModel, AutoTokenizer
from modelscope import GenerationConfig
model_dir = snapshot_download('openbmb/MiniCPM-V-2', cache_dir='/root/autodl-tmp', revision='master')
```

ä¿å­˜å¥½ååœ¨ç»ˆç«¯è¿è¡Œ python /root/autodl-tmp/download.py æ‰§è¡Œä¸‹è½½ï¼Œä¸‹è½½æ¨¡å‹éœ€è¦ä¸€äº›æ—¶é—´ã€‚

```bash
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
pip install  modelscope transformers
python /root/autodl-tmp/download.py 
```

## å¾®è°ƒç¯å¢ƒå‡†å¤‡-ç›¸å…³åŒ…ç¼–è¯‘

```bash
sudo apt-get update
sudo apt-get install cmake build-essential pkg-config libgoogle-perftools-dev
mamba activate minicpm-2-v
git clone https://github.com/google/sentencepiece.git 
cd sentencepiece
mkdir build
cd build
cmake ..
make -j $(nproc)
sudo make install
sudo ldconfig -v
pip install SentencePiece
pip install tensorboardX
```

![](assets/2024-06-12-11-37-43-image.png)

ç„¶åå°±æ¥åˆ°äº†è¾ƒä¸ºéº»çƒ¦çš„deepspeedæºç ç¼–è¯‘äº†

å‚è€ƒï¼š

[deepspeedä½¿ç”¨zero3 + offloadæŠ¥é”™:AttributeError: â€˜DeepSpeedCPUAdamâ€˜ object has no attribute â€˜ds_opt_adam](https://blog.csdn.net/qq_44193969/article/details/137051032)

[Installation Details - DeepSpeed](https://www.deepspeed.ai/tutorials/advanced-install/#pre-install-deepspeed-ops)

```bash
pip uninstall deepspeed
# DS_BUILD_CPU_ADAM=1 pip install deepspeed è¿™ä¸ªæ–¹æ³•å¯èƒ½æœ‰å…¼å®¹æ€§é—®é¢˜ï¼Œæ¨èæºç ç¼–è¯‘

git clone https://github.com/microsoft/DeepSpeed
cd DeepSpeed/

DS_BUILD_CPU_ADAM=1 python setup.py build_ext -j8 bdist_wheel
pip install dist/deepspeed-0.14.3+b6e24adb-cp312-cp312-linux_x86_64.whl

pip install timm
```

## å¼€å¯å¾®è°ƒ

```bash
git clone https://github.com/Ethan-Chen-plus/self-llm.git
cd self-llm/MiniCPM-2-V/
git clone https://github.com/Ethan-Chen-plus/llava-en-zh-2k-mini.git
mv llava-en-zh-2k-mini data
cp -r ./data/img ./img
```

ç„¶ååœ¨finetune_loraé‡Œé¢è®¾ç½®

```text
MODEL="/root/autodl-tmp/openbmb/MiniCPM-V-2" # or openbmb/MiniCPM-V-2
DATA="./data/sample_50_train.json" # json file
EVAL_DATA="./data/sample_10_test.json" # json file
LLM_TYPE="minicpm" # if use openbmb/MiniCPM-V-2, please set LLM_TYPE=minicpm
```

```bash
sh finetune_lora.sh
```

![](assets/2024-06-12-15-04-16-image.png)

## ![](assets/2024-06-12-15-06-17-image.png)

![](assets/2024-06-12-16-00-34-image.png)

## éƒ¨ç½²ä»£ç å‡†å¤‡

åœ¨`/root/autodl-tmp`è·¯å¾„ä¸‹æ–°å»º `chatBot.py` æ–‡ä»¶å¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼Œç²˜è´´ä»£ç åè®°å¾—ä¿å­˜æ–‡ä»¶ã€‚ä¸‹é¢çš„ä»£ç æœ‰å¾ˆè¯¦ç»†çš„æ³¨é‡Šï¼Œå¤§å®¶å¦‚æœ‰ä¸ç†è§£çš„åœ°æ–¹ï¼Œæ¬¢è¿æå‡ºissueã€‚

chatBot.pyä»£ç å¦‚ä¸‹

```python
import streamlit as st  # å¯¼å…¥ Streamlit åº“ï¼Œç”¨äºæ„å»ºç½‘é¡µåº”ç”¨
from PIL import Image  # å¯¼å…¥ PIL åº“ä¸­çš„ Image æ¨¡å—ï¼Œç”¨äºå¤„ç†å›¾åƒ
import torch  # å¯¼å…¥ PyTorch åº“ï¼Œç”¨äºæ·±åº¦å­¦ä¹ æ¨¡å‹
from transformers import AutoModel, AutoTokenizer  # å¯¼å…¥ transformers åº“ä¸­çš„ AutoModel å’Œ AutoTokenizerï¼Œç”¨äºåŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Union, Literal, Tuple
from peft import LoraConfig, get_peft_model, TaskType

# æ¨¡å‹è·¯å¾„
model_path = "/root/autodl-tmp/openbmb/MiniCPM-V-2"
path_to_adapter = "/root/self-llm/MiniCPM-2-V/output/output_minicpmv2_lora"
# ç”¨æˆ·å’ŒåŠ©æ‰‹çš„åç§°
U_NAME = "User"
A_NAME = "Assistant"

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="ğŸ’¬MiniCPM-V-2 Streamlit",  # é¡µé¢æ ‡é¢˜
    page_icon=":robot:",  # é¡µé¢å›¾æ ‡
    layout="wide"  # é¡µé¢å¸ƒå±€ä¸ºå®½å±
)

@dataclass
class LoraArguments:
    lora_r: int = 64
    lora_alpha: int = 64
    lora_dropout: float = 0.05
    lora_target_modules: str = r"llm\..*layers\.\d+\.self_attn\.(q_proj|k_proj|v_proj)"
    lora_weight_path: str = ""
    lora_bias: str = "none"
    q_lora: bool = False
    lora_modules_to_save: str = ""
    lora_layer_replication: Optional[List[Tuple[int, int]]] = None
    lora_layers_to_transform: Optional[List[int]] = None
    lora_layers_pattern: Optional[str] = None

from peft import LoraConfig, get_peft_model, TaskType

def load_lora_config(model):
    config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=64,
        lora_alpha=64,
        lora_dropout=0.05,
        target_modules=r"llm\..*layers\.\d+\.self_attn\.(q_proj|k_proj|v_proj)"
    )
    return get_peft_model(model, config)

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨çš„å‡½æ•°ï¼Œå¹¶ç¼“å­˜ç»“æœä»¥æé«˜æ€§èƒ½
@st.cache_resource
def load_model_and_tokenizer():
    print(f"load_model_and_tokenizer from {model_path}")
    # ä»é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œå¹¶å°†æ¨¡å‹åŠ è½½åˆ° CUDA è®¾å¤‡ä¸Šï¼ˆå¦‚æœå¯ç”¨ï¼‰
    model = AutoModel.from_pretrained(model_path, trust_remote_code=True).to(device="cuda")
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    model = load_lora_config(model)
    vpm_resampler_embedtokens_weight = torch.load(f"{path_to_adapter}/vpm_resampler_embedtokens.pt")
    msg = model.load_state_dict(vpm_resampler_embedtokens_weight, strict=False)
    return model, tokenizer

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'model' not in st.session_state:
    st.session_state.model, st.session_state.tokenizer = load_model_and_tokenizer()
    st.session_state.model.eval()  # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    print("model and tokenizer had loaded completed!")

# åˆå§‹åŒ–èŠå¤©è®°å½•çš„ä¼šè¯çŠ¶æ€
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []

# ä¾§è¾¹æ è®¾ç½®
sidebar_name = st.sidebar.title("ğŸ’¬MiniCPM-V-2 Streamlit-powered by self-llm")
# åœ¨ä¾§è¾¹æ åˆ›å»ºæ»‘å—ï¼Œç”¨äºè®¾ç½®ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦ã€é‡å¤æƒ©ç½šã€top_pã€top_k å’Œæ¸©åº¦
max_length = st.sidebar.slider("max_length", 0, 4096, 2048, step=2)
repetition_penalty = st.sidebar.slider("repetition_penalty", 0.0, 2.0, 1.05, step=0.01)
top_p = st.sidebar.slider("top_p", 0.0, 1.0, 0.8, step=0.01)
top_k = st.sidebar.slider("top_k", 0, 100, 100, step=1)
temperature = st.sidebar.slider("temperature", 0.0, 1.0, 0.7, step=0.01)
# åœ¨ä¾§è¾¹æ ä¸­åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªé“¾æ¥
with st.sidebar:
    st.markdown("## MiniCPM LLM")
    "[å¼€æºå¤§æ¨¡å‹é£Ÿç”¨æŒ‡å— self-llm](https://github.com/datawhalechina/self-llm.git)"
    "[å¼€æºå¤§æ¨¡å‹æ¶æ„æ•™ç¨‹ llms-from-scratch](https://github.com/datawhalechina/llms-from-scratch-cn.git)"

# æ¸…é™¤èŠå¤©è®°å½•çš„æŒ‰é’®
buttonClean = st.sidebar.button("Clear chat history", key="clean")
if buttonClean:
    st.session_state.chat_history = []  # æ¸…ç©ºèŠå¤©è®°å½•
    st.session_state.response = ""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()  # æ¸…ç©º CUDA ç¼“å­˜
    st.rerun()  # é‡æ–°è¿è¡Œé¡µé¢

# æ˜¾ç¤ºèŠå¤©è®°å½•
for i, message in enumerate(st.session_state.chat_history):
    if message["role"] == "user":
        # å¦‚æœæ¶ˆæ¯æ˜¯ç”¨æˆ·çš„ï¼Œæ˜¾ç¤ºç”¨æˆ·çš„æ¶ˆæ¯
        with st.chat_message(name="user", avatar="user"):
            if message["image"] is not None:
                st.image(message["image"], caption='User uploaded image', width=448, use_column_width=False)
                continue
            elif message["content"] is not None:
                st.markdown(message["content"])
    else:
        # å¦‚æœæ¶ˆæ¯æ˜¯åŠ©æ‰‹çš„ï¼Œæ˜¾ç¤ºåŠ©æ‰‹çš„æ¶ˆæ¯
        with st.chat_message(name="model", avatar="assistant"):
            st.markdown(message["content"])

# é€‰æ‹©æ¨¡å¼
selected_mode = st.sidebar.selectbox("Select mode", ["Text", "Image"])
if selected_mode == "Image":
    # å›¾ç‰‡æ¨¡å¼
    uploaded_image = st.sidebar.file_uploader("Upload image", key=1, type=["jpg", "jpeg", "png"], accept_multiple_files=False)
    if uploaded_image is not None:
        st.image(uploaded_image, caption='User uploaded image', width=468, use_column_width=False)
        # å°†ä¸Šä¼ çš„å›¾ç‰‡æ·»åŠ åˆ°èŠå¤©è®°å½•ä¸­
        st.session_state.chat_history.append({"role": "user", "content": None, "image": uploaded_image})

# ç”¨æˆ·è¾“å…¥æ¡†
user_text = st.chat_input("Enter your question")
if user_text:
    with st.chat_message(U_NAME, avatar="user"):
        # å°†ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬æ·»åŠ åˆ°èŠå¤©è®°å½•ä¸­
        st.session_state.chat_history.append({"role": "user", "content": user_text, "image": None})
        st.markdown(f"{U_NAME}: {user_text}")

    # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›å¤
    model = st.session_state.model
    tokenizer = st.session_state.tokenizer

    with st.chat_message(A_NAME, avatar="assistant"):
        # å¦‚æœå‰ä¸€æ¡æ¶ˆæ¯åŒ…å«å›¾ç‰‡ï¼Œå°†å›¾ç‰‡ä¼ é€’ç»™æ¨¡å‹
        if len(st.session_state.chat_history) > 1 and st.session_state.chat_history[-2]["image"] is not None:
            uploaded_image = st.session_state.chat_history[-2]["image"]
            imagefile = Image.open(uploaded_image).convert('RGB')

        msgs = [{"role": "user", "content": user_text}]
        # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›å¤æ–‡æœ¬
        res = model.chat(image=imagefile, msgs=msgs, context=None, tokenizer=tokenizer,
                         sampling=True, top_p=top_p, top_k=top_k, repetition_penalty=repetition_penalty,
                         temperature=temperature, stream=True)

        # æ”¶é›†ç”Ÿæˆçš„æ–‡æœ¬å­—ç¬¦ä¸²
        generated_text = st.write_stream(res)

        st.session_state.chat_history.append({"role": "model", "content": generated_text, "image": None})

    st.divider()  # æ·»åŠ åˆ†å‰²çº¿
```

## è¿è¡Œdemo

åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œå¯åŠ¨streamlitæœåŠ¡

```
streamlit run /root/autodl-tmp/chatBot.py --server.address 127.0.0.1 --server.port 6006
```

ç‚¹å‡»è‡ªå®šä¹‰æœåŠ¡

![](assets/2024-06-12-16-37-45-image.png)

ç‚¹å¼€linux

![](assets/2024-06-12-16-38-35-image.png)

ç„¶åwin+Ræ‰“å¼€powershell

```
ssh -CNg -L 6006:127.0.0.1:6006 root@connect.yza1.seetacloud.com -p 39423
```

è¾“å…¥sshä¸å¯†ç ï¼ŒæŒ‰ä¸‹å›è½¦è‡³è¿™æ ·å³å¯ï¼Œä¿æŒå‘½ä»¤è¡Œç•Œé¢ä¸ºå¼€å¯çŠ¶æ€

![](assets/2024-06-12-16-39-35-image.png)

åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€é“¾æ¥ http://localhost:6006/ ï¼Œå³å¯çœ‹åˆ°èŠå¤©ç•Œé¢ã€‚è¿è¡Œæ•ˆæœå¦‚ä¸‹ï¼š

![](assets/2024-06-12-16-40-17-image.png)

æˆ‘ä¸Šä¼ çš„å›¾ç‰‡å¦‚ä¸‹

![](assets/2024-06-12-17-02-09-è”æƒ³æˆªå›¾_20240528180602.png)

![](assets/2024-06-12-17-05-31-image.png)

å¯ä»¥çœ‹åˆ°æ•ˆæœè¿˜æ˜¯å¾ˆä¸é”™çš„ã€‚é‚£ä¹ˆæˆ‘ä»¬è¿™æœŸå†…å®¹å°±åˆ°è¿™é‡Œäº†ã€‚å¦‚æœæƒ³è¦æ·±å…¥äº†è§£æ¨¡å‹çš„åŸç†ï¼Œå¯ä»¥è®¿é—®æˆ‘ä»¬çš„ä»“åº“ï¼š[datawhalechina/llms-from-scratch-cn](https://github.com/datawhalechina/llms-from-scratch-cn/)

<a href ="https://github.com/datawhalechina/llms-from-scratch-cn/"><img src="assets/2024-06-12-16-59-50-image.png"></a>
