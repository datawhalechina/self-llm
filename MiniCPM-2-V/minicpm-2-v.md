# MiniCPM-2-V transformers éƒ¨ç½²å¾®è°ƒå’Œåº”ç”¨

MiniCPM-V 2.0 æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…·å¤‡ 2.8B å‚æ•°ã€‚è¯¥æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬ OCRBenchã€TextVQAã€MME ç­‰ï¼Œè¶…è¶Šäº†è®¸å¤šå‚æ•°é‡æ›´å¤§çš„æ¨¡å‹ã€‚MiniCPM-V 2.0 å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

1. **æ€§èƒ½å“è¶Š**ï¼šåœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ï¼Œå°¤å…¶åœ¨åœºæ™¯æ–‡å­—ç†è§£ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä¸ Gemini Pro ç›¸å½“ã€‚
2. **å¯é è¡Œä¸º**ï¼šé€šè¿‡å¤šæ¨¡æ€ RLHF æŠ€æœ¯ï¼ˆå¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼‰ï¼Œç¡®ä¿ç”Ÿæˆå†…å®¹çš„å¯ä¿¡åº¦ï¼ŒåŒ¹é… GPT-4V çš„é˜²å¹»è§‰èƒ½åŠ›ã€‚
3. **é«˜åˆ†è¾¨ç‡å›¾åƒå¤„ç†**ï¼šæ”¯æŒä»»æ„é•¿å®½æ¯”çš„é«˜åˆ†è¾¨ç‡å›¾åƒè¾“å…¥ï¼Œæå‡ç»†ç²’åº¦è§†è§‰ä¿¡æ¯æ„ŸçŸ¥èƒ½åŠ›ã€‚
4. **é«˜æ•ˆéƒ¨ç½²**ï¼šèƒ½å¤Ÿåœ¨å¤šæ•° GPU å¡å’Œä¸ªäººç”µè„‘ä¸Šé«˜æ•ˆè¿è¡Œï¼Œç”šè‡³å¯åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šè¿è¡Œã€‚
5. **åŒè¯­æ”¯æŒ**ï¼šå…·å¤‡å¼ºå¤§çš„ä¸­è‹±æ–‡å¤šæ¨¡æ€èƒ½åŠ›ï¼Œæ”¯æŒè·¨è¯­è¨€å¤šæ¨¡æ€åº”ç”¨ã€‚

æ¨¡å‹å¯ä»¥åœ¨ NVIDIA GPU æˆ– Mac çš„ MPS ä¸Šè¿›è¡Œæ¨ç†ï¼Œå¹¶é€šè¿‡ vLLM å®ç°é«˜æ•ˆæ¨ç†ã€‚è¯¦ç»†çš„å®‰è£…å’Œä½¿ç”¨æŒ‡å—è¯·å‚è€ƒ [GitHub ä»“åº“](https://github.com/OpenBMB/MiniCPM-V)ã€‚

MiniCPM-V 2.0 å®Œå…¨å¼€æºï¼Œå…è´¹ä¾›å­¦æœ¯ç ”ç©¶ä½¿ç”¨ï¼Œå¹¶åœ¨å¡«å†™é—®å·åå…è´¹ç”¨äºå•†ä¸šç”¨é€”ã€‚æœ‰å…³æ¨¡å‹çš„æ›´å¤šä¿¡æ¯å’ŒæŠ€æœ¯ç»†èŠ‚ï¼Œè¯·è®¿é—® [æŠ€æœ¯åšå®¢](https://openbmb.vercel.app/minicpm-v-2)ã€‚

å¯ä»¥é€šè¿‡å¦‚ä¸‹çš„æ–¹å¼æ¨ç†ï¼š


```python
from chat import MiniCPMVChat, img2base64
import torch
import json

torch.manual_seed(0)

chat_model = MiniCPMVChat('openbmb/MiniCPM-V-2')

im_64 = img2base64('./assets/airplane.jpeg')

# First round chat 
msgs = [{"role": "user", "content": "Tell me the model of this aircraft."}]

inputs = {"image": im_64, "question": json.dumps(msgs)}
answer = chat_model.chat(inputs)
print(answer)

# Second round chat 
# pass history context of multi-turn conversation
msgs.append({"role": "assistant", "content": answer})
msgs.append({"role": "user", "content": "Introduce something about Airbus A380."})

inputs = {"image": im_64, "question": json.dumps(msgs)}
answer = chat_model.chat(inputs)
print(answer)
```

### æ•°æ®å‡†å¤‡

è¯·å°†æ•°æ®å‡†å¤‡ä¸ºå¦‚ä¸‹çš„jsonæ ¼å¼ï¼Œå¯¹äºå¤šæ¨¡æ€å›¾åƒï¼Œéœ€è¦è®¾ç½®å›¾åƒè·¯å¾„ï¼Œæ”¯æŒå¤šè½®å¯¹è¯ï¼Œä½†æ˜¯æ¯è½®åªèƒ½ä½¿ç”¨ä¸€å¼ å›¾ç‰‡ã€‚


```python
  [
    {
      "id": "0",
      "image": 'path/to/image_0.jpg',
      "conversations": [
            {
              'role': 'user', 
              'content': '<image>\nHow many desserts are on the white plate?'
            }, 
            {
                'role': 'assistant', 
                'content': 'There are three desserts on the white plate.'
            },   
            {
                'role': 'user', 
                'content': 'What type of desserts are they?'
            },
            {
                'role': 'assistant', 
                'content': 'The desserts are cakes with bananas and pecans on top. They share similarities with donuts, but the presence of bananas and pecans differentiates them.'
            }, 
            {
                'role': 'user', 
                'content': 'What is the setting of the image?'}, 
            {
                'role': 'assistant', 
                'content': 'The image is set on a table top with a plate containing the three desserts.'
            },
        ]
    },
  ]
```

åœ¨è®­ç»ƒ MiniCPM-V 2.0 æ¨¡å‹æ—¶ï¼Œå¯ä»¥ä½¿ç”¨ `finetune_lora.sh` è„šæœ¬ã€‚æ ¹æ®éªŒè¯ï¼Œæœ€å°çš„è®­ç»ƒèµ„æºéœ€æ±‚ä¸º3å¼ RTX 3090æ˜¾å¡ï¼ŒåŒæ—¶éœ€è¦ä½¿ç”¨ `cpu_low_memo` çš„ `ds_config_zero2.json` é…ç½®æ–‡ä»¶ï¼Œå› æ­¤éœ€è¦ä¾èµ– DeepSpeed æ¡†æ¶ã€‚åœ¨æ–‡ä»¶å¼€å¤´éœ€è¦è¿›è¡Œå¦‚ä¸‹è®¾ç½®ï¼š


### è®¾ç½®æ­¥éª¤

1. **DeepSpeed é…ç½®æ–‡ä»¶**ï¼šéœ€è¦å‡†å¤‡ `ds_config_zero2.json` é…ç½®æ–‡ä»¶ï¼Œè®¾ç½®ä¸ºä½å†…å­˜ä½¿ç”¨æ¨¡å¼ï¼ˆ`cpu_low_memo`ï¼‰ã€‚
2. **å¤šGPUè®¾ç½®**ï¼šåœ¨è„šæœ¬å¼€å¤´æŒ‡å®šä½¿ç”¨3å¼ RTX 3090æ˜¾å¡è¿›è¡Œè®­ç»ƒã€‚
3. **ä¾èµ–å®‰è£…**ï¼šç¡®ä¿ç¯å¢ƒä¸­å·²ç»å®‰è£…äº† DeepSpeedï¼Œå¹¶æ­£ç¡®é…ç½®è·¯å¾„å’Œä¾èµ–ã€‚


```python
#!/bin/bash

export CUDA_VISIBLE_DEVICES=0,1,2

# è®¾ç½® HF_HOME ç¯å¢ƒå˜é‡ è®¾ç½®ä¸‹è½½è·¯å¾„
export HF_HOME=/home/data/username/hf-models/
export HF_ENDPOINT=https://hf-mirror.com

GPUS_PER_NODE=3
NNODES=1
NODE_RANK=0
MASTER_ADDR=localhost
MASTER_PORT=6001


MODEL="openbmb/MiniCPM-V-2" # or openbmb/MiniCPM-V-2
DATA="./data/train_en_train.json" # json file
EVAL_DATA="./data/train_zh_train.json" # json file
LLM_TYPE="minicpm" # if use openbmb/MiniCPM-V-2, please set LLM_TYPE=minicpm
```


```python
!sh finetune_lora.sh
```


    /home/data/ckw/micromamba/envs/kewei-ai/lib/python3.12/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
      warnings.warn(
    [2024-06-08 09:05:58,136] [INFO] [comm.py:637:init_distributed] cdb=None
    [2024-06-08 09:05:58,136] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
    /home/data/ckw/micromamba/envs/kewei-ai/lib/python3.12/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
      warnings.warn(
    [2024-06-08 09:05:58,153] [INFO] [comm.py:637:init_distributed] cdb=None
    /home/data/ckw/micromamba/envs/kewei-ai/lib/python3.12/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
      warnings.warn(
    [2024-06-08 09:05:58,205] [INFO] [comm.py:637:init_distributed] cdb=None
    The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
    The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
    The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
    Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.06s/it]
    Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.08s/it]
    Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.17s/it]
    max_steps is given, it will override any value given in num_train_epochs
    Currently using LoRA for fine-tuning the MiniCPM-V model.
    max_steps is given, it will override any value given in num_train_epochs
    {'Total': 3458558752, 'Trainable': 733677856}
    llm_type=minicpm
    Loading data...
    max_steps is given, it will override any value given in num_train_epochs
      0%|                                                   | 0/998 [00:00<?, ?it/s]
      ...
    /home/data/ckw/micromamba/envs/kewei-ai/lib/python3.12/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
      warnings.warn(
    {'loss': 1.2035, 'grad_norm': 2.8456802368164062, 'learning_rate': 0.0, 'epoch': 0.01}
    {'loss': 1.2772, 'grad_norm': 4.5663909912109375, 'learning_rate': 3.010299956639811e-07, 'epoch': 0.01}
    {'loss': 1.3038, 'grad_norm': 4.5663909912109375, 'learning_rate': 3.010299956639811e-07, 'epoch': 0.02}
    {'loss': 1.4214, 'grad_norm': 4.5663909912109375, 'learning_rate': 3.010299956639811e-07, 'epoch': 0.02}
    {'loss': 1.279, 'grad_norm': 3.770563840866089, 'learning_rate': 4.771212547196623e-07, 'epoch': 0.03}
    ...
    {'loss': 1.0607, 'grad_norm': 3.499253988265991, 'learning_rate': 1e-06, 'epoch': 5.95}
    {'loss': 1.0804, 'grad_norm': 2.7949860095977783, 'learning_rate': 1e-06, 'epoch': 5.95}
    {'loss': 1.2137, 'grad_norm': 3.113947629928589, 'learning_rate': 1e-06, 'epoch': 5.96}
    {'loss': 0.9199, 'grad_norm': 3.8179216384887695, 'learning_rate': 1e-06, 'epoch': 5.96}
    {'loss': 1.0886, 'grad_norm': 2.0026695728302, 'learning_rate': 1e-06, 'epoch': 5.97}
    {'loss': 1.0101, 'grad_norm': 3.278071641921997, 'learning_rate': 1e-06, 'epoch': 5.98}
    {'train_runtime': 3244.7018, 'train_samples_per_second': 1.845, 'train_steps_per_second': 0.308, 'train_loss': 1.1752079226569327, 'epoch': 5.98}
    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 998/998 [54:04<00:00,  3.25s/it]
    /home/data/ckw/micromamba/envs/kewei-ai/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
      warnings.warn(


### è½½å…¥loraæ¨¡å‹


```python
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Union, Literal, Tuple
```


```python
@dataclass
class LoraArguments:
    lora_r: int = 64
    lora_alpha: int = 64
    lora_dropout: float = 0.05
    lora_target_modules: str = r"llm\..*layers\.\d+\.self_attn\.(q_proj|k_proj|v_proj)"
    lora_weight_path: str = ""
    lora_bias: str = "none"
    q_lora: bool = False
    lora_modules_to_save: str = ""
    lora_layer_replication: Optional[List[Tuple[int, int]]] = None
    lora_layers_to_transform: Optional[List[int]] = None
    lora_layers_pattern: Optional[str] = None
```


```python
from peft import LoraConfig, get_peft_model, TaskType
def load_lora_config(model):
    config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=64,
        lora_alpha=64,
        lora_dropout=0.05,
        target_modules=r"llm\..*layers\.\d+\.self_attn\.(q_proj|k_proj|v_proj)"
    )
    return get_peft_model(model, config)
```


```python
model = load_lora_config(model)
vpm_resampler_embedtokens_weight = torch.load(f"{path_to_adapter}/vpm_resampler_embedtokens.pt")
msg = model.load_state_dict(vpm_resampler_embedtokens_weight, strict=False)
```


```python
image = Image.open('å±å¹•æˆªå›¾ 2024-05-13 104621.png').convert('RGB')
question = 'What is in the image? Please Speak English.'
msgs = [{'role': 'user', 'content': question}]

res, context, _ = model.chat(
    image=image,
    msgs=msgs,
    context=None,
    tokenizer=tokenizer,
    sampling=True,
    temperature=0.7
)
print(res)
```
Output:

The image is a screenshot of the Windows File Explorer's 'æœç´¢' (search) menu. It includes options such as searching by file type, date modified or created, and other advanced search settings like excluding certain directories from searches using wildcards (*), including files with specific extensions ('* ...'), specifying subfolders to include in your query (`C: \My\Subfolder` for example), filtering based on size ranges within bytes/megabytes etc,'ä»¥åŠè®¾ç½®æ˜¯å¦åœ¨æ–‡ä»¶å¤¹æˆ–æ–‡ä»¶ä¸Šæ˜¾ç¤ºä¿®æ”¹æ—¶é—´ã€‚