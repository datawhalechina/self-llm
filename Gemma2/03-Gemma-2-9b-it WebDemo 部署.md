# Yuan2.0-2B WebDemoéƒ¨ç½²

## ç¯å¢ƒå‡†å¤‡

åœ¨ Autodl å¹³å°ä¸­ç§Ÿèµä¸€ä¸ª RTX 3090/24G æ˜¾å­˜çš„æ˜¾å¡æœºå™¨ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œé•œåƒé€‰æ‹© PyTorch-->2.1.0-->3.10(ubuntu22.04)-->12.1ã€‚

![å¼€å¯æœºå™¨é…ç½®é€‰æ‹©](images/01-1.png)

ç„¶åæ‰“å¼€å…¶ä¸­çš„ç»ˆç«¯ï¼Œå¼€å§‹ç¯å¢ƒé…ç½®ã€æ¨¡å‹ä¸‹è½½å’Œè¿è¡Œæ¼”ç¤ºã€‚  

## ç¯å¢ƒé…ç½®

pip æ¢æºåŠ é€Ÿä¸‹è½½å¹¶å®‰è£…ä¾èµ–åŒ…

```shell
# å‡çº§pip
python -m pip install --upgrade pip

# æ›´æ¢ pypi æºåŠ é€Ÿåº“çš„å®‰è£…
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

# å®‰è£… einops modelscope streamlit
pip install einops modelscope streamlit==1.24.0
```  

> è€ƒè™‘åˆ°éƒ¨åˆ†åŒå­¦é…ç½®ç¯å¢ƒå¯èƒ½ä¼šé‡åˆ°ä¸€äº›é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨AutoDLå¹³å°å‡†å¤‡äº†Gemma2 çš„ç¯å¢ƒé•œåƒï¼Œè¯¥é•œåƒé€‚ç”¨äºè¯¥ä»“åº“çš„ Gemma2 æ•™ç¨‹æ‰€æœ‰éƒ¨ç½²ç¯å¢ƒã€‚ç‚¹å‡»ä¸‹æ–¹é“¾æ¥å¹¶ç›´æ¥åˆ›å»ºAutodlç¤ºä¾‹å³å¯ã€‚
> ***https://www.codewithgpu.com/i/datawhalechina/self-llm/self-llm-gemma2***


## æ¨¡å‹ä¸‹è½½  

ä½¿ç”¨ modelscope ä¸­çš„ snapshot_download å‡½æ•°ä¸‹è½½æ¨¡å‹ï¼Œç¬¬ä¸€ä¸ªå‚æ•°ä¸ºæ¨¡å‹åç§°ï¼Œå‚æ•° cache_dir ä¸ºæ¨¡å‹çš„ä¸‹è½½è·¯å¾„ã€‚

ç„¶åè¿è¡Œä¸‹é¢ä»£ç ï¼Œæ‰§è¡Œæ¨¡å‹ä¸‹è½½ã€‚æ¨¡å‹å¤§å°ä¸º 18GBå·¦å³ï¼Œä¸‹è½½å¤§æ¦‚éœ€è¦ 5 åˆ†é’Ÿã€‚

```python
# å¯¼å…¥æ‰€éœ€çš„åº“
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import streamlit as st

# åœ¨ä¾§è¾¹æ ä¸­åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªé“¾æ¥
with st.sidebar:
    st.markdown("## Gemma2.0 LLM")
    "[å¼€æºå¤§æ¨¡å‹é£Ÿç”¨æŒ‡å— self-llm](https://github.com/datawhalechina/self-llm.git)"
    # åˆ›å»ºä¸€ä¸ªæ»‘å—ï¼Œç”¨äºé€‰æ‹©æœ€å¤§é•¿åº¦ï¼ŒèŒƒå›´åœ¨0åˆ°1024ä¹‹é—´ï¼Œé»˜è®¤å€¼ä¸º512
    max_length = st.slider("max_length", 0, 1024, 512, step=1)

# åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªå‰¯æ ‡é¢˜
st.title("ğŸ’¬ Gemma2.0 Chatbot")
st.caption("ğŸš€ A streamlit chatbot powered by Self-LLM")

# å®šä¹‰æ¨¡å‹è·¯å¾„
path = '/root/autodl-tmp/LLM-Research/gemma-2-9b-it'

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè·å–æ¨¡å‹å’Œtokenizer
@st.cache_resource
def get_model():
    print("Creat tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(path)
    print("Creat model...")
    model = AutoModelForCausalLM.from_pretrained(path, device_map="cuda", torch_dtype=torch.bfloat16,)
  
    return tokenizer, model

# åŠ è½½emma-2-9b-itçš„modelå’Œtokenizer
tokenizer, model = get_model()

# å¦‚æœsession_stateä¸­æ²¡æœ‰"messages"ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªåŒ…å«é»˜è®¤æ¶ˆæ¯çš„åˆ—è¡¨
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# éå†session_stateä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œå¹¶æ˜¾ç¤ºåœ¨èŠå¤©ç•Œé¢ä¸Š
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# å¦‚æœç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ¡†ä¸­è¾“å…¥äº†å†…å®¹ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
if prompt := st.chat_input():
    # å°†ç”¨æˆ·çš„è¾“å…¥æ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "user", "content": prompt})

    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºç”¨æˆ·çš„è¾“å…¥
    st.chat_message("user").write(prompt)

    # è°ƒç”¨æ¨¡å‹
    inputs = tokenizer.apply_chat_template(st.session_state.messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer.encode(inputs, add_special_tokens=False, return_tensors="pt")
    outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=150)
    outputs = tokenizer.decode(outputs[0])
    response = outputs.split('model')[-1].replace('<end_of_turn>\n<eos>', '')

    # å°†æ¨¡å‹çš„è¾“å‡ºæ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "model", "content": response})

    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
    st.chat_message("model").write(response)

    # print(st.session_state)
```

# é…ç½®vscode ssh

å¤åˆ¶æœºå™¨sshç™»å½•æŒ‡ä»¤

![](images/03-0.png)

ç²˜è´´åˆ°æœ¬åœ°ç”µè„‘çš„.ssh/configï¼Œå¹¶ä¿®æ”¹æˆå¦‚ä¸‹æ ¼å¼

![](images/03-1.png)

ç„¶åè¿æ¥åˆ°æ­¤sshï¼Œé€‰æ‹©linx

![](images/03-2.png)

å¤åˆ¶å¯†ç å¹¶è¾“å…¥ï¼ŒæŒ‰ä¸‹å›è½¦å³å¯ç™»å½•åˆ°æœºå™¨

## è¿è¡Œdemo

åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œå¯åŠ¨streamlitæœåŠ¡

```shell
streamlit run chatBot.py --server.address 127.0.0.1 --server.port 6006
```

ç‚¹å‡»åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ï¼Œå³å¯çœ‹åˆ°èŠå¤©ç•Œé¢ã€‚

è¿è¡Œæ•ˆæœå¦‚ä¸‹ï¼š

![alt text](./images/03-3.png)

