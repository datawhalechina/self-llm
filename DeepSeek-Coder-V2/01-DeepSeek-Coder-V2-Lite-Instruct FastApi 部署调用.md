# DeepSeek-Coder-V2-Lite-Instruct FastApi éƒ¨ç½²è°ƒç”¨

## **ç¯å¢ƒå‡†å¤‡**

åœ¨ `AutoDL` å¹³å°ä¸­ç§Ÿèµä¸¤ä¸ª 3090 ç­‰ 24G*2 æ˜¾å­˜å¤§å°çš„å®¹å™¨å®ä¾‹ï¼Œé•œåƒé€‰æ‹©å¦‚ä¸‹ `PyTorch`â†’`2.1.0`â†’`3.10(ubuntu22.04)`â†’`12.1`

![fig1-1](images\fig1-1.png)

æ¥ä¸‹æ¥æ‰“å¼€æœ¬åœ°è®¾å¤‡ç»ˆç«¯ä½¿ç”¨ `ssh` çš„æ–¹å¼è®¿é—®ï¼Œåœ¨ç»ˆç«¯ä¸­ä¾æ¬¡å¤åˆ¶ç™»å½•æŒ‡ä»¤å’Œå¯†ç å®Œæˆç™»å½•

![fig1-2](images\fig1-2.png)

`ssh` ç™»å½•æˆåŠŸåçš„ç•Œé¢å¦‚å›¾æ‰€ç¤ºğŸ‘‡

![fig1-3](images\fig1-3.png)

æˆ–è€…ä¹Ÿå¯ä»¥ç›´æ¥æ‰“å¼€ `AutoDL` ç½‘é¡µç«¯çš„å¿«æ·å·¥å…·ä¸­é€‰æ‹© `JupyterLab` å¹¶åœ¨å…¶ä¸­ç‚¹å‡»ç»ˆç«¯æ‰“å¼€ï¼ˆè¿™ç§æ–¹å¼ä¸éœ€è¦éªŒè¯ğŸ« ï¼‰

![fig1-4](images\fig1-4.png)æ¥ä¸‹æ¥å¼€å§‹ç¯å¢ƒé…ç½®ã€æ¨¡å‹ä¸‹è½½å’Œè¿è¡Œæ¼”ç¤º ~

`pip` æ¢æºåŠ é€Ÿä¸‹è½½å¹¶å®‰è£…ä¾èµ–åŒ…

```bash
# å‡çº§pip
python -m pip install --upgrade pip
# æ›´æ¢ pypi æºåŠ é€Ÿåº“çš„å®‰è£…
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

pip install fastapi==0.104.1
pip install uvicorn==0.24.0.post1
pip install requests==2.32.3
pip install modelscope==1.9.5
pip install transformers==4.39.2
pip install streamlit==1.24.0
pip install sentencepiece==0.1.99
pip install accelerate==0.27.0
pip install tiktoken==0.7.0
pip install huggingface_hub==0.23.4
pip install flash-attn==2.5.9.post1
```

> è€ƒè™‘åˆ°éƒ¨åˆ†åŒå­¦é…ç½®ç¯å¢ƒå¯èƒ½ä¼šé‡åˆ°ä¸€äº›é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨ `AutoDL` å¹³å°å‡†å¤‡äº† `DeepSeek-Coder-V2-Lite-Instruct` çš„ç¯å¢ƒé•œåƒã€‚ç‚¹å‡»ä¸‹æ–¹é“¾æ¥å¹¶ç›´æ¥åˆ›å»º `Autodl` ç¤ºä¾‹å³å¯ã€‚
> ***https://www.codewithgpu.com/i/datawhalechina/self-llm/deepseek-coder***



## æ¨¡å‹ä¸‹è½½

ä½¿ç”¨ `modelscope` ä¸­çš„ `snapshot_download` å‡½æ•°ä¸‹è½½æ¨¡å‹ï¼Œç¬¬ä¸€ä¸ªå‚æ•°ä¸ºæ¨¡å‹åç§°ï¼Œå‚æ•° `cache_dir` ä¸ºè‡ªå®šä¹‰çš„æ¨¡å‹ä¸‹è½½è·¯å¾„ï¼Œå‚æ•°`revision`ä¸ºæ¨¡å‹ä»“åº“åˆ†æ”¯ç‰ˆæœ¬ï¼Œmasterä»£è¡¨ä¸»åˆ†æ”¯ï¼Œä¹Ÿæ˜¯ä¸€èˆ¬æ¨¡å‹ä¸Šä¼ çš„é»˜è®¤åˆ†æ”¯ã€‚

å…ˆåˆ‡æ¢åˆ° `autodl-tmp` ç›®å½•ï¼Œ`cd /root/autodl-tmp` 

ç„¶åæ–°å»ºåä¸º `model_download.py` çš„ `python` è„šæœ¬ï¼Œå¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹å¹¶ä¿å­˜

```python
# model_download.py
import torch
from modelscope import snapshot_download, AutoModel, AutoTokenizer

model_dir = snapshot_download('deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct', cache_dir='/root/autodl-tmp', revision='master')
```

ç„¶ååœ¨ç»ˆç«¯ä¸­è¾“å…¥ `python model_download.py` æ‰§è¡Œä¸‹è½½ï¼Œæ³¨æ„è¯¥æ¨¡å‹æƒé‡æ–‡ä»¶æ¯”è¾ƒå¤§ï¼Œå› æ­¤è¿™é‡Œéœ€è¦è€å¿ƒç­‰å¾…ä¸€æ®µæ—¶é—´ç›´åˆ°æ¨¡å‹ä¸‹è½½å®Œæˆã€‚



## ä»£ç å‡†å¤‡

åœ¨ `/root/autodl-tmp` è·¯å¾„ä¸‹æ–°å»º `api.py` æ–‡ä»¶å¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼Œç²˜è´´ä»£ç åè®°å¾—ä¿å­˜æ–‡ä»¶ã€‚ä¸‹é¢çš„ä»£ç æœ‰å¾ˆè¯¦ç»†çš„æ³¨é‡Šï¼Œå¤§å®¶å¦‚æœ‰ä¸ç†è§£çš„åœ°æ–¹ï¼Œæ¬¢è¿æ `issue` ğŸ˜Š

```python
# api.py
from fastapi import FastAPI, Request
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import uvicorn
import json
import datetime
import torch

# è®¾ç½®è®¾å¤‡å‚æ•°
DEVICE = "cuda"  # ä½¿ç”¨CUDA
DEVICE_ID = "0"  # CUDAè®¾å¤‡IDï¼Œå¦‚æœæœªè®¾ç½®åˆ™ä¸ºç©º
CUDA_DEVICE = f"{DEVICE}:{DEVICE_ID}" if DEVICE_ID else DEVICE  # ç»„åˆCUDAè®¾å¤‡ä¿¡æ¯

# æ¸…ç†GPUå†…å­˜å‡½æ•°
def torch_gc():
    if torch.cuda.is_available():  # æ£€æŸ¥æ˜¯å¦å¯ç”¨CUDA
        with torch.cuda.device(CUDA_DEVICE):  # æŒ‡å®šCUDAè®¾å¤‡
            torch.cuda.empty_cache()  # æ¸…ç©ºCUDAç¼“å­˜
            torch.cuda.ipc_collect()  # æ”¶é›†CUDAå†…å­˜ç¢ç‰‡

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI()

# å¤„ç†POSTè¯·æ±‚çš„ç«¯ç‚¹
@app.post("/")
async def create_item(request: Request):
    global model, tokenizer  # å£°æ˜å…¨å±€å˜é‡ä»¥ä¾¿åœ¨å‡½æ•°å†…éƒ¨ä½¿ç”¨æ¨¡å‹å’Œåˆ†è¯å™¨
    json_post_raw = await request.json()  # è·å–POSTè¯·æ±‚çš„JSONæ•°æ®
    json_post = json.dumps(json_post_raw)  # å°†JSONæ•°æ®è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    json_post_list = json.loads(json_post)  # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºPythonå¯¹è±¡
    prompt = json_post_list.get('prompt')  # è·å–è¯·æ±‚ä¸­çš„æç¤º
    history = json_post_list.get('history')  # è·å–è¯·æ±‚ä¸­çš„å†å²è®°å½•
    max_length = json_post_list.get('max_length')  # è·å–è¯·æ±‚ä¸­çš„æœ€å¤§é•¿åº¦
    top_p = json_post_list.get('top_p')  # è·å–è¯·æ±‚ä¸­çš„top_på‚æ•°
    top_k = json_post_list.get('top_k')  # è·å–è¯·æ±‚ä¸­çš„top_kå‚æ•°
    temperature = json_post_list.get('temperature')  # è·å–è¯·æ±‚ä¸­çš„æ¸©åº¦å‚æ•°
    repetition_penalty = json_post_list.get('repetition_penalty')  # è·å–è¯·æ±‚ä¸­çš„é‡å¤æƒ©ç½šå‚æ•°
    
    model_input = []
    for q, a in history:
        model_input.append({"role": "user", "content": q})
        model_input.append({"role": "assistant", "content": a})
    model_input.append({"role": "user", "content": prompt})

    # è°ƒç”¨æ¨¡å‹è¿›è¡Œå¯¹è¯ç”Ÿæˆ
    model_output = generator(
        model_input, 
        max_new_tokens=max_length if max_length else 1024,
        top_k=top_k if top_k else 5, # å¦‚æœæœªæä¾›top_kå‚æ•°ï¼Œé»˜è®¤ä½¿ç”¨0.5
        top_p=top_p if top_p else 0.7,  # å¦‚æœæœªæä¾›top_på‚æ•°ï¼Œé»˜è®¤ä½¿ç”¨0.7
        temperature=temperature if temperature else 0.95,  # å¦‚æœæœªæä¾›æ¸©åº¦å‚æ•°ï¼Œé»˜è®¤ä½¿ç”¨0.95, 
        repetition_penalty=repetition_penalty if repetition_penalty else 1.1, # å¦‚æœæœªæä¾›é‡å¤æƒ©ç½šå‚æ•°ï¼Œé»˜è®¤ä½¿ç”¨1.1, 
        do_sample=True
    )
    response = model_output[0]['generated_text'][-1]['content']
    history.append([prompt, response])
    now = datetime.datetime.now()  # è·å–å½“å‰æ—¶é—´
    time = now.strftime("%Y-%m-%d %H:%M:%S")  # æ ¼å¼åŒ–æ—¶é—´ä¸ºå­—ç¬¦ä¸²
    # æ„å»ºå“åº”JSON
    answer = {
        "response": response,
        "history": history,
        "status": 200,
        "time": time
    }
    # æ„å»ºæ—¥å¿—ä¿¡æ¯
    log = "[" + time + "] " + '", prompt:"' + prompt + '", response:"' + repr(response) + '"'
    print(log)  # æ‰“å°æ—¥å¿—
    torch_gc()  # æ‰§è¡ŒGPUå†…å­˜æ¸…ç†
    return answer  # è¿”å›å“åº”

# ä¸»å‡½æ•°å…¥å£
if __name__ == '__main__':
    # åŠ è½½é¢„è®­ç»ƒçš„åˆ†è¯å™¨å’Œæ¨¡å‹
    mode_name_or_path = '/root/autodl-tmp/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct'
    tokenizer = AutoTokenizer.from_pretrained(mode_name_or_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(mode_name_or_path, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map="auto")
    generator = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer, 
        trust_remote_code=True, 
        device_map="auto"
    )
    
    # å¯åŠ¨FastAPIåº”ç”¨
    # ç”¨6006ç«¯å£å¯ä»¥å°†autodlçš„ç«¯å£æ˜ å°„åˆ°æœ¬åœ°ï¼Œä»è€Œåœ¨æœ¬åœ°ä½¿ç”¨api
    uvicorn.run(app, host='0.0.0.0', port=6006, workers=1)  # åœ¨æŒ‡å®šç«¯å£å’Œä¸»æœºä¸Šå¯åŠ¨åº”ç”¨
```



## API éƒ¨ç½²

åœ¨ç»ˆç«¯è¾“å…¥ä»¥ä¸‹å‘½ä»¤å¯åŠ¨ `api` æœåŠ¡ã€‚

```shell
cd /root/autodl-tmp
python api.py
```

ç»ˆç«¯å‡ºç°ä»¥ä¸‹ç»“æœè¡¨ç¤ºå¯ç”¨ `api` æœåŠ¡æˆåŠŸã€‚

![fig1-5](images/fig1-5.png)

é»˜è®¤éƒ¨ç½²åœ¨ `6006` ç«¯å£ï¼Œé€šè¿‡ `POST` æ–¹æ³•è¿›è¡Œè°ƒç”¨ï¼Œå¯ä»¥æ–°å»ºä¸€ä¸ªç»ˆç«¯ä½¿ç”¨ `curl` è°ƒç”¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```shell
curl -X POST "http://127.0.0.1:6006" \
     -H 'Content-Type: application/json' \
     -d '{"prompt": "ä½ å¥½", "history": []}'
```

å¾—åˆ°çš„è¿”å›å€¼å¦‚ä¸‹æ‰€ç¤ºï¼š

```json
{
  "response": " æ‚¨å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿæ— è®ºæ˜¯æ—¥å¸¸ç”Ÿæ´»ä¸­çš„é—®é¢˜ï¼Œè¿˜æ˜¯å·¥ä½œå­¦ä¹ ä¸Šçš„ç–‘æƒ‘ï¼Œæˆ‘éƒ½åœ¨è¿™é‡Œå°½åŠ›ä¸ºæ‚¨æä¾›å¸®åŠ©å’Œè§£ç­”ã€‚",
  "history": [
    [
      "ä½ å¥½",
      " æ‚¨å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿæ— è®ºæ˜¯æ—¥å¸¸ç”Ÿæ´»ä¸­çš„é—®é¢˜ï¼Œè¿˜æ˜¯å·¥ä½œå­¦ä¹ ä¸Šçš„ç–‘æƒ‘ï¼Œæˆ‘éƒ½åœ¨è¿™é‡Œå°½åŠ›ä¸ºæ‚¨æä¾›å¸®åŠ©å’Œè§£ç­”ã€‚"
    ]
  ],
  "status": 200,
  "time": "2024-06-23 23:56:27"
}
```

è°ƒç”¨ç¤ºä¾‹ç»“æœå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![fig1-6](images/fig1-6.png)

æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨ `Python` ä¸­çš„ `requests` åº“è¿›è¡Œè°ƒç”¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
# request.py
import requests
import json

def get_completion(prompt):
    headers = {'Content-Type': 'application/json'}
    data = {"prompt": prompt, "history": []}
    response = requests.post(url='http://127.0.0.1:6006', headers=headers, data=json.dumps(data))
    return response.json()['response']

if __name__ == '__main__':
    print(get_completion('ä½ å¥½ï¼Œå¸®æˆ‘ä»‹ç»ä¸€ä¸‹DeepSeek-Coder-V2-Lite-Instructå¤§è¯­è¨€æ¨¡å‹~'))
```

å¾—åˆ°çš„è¿”å›å€¼å¦‚ä¸‹æ‰€ç¤ºï¼š

```
æ·±åº¦æ±‚ç´¢ï¼ˆDeepSeekï¼‰æ˜¯ä¸€å®¶ä¸“æ³¨å®ç°AGIçš„ä¸­å›½çš„äººå·¥æ™ºèƒ½å…¬å¸ï¼Œå¦‚æœä½ æåˆ°çš„â€œDeepSeek-Coder-V2-Lite-Instructâ€æ˜¯è¯¥å…¬å¸å¼€å‘çš„æŸç§äº§å“æˆ–æœåŠ¡çš„ä¸€éƒ¨åˆ†ï¼Œé‚£ä¹ˆä»¥ä¸‹æ˜¯å¯¹å…¶çš„ä¸€èˆ¬æ€§ä»‹ç»ï¼š

1. **æ¨¡å‹æ¦‚è¿°**ï¼š
   - DeepSeek Coder-V2-Lite-Instructå¯èƒ½æ˜¯ä¸€ä¸ªç‰¹å®šç‰ˆæœ¬çš„å¤§è¯­è¨€æ¨¡å‹ã€‚å¤§è¯­è¨€æ¨¡å‹é€šå¸¸æ˜¯æŒ‡èƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è‡ªç„¶è¯­è¨€çš„è®¡ç®—æœºç¨‹åºã€‚è¿™ç±»æ¨¡å‹é€šå¸¸é€šè¿‡å¤§é‡æ•°æ®è®­ç»ƒè€Œæˆï¼Œä»¥ä¾¿åœ¨å¹¿æ³›çš„æ–‡æœ¬ä¸Š å­¦ä¹ è¯­è¨€æ¨¡å¼å’Œç»“æ„ã€‚
   - "Instruct"éƒ¨åˆ†å¯èƒ½è¡¨ç¤ºè¯¥æ¨¡å‹ç»è¿‡ä¸“é—¨è®¾è®¡ï¼Œç”¨äºéµå¾ªæŒ‡ä»¤æˆ–è€…æ‰§è¡Œç‰¹å®šçš„ä»»åŠ¡æŒ‡ç¤ºã€‚è¿™ç§ç±»å‹çš„æ¨¡å‹æ—¨åœ¨æ›´å¥½åœ°ç†è§£ç”¨æˆ·çš„æŸ¥è¯¢æ„å›¾å¹¶æä¾›æ›´å‡†ç¡®çš„å›ç­”ã€‚

2. **æŠ€æœ¯ç‰¹ç‚¹**ï¼š
   - **æ€§èƒ½ä¼˜åŒ–**ï¼šä½œä¸ºâ€œLiteâ€ç‰ˆæœ¬ï¼Œå®ƒå¯èƒ½åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œå¯¹èµ„æºçš„ä½¿ç”¨è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä½¿å¾—å®ƒåœ¨è®¡ç®—æˆæœ¬è¾ƒä½çš„ç¯å¢ƒä¸­ä¹Ÿèƒ½é«˜æ•ˆè¿è¡Œã€‚
   - **æŒ‡ä»¤è·Ÿéšèƒ½åŠ›**ï¼šâ€œInstructâ€è¡¨æ˜è¯¥æ¨¡å‹ç‰¹åˆ«æ“…é•¿å¤„ç†é‚£äº›éœ€è¦æ˜ç¡®æŒ‡ä»¤æ‰èƒ½æ­£ç¡®æ‰§è¡Œçš„ä»»åŠ¡ï¼Œæ¯”å¦‚é—®ç­”ã€æ–‡æœ¬ç”Ÿæˆç­‰ã€‚

3. **åº”ç”¨åœºæ™¯**ï¼š
   - ç”±äºå¤§è¯­è¨€æ¨¡å‹çš„çµæ´»æ€§å’Œå¼ºå¤§çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ï¼Œå®ƒä»¬å¯ä»¥åº”ç”¨äºå¤šç§åœºæ™¯ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š
     - å†…å®¹åˆ›ä½œï¼šå¸®åŠ©æ’°å†™æ–‡ç« ã€æŠ¥å‘Šç­‰ã€‚
     - å®¢æˆ·æ”¯æŒï¼šè‡ªåŠ¨å›ç­”å¸¸è§é—®é¢˜ï¼Œæä¾›å³æ—¶çš„å®¢æˆ·æ”¯æŒã€‚
     - æ•™è‚²å’ŒåŸ¹è®­ï¼šä¸ºå­¦ç”Ÿå’Œä¸“ä¸šäººå£«æä¾›ä¸ªæ€§åŒ–çš„å­¦ä¹ ææ–™å’ŒæŒ‡å¯¼ã€‚
     - ç ”ç©¶å’Œåˆ†æï¼šè¾…åŠ©ç ”ç©¶äººå‘˜è¿›è¡Œæ–‡çŒ®ç»¼è¿°ã€æ•°æ®åˆ†æç­‰å·¥ä½œã€‚

4. **æœªæ¥å±•æœ›**ï¼š
   - éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ï¼Œå¤§è¯­è¨€æ¨¡å‹å¯èƒ½ä¼šå˜å¾—æ›´åŠ å¤æ‚å’Œå¼ºå¤§ï¼Œèƒ½å¤Ÿåœ¨æ›´å¤šé¢†åŸŸå‘æŒ¥ä½œç”¨ï¼Œå¹¶ä¸”æ›´åŠ ç²¾å‡†åœ°æ»¡è¶³ç”¨æˆ·çš„éœ€æ±‚ã€‚

è¯·æ³¨æ„ï¼Œä¸Šè¿°æè¿°åŸºäºä¸€èˆ¬å¤§è¯­è¨€æ¨¡å‹çš„ç‰¹å¾ï¼Œå¹¶ä¸ç‰¹æŒ‡â€œDeepSeek-Coder-V2-Lite-Instructâ€çš„å…·ä½“åŠŸèƒ½å’Œæ€§èƒ½ã€‚å¦‚æœéœ€è¦äº†è§£è¯¥æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯ï¼Œå»ºè®®ç›´æ¥å’¨è¯¢æ·±åº¦æ±‚ç´¢å…¬å¸æˆ–æŸ¥çœ‹å®˜æ–¹å‘å¸ƒçš„æ–‡æ¡£å’Œèµ„æ–™ã€‚
```

è°ƒç”¨ç¤ºä¾‹ç»“æœå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![fig1-7](images/fig1-7.png)

